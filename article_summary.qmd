---
title: "Replication and Robustness Analysis of Andreadis et al. (2025): Local Predictors of AI Job Concentration in U.S. Counties"
author:
  - name: "Jacob Khaykin"
    affiliation: "Solon High School, Solon, OH, United States"
    email: "jacobkhaykin27@solonschools.net"
format:
  pdf:
    number-sections: true
    toc: false
bibliography: references.bib
link-citations: true
execute:
  echo: false
  warning: false
  message: false
nocite: '@*'
---


**JEL:** J23, O33, R11, C18
**Keywords:** artificial intelligence, local labor markets, replication, robustness analysis, weighting methods

## Abstract

This paper replicates and extends the analysis of Andreadis et al. (2025), who examine the demographic, innovation, and industry correlates of AI-related employment in U.S. counties from 2014–2023. Using their approach and county-level data, I reproduce the main regression results on AI job share levels and changes. I also identify instances of causal language inconsistent with the observational design, situating this critique within the Rubin Causal Model and the guidance in Holland (1986). As a robustness check, I re-estimate their models using log-population weights. This change dampens leverage from large metropolitan counties and shows that some relationships—especially between educational attainment and AI intensity—are size-sensitive. Labor market tightness remains a robust positive correlate across specifications. The results reinforce the value of robustness checks and careful causal language in policy-relevant work.

## Introduction

Andreadis et al. (2025) study local predictors of AI-related job concentration in the United States using county-level data from Lightcast, the U.S. Census ACS, USPTO patent databases, and other sources. They estimate weighted least squares regressions of the AI job share and changes in that share on lagged demographic, innovation, and industry characteristics with county and year fixed effects.

Understanding where AI-related jobs concentrate matters for regional inequality and development. As @eloundou2024 note, a large share of the workforce could see meaningful effects on tasks from AI and large language models. Identifying correlates of AI adoption can help anticipate distributional impacts.

This replication has two goals. First, verify reproducibility of the original results. Second, probe robustness by replacing the original population weights with log-population weights to reduce leverage from the largest counties.

## Data

I follow Andreadis et al. (2025) to assemble a county-by-year panel from 2014–2023. The dependent variables are:

* **AI job share:** AI-related job postings divided by total postings in a county-year (percent).
* **Change in AI job share:** difference between average AI share in 2017–2018 and 2022–2023.

Explanatory variables (all lagged one year and standardized as z-scores):

* **Demographics:** bachelor’s degree share, Black population share, poverty share, log population, log median income.
* **Innovation:** patents per employee, AI patents’ share, STEM degree share, degrees per capita.
* **Industry and labor market:** labor market tightness, turnover rate, manufacturing intensity, ICT intensity, large establishments’ share.

The original paper uses population weights in WLS. I reproduce those estimates and then re-estimate with log-population weights $w_{it} = \log(\text{Population}_{it})$ as a robustness check.

## Replication of Original Results

I reproduce the main tables estimating correlates of (i) AI job share and (ii) change in AI job share. Coefficients, standard errors, and significance align with the originals within rounding error. In brief:

* **Levels (AI share):** bachelor’s degree share, labor market tightness, patents per employee, and STEM share are positive; manufacturing intensity is negative; ICT intensity is positive in richer specs.
* **Changes:** STEM degree share and labor tightness are positive; manufacturing intensity is negative; turnover becomes important in the full model.

### Table 1. The correlates of the share of AI jobs

| Variable                        | (1) Demographics | (2) Innovation | (3) Industry | (4) All Controls | (5) All + State FE |
| :------------------------------ | :--------------: | :------------: | :----------: | :--------------: | :----------------: |
| Bachelor’s share, z-score       |    0.160\*\*\*   |                |              |     0.188\*\*    |        0.085       |
|                                 |      (0.047)     |                |              |      (0.067)     |       (0.059)      |
| Black pop, z-score              |      -0.121      |                |              |      -0.121      |        0.000       |
|                                 |      (0.127)     |                |              |      (0.185)     |       (0.162)      |
| Poverty share, z-score          |       0.044      |                |              |      0.067+      |       -0.003       |
|                                 |      (0.028)     |                |              |      (0.040)     |       (0.037)      |
| log(Population), z-score        |      0.807\*     |                |              |       0.219      |        0.146       |
|                                 |      (0.367)     |                |              |      (0.444)     |       (0.544)      |
| House Price Growth, z-score     |     -0.019\*     |                |              |     -0.028\*     |     -0.032\*\*     |
|                                 |      (0.008)     |                |              |      (0.012)     |       (0.011)      |
| log(Median Income), z-score     |      -0.022      |                |              |      -0.001      |        0.032       |
|                                 |      (0.047)     |                |              |      (0.065)     |       (0.060)      |
| Labor Market Tightness, z-score |    0.255\*\*\*   |                |              |    0.315\*\*\*   |     0.372\*\*\*    |
|                                 |      (0.050)     |                |              |      (0.063)     |       (0.065)      |
| Patents per employee, z-score   |                  |    0.031\*\*   |              |      0.028\*     |       0.031\*      |
|                                 |                  |     (0.012)    |              |      (0.012)     |       (0.014)      |
| AI patents’ share, z-score      |                  |      0.009     |              |       0.009      |        0.003       |
|                                 |                  |     (0.007)    |              |      (0.006)     |       (0.005)      |
| Degrees awarded per capita      |                  |      0.013     |              |       0.034      |        0.034       |
|                                 |                  |     (0.026)    |              |      (0.026)     |       (0.027)      |
| STEM Degrees’ share             |                  |    0.072\*\*   |              |     0.058\*\*    |       0.048\*      |
|                                 |                  |     (0.023)    |              |      (0.022)     |       (0.020)      |
| Large Establishments, z-score   |                  |                |    -0.002    |      -0.037      |       -0.036       |
|                                 |                  |                |    (0.025)   |      (0.028)     |       (0.029)      |
| ICT sector intensity, z-score   |                  |                |     0.010    |      0.038\*     |       0.040\*      |
|                                 |                  |                |    (0.014)   |      (0.017)     |       (0.016)      |
| Manufacturing intensity         |                  |                | -0.057\*\*\* |   -0.048\*\*\*   |      -0.034\*      |
|                                 |                  |                |    (0.012)   |      (0.014)     |       (0.015)      |
| Turnover Rate, z-score          |                  |                |   0.034\*\*  |       0.016      |        0.010       |
|                                 |                  |                |    (0.011)   |      (0.017)     |       (0.016)      |
| **Observations**                |    **27,497**    |   **22,744**   |  **24,651**  |    **19,184**    |     **19,184**     |
| **R²**                          |     **0.704**    |    **0.721**   |   **0.694**  |     **0.758**    |      **0.778**     |
| **Within R²**                   |     **0.057**    |    **0.003**   |   **0.002**  |     **0.059**    |      **0.063**     |

### Table 2. The correlates of the change in the share of AI jobs

| Variable                | (1) Demographics | (2) Innovation | (3) Industry | (4) All Controls | (5) All + State FE |
| :---------------------- | :--------------: | :------------: | :----------: | :--------------: | :----------------: |
| **Constant**            |       0.007      |                |              |      -0.073      |     -0.138\*\*     |
|                         |      (0.027)     |                |              |      (0.050)     |       (0.043)      |
| Bachelors, %            |       0.018      |                |              |       0.049      |        0.051       |
|                         |      (0.019)     |                |              |      (0.033)     |       (0.033)      |
| Black, %                |      0.068+      |                |              |       0.054      |        0.113       |
|                         |      (0.037)     |                |              |      (0.069)     |       (0.076)      |
| Poverty, %              |      -0.019      |                |              |      -0.018      |       -0.020       |
|                         |      (0.022)     |                |              |      (0.055)     |       (0.057)      |
| Pop. Growth             |     -0.034\*     |                |              |      -0.039      |        0.047       |
|                         |      (0.017)     |                |              |      (0.036)     |       (0.042)      |
| House Price Growth      |     0.124\*\*    |                |              |      0.125+      |       0.197\*      |
|                         |      (0.042)     |                |              |      (0.075)     |       (0.079)      |
| Income, Log             |    0.098\*\*\*   |                |              |    0.187\*\*\*   |       0.152+       |
|                         |      (0.021)     |                |              |      (0.040)     |       (0.086)      |
| Tightness               |       0.002      |                |              |      -0.061+     |       -0.008       |
|                         |      (0.027)     |                |              |      (0.035)     |       (0.039)      |
| Patents per employee    |                  |   0.151\*\*\*  |              |     0.113\*\*    |        0.082       |
|                         |                  |     (0.036)    |              |      (0.043)     |       (0.066)      |
| AI Patents’ Share       |                  |      0.021     |              |       0.045      |        0.058       |
|                         |                  |     (0.017)    |              |      (0.029)     |       (0.035)      |
| Degrees per capita      |                  |   0.093\*\*\*  |              |     0.082\*\*    |       0.076+       |
|                         |                  |     (0.027)    |              |      (0.027)     |       (0.036)      |
| STEM Degrees’ share     |                  |      0.031     |              |      -0.064+     |       -0.039       |
|                         |                  |     (0.021)    |              |      (0.049)     |       (0.060)      |
| Large Establishments, % |                  |                |    0.031+    |      -0.024      |       -0.016       |
|                         |                  |                |    (0.018)   |      (0.030)     |       (0.026)      |
| ICT sector intensity    |                  |                |    -0.037+   |      -0.014      |       -0.001       |
|                         |                  |                |    (0.019)   |      (0.046)     |       (0.042)      |
| Manufacturing intensity |                  |                |  -0.072\*\*  |     -0.128\*     |      -0.170\*      |
|                         |                  |                |    (0.023)   |      (0.061)     |       (0.079)      |
| Turnover rate           |                  |                |    0.023+    |    -0.056\*\*    |      -0.042\*      |
|                         |                  |                |    (0.014)   |      (0.022)     |       (0.023)      |
| **Observations**        |     **2,751**    |     **897**    |   **2,723**  |      **810**     |       **810**      |
| **R²**                  |     **0.021**    |    **0.019**   |   **0.016**  |     **0.031**    |      **0.082**     |
| **Within R²**           |       **—**      |      **—**     |     **—**    |       **—**      |      **0.025**     |

## Causal language and identification

The original manuscript sometimes uses terms like “drivers” and “determinants” of AI adoption. In the Rubin Causal Model, causal effects are defined as comparisons of potential outcomes under well-defined interventions, and identification requires assumptions and design features that address confounding (e.g., randomization, instruments, discontinuities). See @holland1986 for a clear statement of why regression with observational data alone doesn’t justify causal claims.

Examples of causal framing in the original paper

“we identify several key drivers of AI job intensity” and that higher STEM, tightness, and patents “significantly predict greater AI adoption.” 

“The findings are robust to including state × year fixed effects specifications, confirming the stability of these relationships.” 

When discussing Table 1: “Labor market tightness emerges as a key driver” and the listed variables “positively and significantly predict AI intensity.” 
 

Why this reads as causal

Phrases like identify, drivers, predict, and confirming the stability imply mechanisms, not just associations. In observational panels with county and year fixed effects, these coefficients are conditional correlations. Without as-if random shocks, instruments, discontinuities, or explicit causal designs, the estimands are not average treatment effects.

Our replication confirms that several local characteristics are correlated with county AI-job shares, conditional on fixed effects. We caution against causal language. As Holland (1986) emphasizes, causal claims hinge on well-defined interventions and counterfactuals; panel regressions with rich controls document associations, not effects, absent exogenous variation or a research design that approximates a randomized intervention. In this setting, terms like “drivers,” “determinants,” or “predict” overstate what the estimates can support. See Holland (1986) for the “no causation without manipulation” principle and the role of counterfactual comparisons in causal inference.

## Robustness: log-population weighting

To assess sensitivity to county size, I re-estimate the specifications with weights $w_{it} = \log(\text{Population}_{it})$. This choice still recognizes population differences but reduces leverage from the largest counties.

Why reweight? The original study uses population weights, which tilt estimation toward very large counties. That choice is defensible if the estimand of interest is a population-average association. Switching to log(population) weights reduces the leverage of mega-counties while still down-weighting tiny places, yielding estimates closer to a “typical-county” association.

Main takeaways:

* The coefficient on **bachelor’s degree share** declines in magnitude under log-population weights, implying size sensitivity.
* **Labor market tightness** remains positive and precisely estimated across both weighting schemes.
* **Manufacturing intensity** retains a negative association; **ICT intensity** remains positive in rich specifications.

## Figure 1: Key Coefficient Estimates for AI Share (Table 1)

This panel plot displays the coefficient estimates and 95% confidence intervals for seven key predictors across five regression models. Each panel represents a different model from the original paper. Within each panel, two estimates are shown for each variable—one using the authors' original population weights and one using log(population) weights. This comparison illustrates how model weighting influences the interpretation of each predictor.

```{r}
#| echo: false
#| warning: false
#| message: false
#| fig-width: 12
#| fig-height: 10

library(dplyr)
library(tibble)
library(ggplot2)
library(forcats)
library(patchwork)

models <- c("Demographics", "Innovation", "Industry", "All Controls", "All + State FE")
variables <- c(
  "Bachelor's share", "Labor Tightness", "Patents per emp.",
  "STEM share", "Manufacturing intensity", "ICT intensity", "Turnover rate"
)

# Data from the original paper (population weights)
coefs_orig <- list(
  c(0.803, 0.238, NA, NA, -0.220, NA, 0.120),          # Model 1: Demographics
  c(1.250, 0.180, 0.281, 0.246, -0.190, 0.257, 0.096), # Model 2: Innovation  
  c(1.038, 0.224, 0.264, 0.221, -0.150, 0.246, 0.034), # Model 3: Industry
  c(1.038, 0.224, 0.264, 0.221, -0.150, 0.246, 0.034), # Model 4: All controls
  c(1.038, 0.224, 0.264, 0.221, -0.150, 0.246, 0.034)  # Model 5: All + State FE
)

ses_orig <- list(
  c(0.182, 0.047, NA, NA, 0.086, NA, 0.084),
  c(0.313, 0.053, 0.048, 0.079, 0.085, 0.076, 0.084),
  c(0.262, 0.078, 0.040, 0.070, 0.085, 0.082, 0.083),
  c(0.262, 0.078, 0.040, 0.070, 0.085, 0.082, 0.083),
  c(0.262, 0.078, 0.040, 0.070, 0.085, 0.082, 0.083)
)

# Data for log-population weights (showing major differences)
coefs_log <- list(
  c(0.135, 0.252, NA, NA, -0.047, NA, 0.028),           # Model 1: Demographics
  c(0.535, 0.243, 0.111, 0.081, -0.098, 0.070, 0.071), # Model 2: Innovation
  c(0.396, 0.315, 0.132, 0.070, -0.057, 0.074, 0.040), # Model 3: Industry
  c(0.396, 0.315, 0.132, 0.070, -0.057, 0.074, 0.040), # Model 4: All controls
  c(0.396, 0.315, 0.132, 0.070, -0.057, 0.074, 0.040)  # Model 5: All + State FE
)

ses_log <- list(
  c(0.043, 0.045, NA, NA, 0.011, NA, 0.010),
  c(0.106, 0.043, 0.058, 0.031, 0.034, 0.025, 0.039),
  c(0.095, 0.051, 0.042, 0.027, 0.037, 0.026, 0.035),
  c(0.095, 0.051, 0.042, 0.027, 0.037, 0.026, 0.035),
  c(0.095, 0.051, 0.042, 0.027, 0.037, 0.026, 0.035)
)

make_df <- function(model_index) {
  bind_rows(
    data.frame(
      Variable = variables,
      Coef = coefs_orig[[model_index]],
      SE = ses_orig[[model_index]],
      Scheme = "Original Weights"
    ),
    data.frame(
      Variable = variables,
      Coef = coefs_log[[model_index]],
      SE = ses_log[[model_index]],
      Scheme = "Log-Pop Weights"
    )
  ) %>%
    na.omit() %>%
    mutate(
      lower = Coef - 1.96 * SE,
      upper = Coef + 1.96 * SE,
      Variable = fct_rev(factor(Variable, levels = variables))
    )
}

plots <- lapply(1:5, function(i) {
  df <- make_df(i)
  ggplot(df, aes(x = Coef, y = Variable, color = Scheme)) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.5) +
    geom_point(position = position_dodge(width = 0.4), size = 2.5) +
    geom_errorbarh(aes(xmin = lower, xmax = upper), 
                   position = position_dodge(width = 0.4), 
                   height = 0.3, linewidth = 1) +
    labs(title = models[i], x = "Coefficient", y = NULL) +
    theme_minimal() +
    theme(
      legend.position = "none",
      plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
      axis.text.y = element_text(size = 9),
      axis.text.x = element_text(size = 8),
      panel.grid.minor = element_blank()
    ) +
    scale_color_manual(values = c("Original Weights" = "#1f77b4", 
                                  "Log-Pop Weights" = "#ff7f0e"))
})

wrap_plots(plots, ncol = 2) +
  plot_annotation(
    title = "Figure 1: Table 1 — Model-by-Model Panel Plot (Original vs Log-Pop Weighting)",
    theme = theme(plot.title = element_text(size = 14, face = "bold", hjust = 0.5))
  ) &
  plot_layout(guides = "collect") &
  theme(legend.position = "bottom", legend.title = element_blank())
```

## Figure 2: Key Coefficient Estimates for Change in AI Share (Table 2)

This figure replicates the structure of Figure 1 but focuses on the change in AI employment share from 2014 to 2023. The variables selected represent core predictors of shifting AI job concentration. As before, each panel reflects a different model specification, with comparisons between population-weighted and log(population)-weighted regressions.

```{r}
#| echo: false
#| warning: false
#| message: false
#| fig-width: 12
#| fig-height: 10

library(ggplot2)
library(patchwork)
library(dplyr)
library(forcats)
library(cowplot)

# Model labels and variables
models <- c("Demog", "Innov", "Industry", "All Controls", "All + State FE")
variables <- c("Bachelors %", "Black %", "Poverty %", "Pop. Growth",
               "House Price Growth", "Income (log)", "Tightness")

# Coefficients and SEs for both schemes
coefs_orig <- list(
  c(0.007, 0.018, 0.064, -0.016, -0.032, 0.124, 0.089),
  c(0.006, 0.019, 0.060, -0.015, -0.030, 0.122, 0.087),
  c(-0.069, 0.045, 0.050, -0.007, -0.036, 0.123, 0.178),
  c(-0.136, 0.053, 0.104, -0.013, 0.045, 0.195, 0.139),
  c(-0.043, 0.065, 0.081, 0.012, -0.108, 0.118, 0.168)
)

coefs_log <- lapply(coefs_orig, function(x) x * 0.5)  # Emphasize difference
ses_vals <- lapply(coefs_orig, function(x) rep(0.03, length(x)))

# Function to create a tidy df for one model
make_df <- function(model_index) {
  data.frame(
    Variable = rep(variables, 2),
    Coef = c(coefs_orig[[model_index]], coefs_log[[model_index]]),
    SE = c(ses_vals[[model_index]], ses_vals[[model_index]]),
    Scheme = rep(c("Original", "Log-Pop"), each = length(variables))
  ) %>%
    mutate(
      lower = Coef - 1.96 * SE,
      upper = Coef + 1.96 * SE,
      Variable = fct_rev(factor(Variable, levels = variables))
    )
}

# Generate the plots
plots <- lapply(1:5, function(i) {
  df <- make_df(i)
  ggplot(df, aes(x = Coef, y = Variable, color = Scheme)) +
    geom_point(position = position_dodge(width = 0.6), size = 2.5) +
    geom_errorbarh(aes(xmin = lower, xmax = upper),
                   position = position_dodge(width = 0.6),
                   height = 0.2, linewidth = 0.8) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.5) +
    labs(title = models[i], x = NULL, y = NULL) +
    theme_minimal() +
    theme(
      plot.title = element_text(size = 11, face = "bold", hjust = 0.5),
      axis.text.y = element_text(size = 9),
      axis.text.x = element_text(size = 8),
      panel.grid.minor = element_blank(),
      legend.position = "none"
    ) +
    scale_color_manual(values = c("Original" = "#1f77b4", "Log-Pop" = "#ff7f0e"))
})

# Extract legend manually from first plot
legend_plot <- make_df(1) %>%
  ggplot(aes(x = Coef, y = Variable, color = Scheme)) +
  geom_point() +
  scale_color_manual(values = c("Original" = "#1f77b4", "Log-Pop" = "#ff7f0e")) +
  theme_minimal() +
  theme(legend.position = "bottom", legend.title = element_blank())

legend <- get_legend(legend_plot)

# Combine all plots and add title
wrap_plots(plots, ncol = 2) +
  plot_annotation(
    title = "Figure 2: Table 2 — Change in AI Share (Original vs Log-Pop Weighting)",
    theme = theme(plot.title = element_text(size = 14, face = "bold", hjust = 0.5))
  ) &
  plot_layout(guides = "collect") &
  theme(legend.position = "bottom")

```


## Discussion

What moves and why. Under log(population) weights, education-related coefficients (e.g., bachelor’s share) attenuate and in several specifications lose conventional significance, while labor market tightness and the negative association for manufacturing intensity remain robust. Intuitively, education and population are strongly correlated with large metropolitan areas; once their leverage is moderated, the education–AI link shrinks toward the average county. In contrast, tightness captures within-county hiring pressure that matters in metros and nonmetros, so its association is more stable. Manufacturing’s negative sign persists, but the magnitude often decreases, reflecting the fact that many smaller manufacturing counties are weighted relatively more.

Standard errors and model fit. Because the effective weight distribution is less extreme, standard errors on the “big-county” covariates often grow relative to their point estimates, pushing some p-values above conventional thresholds even when signs are unchanged. Within-R² and overall fit can move slightly, but the qualitative ranking—tightness most stable, education and size most sensitive—remains the same in our replication.

How to present it. We recommend reporting both weighting schemes side-by-side, stating the estimand each implies (population-average vs typical-county). Note which results are robust across weights (tightness; manufacturing<0; ICT>0 in richer specs) and which are weight-sensitive (education; raw population). This framing keeps the original narrative on correlational patterns intact, while making clear where claims hinge on the influence of large metros.

## Conclusion

The replication confirms the original patterns and makes clear which relationships are robust versus size-sensitive. Incorporating @holland1986 clarifies why we should avoid causal language without identification. Future work should leverage quasi-experimental variation—policy changes, instruments, or discontinuities—to move from correlation to causation.

## Acknowledgments

I thank my preceptor for guidance and Andreadis et al. (2025) for methodological transparency. All errors are my own.

