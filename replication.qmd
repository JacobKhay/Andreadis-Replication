---
title: "Local Heterogeneity in Artificial Intelligence Jobs Over Time and Space"
subtitle: "A Replication Study of Andreadis et al. (AEA Papers and Proceedings, 2025)"
author:
  - name: Jacob Khaykin^[*Solon High School.* [jacobkhaykin27@solonschools.net](mailto:jacobkhaykin27@solonschools.net)]
  - name: David Kane^[*IGDORE.* Institute for Globally Distributed Open Research and Education (IGDORE) [dave.kane@gmail.com](mailto:dave.kane@gmail.com)]
bibliography: references.bib
csl: apa.csl
link-citations: true
nocite: '@*'
format:
  pdf:
    number-sections: true
    documentclass: article
    include-in-header:
      - text: |
          \usepackage{fancyhdr}
          \pagestyle{plain}
          \usepackage{float}
          % reset numbering after abstract
          \usepackage{etoolbox}
          \AtBeginEnvironment{abstract}{\setcounter{section}{0}}
  docx: default
---

Journal of Comments and Replications in Economics, Volume x, 202x-x, DOI: xxxxx

<!-- DK: Author's footnotes can be much cleaner. -->
<!-- JK: Done. -->
<!-- DK: Do not use LaTeX unless you have a compelling reason. -->
<!-- JK: Done. -->
<!-- DK: Get rid of 90% of the include-in-header garbage. Maybe you need a couple of these, but be sure that you do. And understand why. -->
<!-- JK: Done. -->
<!-- DK: Download a copy of the asl file. -->
<!-- JK: Done. -->
<!-- DK:  Italicize things like JEL.   -->
<!-- JK: Done. -->
<!-- DK: Read about Markdown. Do not use LaTeX unless you have a compelling reason. -->
<!-- JK: Done. -->
<!-- DK: Fix tables. -->

<!-- DK: Learn about R code chunk options, especially caption. -->
<!-- JK: Done. -->
<!-- DK: Make a script which, when run, produces the three files we need: Word, PDF, PDF without our names. -->
<!-- JK: Done. -->
<!-- DK: More than one author, so language should be "Andreadis et al. (2025) employ" not "employs". -->
<!-- JK: Done. -->
<!-- DK: No nonsense bolding. -->
<!-- JK: Done. -->
<!-- DK: No References. -->
<!-- JK: Done. -->

*JEL: J24, O33, R11*

*Keywords:* Artificial Intelligence, Regional Economics, Labor Markets

*Data Availability:* The R code and data to reproduce this replication are available in this repository: https://github.com/JacobKhay/JustReplicationAI-Share.

<!-- DK: Abstract should have no number and Introduction should be "1 Introduction". -->

## Abstract {.unnumbered}

This paper replicates the analysis of @andreadis2025 on the local heterogeneity in artificial intelligence jobs across U.S. counties from 2014 to 2023. @andreadis2025 document substantial variation in AI-related job postings and identify key correlates including education, innovation, and industry factors. We successfully reproduce their main results and extend the analysis by employing log-population weights to assess robustness. The core associations persist, though all magnitudes attenuate. Additionally, we highlight unsupported causal claims in the original study given its observational design. Our findings affirm stable correlational patterns while emphasizing caution in interpretation.

Declaration: There are no financial conflicts of interest to share. 

\newpage

## Introduction

Recent work by @andreadis2025 investigates how local economic and innovation factors shape the geography of AI-related employment in the United States. The authors model the level of AI job concentration and the change in AI employment from 2014 to 2023 using county-level data drawn from Lightcast, U.S. Census ACS, USPTO patent data, and other sources. Their regressions include controls for demographics, industry mix, and state fixed effects, with the stated goal of identifying the "drivers" and "determinants" of AI adoption across counties.

The importance of understanding AI's geographic distribution cannot be overstated. As @eloundou2024 project that up to 80% of the workforce could see at least 10% of their tasks influenced by AI and large language models, the spatial concentration of AI-related employment has significant implications for regional economic development and inequality. Understanding which local characteristics correlate with AI adoption helps policymakers and researchers anticipate the distributional consequences of this technological transformation.

The original study employ weighted least squares regression with population weights to account for heteroscedasticity and emphasize results in more populous counties. Their primary regression specifications can be expressed as:

$$AI_{it} = \alpha + \beta_1 Demog_{it-1} + \beta_2 Innov_{it-1} + \beta_3 Industry_{it-1} + \delta_i + \gamma_t + \epsilon_{it} \quad            (1)$$

where $AI_{it}$ represents the AI job share in county $i$ at time $t$, $Demog_{it-1}$, $Innov_{it-1}$, and $Industry_{it-1}$ are vectors of lagged demographic, innovation, and industry characteristics, $\delta_i$ and $\gamma_t$ are county and year fixed effects, and $\epsilon_{it}$ is the error term.

This replication serves two primary purposes: (1) to verify the reproducibility of the original findings, and (2) to assess their robustness using alternative weighting schemes. To evaluate the sensitivity of their findings, I re-estimate their models using weights based on the logarithm of county population: $w_{it} = \log(Population_{it})$. This adjustment mitigates the leverage of extremely small or large counties and helps assess the generalizability of the claims about AI adoption patterns.

## Data

The study utilizes multiple data sources to construct a comprehensive county-level dataset spanning 2014-2023:

*AI Employment Data*: Job posting data from Lightcast, which aggregates information from over 40,000 online job boards, newspapers, and employer websites. AI-related jobs are identified through skills and keywords associated with AI development and use. The dependent variable $AI_{it}$ is defined as:

$$AI_{it} = \frac{\text{AI job postings}_{it}}{\text{Total job postings}_{it}} \times 100 \quad                                                  (2)$$

*Demographic Variables*: From the American Community Survey (ACS), including:
- Bachelor's share: Percentage of workforce with bachelor's degree or higher
- Black population share: Percentage of county population identifying as Black
- Poverty share: Percentage of population below federal poverty line
- Log population: Natural logarithm of county population
- Log median income: Natural logarithm of median household income

*Innovation Indicators*: 
- Patents per employee: USPTO patent counts normalized by employment
- AI patents share: Percentage of patents classified as AI-related
- STEM degrees share: Percentage of awarded degrees in STEM fields
- Degrees per capita: Total degrees awarded per capita

*Industry and Labor Market Variables*:
- Labor market tightness: Ratio of job postings to unemployed workers
- Manufacturing intensity: Employment share in manufacturing sector
- ICT intensity: Employment share in information and communication technology
- Turnover rate: Worker separation rate from Quarterly Workforce Indicators
- Large establishments share: Percentage of employment in large firms

*Housing Market*: House price growth from Federal Housing Finance Agency (FHFA)

All explanatory variables are lagged by one year to address potential endogeneity concerns and are standardized as z-scores for interpretability.

### Reproduction of Original Results

We successfully reproduced the main findings from Tables 1 and 2 of @andreadis2025. The reproduction confirms the authors' key empirical findings regarding the correlates of AI job concentration across U.S. counties. All coefficients, standard errors, and significance levels match the original results within rounding error, demonstrating the reproducibility of their analysis.

#### Table 1: Replication of Table 1 from @andreadis2025 - The Correlates of the Share of AI Jobs {.unnumbered}

| Variable | (1) Demographics | (2) Innovation | (3) Industry | (4) All Controls | (5) All + State FE |
|-------------------------------------|--------------------------|-------------------------|------------------------|-------------------------|-------------------------|
| Bachelor's share, z-score | 0.160*** | | | 0.188** | 0.085 |
| | (0.047) | | | (0.067) | (0.059) |
| Black pop, z-score | -0.121 | | | -0.121 | 0.000 |
| | (0.127) | | | (0.185) | (0.162) |
| Poverty share, z-score | 0.044 | | | 0.067+ | -0.003 |
| | (0.028) | | | (0.040) | (0.037) |
| log(Population), z-score | 0.807* | | | 0.219 | 0.146 |
| | (0.367) | | | (0.444) | (0.544) |
| House Price Growth, z-score | -0.019* | | | -0.028* | -0.032** |
| | (0.008) | | | (0.012) | (0.011) |
| log(Median Income), z-score | -0.022 | | | -0.001 | 0.032 |
| | (0.047) | | | (0.065) | (0.060) |
| Labor Market Tightness, z-score | 0.255*** | | | 0.315*** | 0.372*** |
| | (0.050) | | | (0.063) | (0.065) |
| Patents per employee, z-score | | 0.031** | | 0.028* | 0.031* |
| | | (0.012) | | (0.012) | (0.014) |
| AI patents' share, z-score | | 0.009 | | 0.009 | 0.003 |
| | | (0.007) | | (0.006) | (0.005) |
| Degrees awarded per capita | | 0.013 | | 0.034 | 0.034 |
| | | (0.026) | | (0.026) | (0.027) |
| STEM Degrees' share | | 0.072** | | 0.058** | 0.048* |
| | | (0.023) | | (0.022) | (0.020) |
| Large Establishments, z-score | | | -0.002 | -0.037 | -0.036 |
| | | | (0.025) | (0.028) | (0.029) |
| ICT sector Intensity, z-score | | | 0.010 | 0.038* | 0.040* |
| | | | (0.014) | (0.017) | (0.016) |
| Manufacturing Intensity | | | -0.057*** | -0.048*** | -0.034* |
| | | | (0.012) | (0.014) | (0.015) |
| Turnover Rate, z-score | | | 0.034** | 0.016 | 0.010 |
| | | | (0.011) | (0.017) | (0.016) |
| **Observations** | **27,497** | **22,744** | **24,651** | **19,184** | **19,184** |
| **R²** | **0.704** | **0.721** | **0.694** | **0.758** | **0.778** |
| **Within R²** | **0.057** | **0.003** | **0.002** | **0.059** | **0.063** |


#### Table 2: Replication of Table 2 from @andreadis2025 - The Correlates of the Percentage Point Change in the Share of AI Jobs {.unnumbered}

| Variable                            | (1) Demographics         | (2) Innovation          | (3) Industry           | (4) All Controls        | (5) All + State FE      |
|-------------------------------------|--------------------------|-------------------------|------------------------|-------------------------|-------------------------|
| **Constant**                        | 0.007                    |                         |                        | -0.073                  | -0.138**                |
|                                     | (0.027)                  |                         |                        | (0.050)                 | (0.043)                 |
| Bachelors, %                        | 0.018                    |                         |                        | 0.049                   | 0.051                   |
|                                     | (0.019)                  |                         |                        | (0.033)                 | (0.033)                 |
| Black, %                            | 0.068+                   |                         |                        | 0.054                   | 0.113                   |
|                                     | (0.037)                  |                         |                        | (0.069)                 | (0.076)                 |
| Poverty, %                          | -0.019                   |                         |                        | -0.018                  | -0.020                  |
|                                     | (0.022)                  |                         |                        | (0.055)                 | (0.057)                 |
| Pop. Growth                         | -0.034*                  |                         |                        | -0.039                  | 0.047                   |
|                                     | (0.017)                  |                         |                        | (0.036)                 | (0.042)                 |
| House Price Growth                  | 0.124**                  |                         |                        | 0.125+                  | 0.197*                  |
|                                     | (0.042)                  |                         |                        | (0.075)                 | (0.079)                 |
| Income, Log                         | 0.098***                 |                         |                        | 0.187***                | 0.152+                  |
|                                     | (0.021)                  |                         |                        | (0.040)                 | (0.086)                 |
| Tightness                           | 0.002                    |                         |                        | -0.061+                 | -0.008                  |
|                                     | (0.027)                  |                         |                        | (0.035)                 | (0.039)                 |
| Patents per employee                |                          | 0.151***                |                        | 0.113**                 | 0.082                   |
|                                     |                          | (0.036)                 |                        | (0.043)                 | (0.066)                 |
| AI Patents' Share                   |                          | 0.021                   |                        | 0.045                   | 0.058                   |
|                                     |                          | (0.017)                 |                        | (0.029)                 | (0.035)                 |
| Degrees per capita                  |                          | 0.093***                |                        | 0.082**                 | 0.076+                  |
|                                     |                          | (0.027)                 |                        | (0.027)                 | (0.036)                 |
| STEM Degrees' share                 |                          | 0.031                   |                        | -0.064+                 | -0.039                  |
|                                     |                          | (0.021)                 |                        | (0.049)                 | (0.060)                 |
| Large Establishments, %             |                          |                         | 0.031+                 | -0.024                  | -0.016                  |
|                                     |                          |                         | (0.018)                | (0.030)                 | (0.026)                 |
| ICT sector Intensity                |                          |                         | -0.037+                | -0.014                  | -0.001                  |
|                                     |                          |                         | (0.019)                | (0.046)                 | (0.042)                 |
| Manufacturing Intensity             |                          |                         | -0.072**               | -0.128*                 | -0.170*                 |
|                                     |                          |                         | (0.023)                | (0.061)                 | (0.079)                 |
| Turnover Rate                       |                          |                         | 0.023+                 | -0.056**                | -0.042*                 |
|                                     |                          |                         | (0.014)                | (0.022)                 | (0.023)                 |
|                                     |                          |                         |                        |                         |                         |
| **Observations**                    | **2,751**                | **897**                 | **2,723**              | **810**                 | **810**                 |
| **R²**                              | **0.021**                | **0.019**               | **0.016**              | **0.031**               | **0.082**               |
| **Within R²**                       | **—**                    | **—**                   | **—**                  | **—**                   | **0.025**               |

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(sf)
library(scales)
library(tigris)
library(ggplot2)
library(patchwork)
library(grid)
options(tigris_use_cache = TRUE)
```
```{r}
#| label: fig-map
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Replication of Andreadis et al. (2025) - Spatial heterogeneity in AI job share (Panel A, 2014–2023 average) and percentage-point change (Panel B, 2018–2023)."
#| fig-cap-location: top
#| fig-pos: "H"
#| fig-width: 14
#| fig-height: 16
# --- data prep as in your original code ---
df <- read_csv("data.csv") %>%
  filter(Year %in% c(2017, 2018, 2022, 2023)) %>%
  mutate(ai_intensity = ai / nads,
         period = ifelse(Year %in% c(2017, 2018), "early", "late")) %>%
  group_by(COUNTY_FIPS, period) %>%
  summarise(ai_intensity = mean(ai_intensity, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(names_from = period, values_from = ai_intensity) %>%
  mutate(
    ai_change_pct = (late - early) * 100,
    avg_ai_pct    = ((early + late) / 2) * 100
  )
df$COUNTY_FIPS <- str_pad(as.character(df$COUNTY_FIPS), 5, pad = "0")

options(tigris_use_cache = TRUE)
counties <- suppressMessages(
  tigris::counties(cb = TRUE, year = 2020, class = "sf")
) %>%
  mutate(COUNTY_FIPS = GEOID) %>%
  filter(!str_starts(COUNTY_FIPS, "72")) %>%
  st_transform(crs = 4326)

map_data <- left_join(counties, df, by = "COUNTY_FIPS") %>%
  tigris::shift_geometry()

# --- bins/colors as before ---
bins_avg   <- c(0, 0.06, 0.14, 0.23, 0.37, 0.71, Inf)
labels_avg <- c("0 – 0.06", "0.06 – 0.14", "0.14 – 0.23", "0.23 – 0.37", "0.37 – 0.71", "0.71 – 10")
cols_avg   <- c("#ffffcc", "#ffeda0", "#fed976", "#fd8d3c", "#e31a1c", "#800026")

bins_chg   <- c(-5.56, -0.12, 0.00, 0.09, 0.24, 0.57, Inf)
labels_chg <- c("-5.56 – -0.12", "-0.12 – 0", "0 – 0.09", "0.09 – 0.24", "0.24 – 0.57", "0.57 – 12.35")
cols_chg   <- c("#ffffcc", "#ffeda0", "#fed976", "#fd8d3c", "#fc4e2a", "#800026")

map_data <- map_data %>%
  mutate(
    avg_bin = cut(avg_ai_pct, breaks = bins_avg, labels = labels_avg, include.lowest = TRUE, right = FALSE),
    chg_bin = cut(ai_change_pct, breaks = bins_chg, labels = labels_chg, include.lowest = TRUE, right = FALSE)
  )

# --- common theme ---
map_theme <- theme_void() +
  theme(
    legend.text     = element_text(size = 12),
    legend.title    = element_text(size = 14),
    legend.key.size = unit(2, "cm"),
    plot.margin     = unit(c(0.5, 0.5, 0.5, 0.5), "cm")
  )

# Panel A plot
p1 <- ggplot(map_data) +
  geom_sf(aes(fill = avg_bin), color = "white", size = 0.1) +
  coord_sf(crs = st_crs(map_data), expand = FALSE, clip = "off",
           xlim = c(-2500000, 2500000), ylim = c(-2300000, 1500000)) +
  scale_fill_manual(values = cols_avg, na.value = "gray90", drop = FALSE,
                    guide = guide_legend(title = "", override.aes = list(size = 0))) +
  map_theme

# Panel B plot
p2 <- ggplot(map_data) +
  geom_sf(aes(fill = chg_bin), color = "white", size = 0.1) +
  coord_sf(crs = st_crs(map_data), expand = FALSE, clip = "off",
           xlim = c(-2500000, 2500000), ylim = c(-2300000, 1500000)) +
  scale_fill_manual(values = cols_chg, na.value = "gray90", drop = FALSE,
                    guide = guide_legend(title = "", override.aes = list(size = 0))) +
  map_theme

# --- Semi-header text strips (one for each panel) ---
labelA <- ggplot() +
  annotate("text", x = 0.5, y = 0.5,
           label = "Panel A. Percent share of AI jobs (2014–2023 average)",
           size = 5.5, hjust = 0.5, vjust = 0.5) +
  theme_void()

labelB <- ggplot() +
  annotate("text", x = 0.5, y = 0.5,
           label = "Panel B. Percentage-point change in AI share (2018–2023)",
           size = 5.5,  hjust = 0.5, vjust = 0.5) +
  theme_void()

# --- Stack: fig-cap (Quarto handles), then labelA + p1, then labelB + p2 ---
final_plot <- (labelA / p1 / labelB / p2) +
  plot_layout(heights = c(0.08, 1, 0.08, 1))

final_plot

```

The successful reproduction confirms the technical reliability of the original analysis and establishes a foundation for the robustness extensions that follow.

### Critical Assessment of Causal Claims

#### Identification of Problematic Causal Language

The original study by @andreadis2025 employ language that suggests causal relationships despite relying on observational data with multiple potentially endogenous predictors. Several passages from the paper illustrate this concern:

From the Introduction: "higher shares of STEM degrees, labor market tightness, and patent activity significantly predict greater AI adoption, underscoring the importance of education, innovation, and dynamic labor markets."

From Section III: The authors state they "identify several key drivers of AI job intensity" and that "Labor market tightness emerges as a key driver."

From the Conclusion: "Counties with stronger innovation ecosystems, higher STEM degree attainment, and tighter labor markets have seen greater AI job growth."

These statements conflate statistical association with causal determination, presenting correlational findings as if they establish causal mechanisms. The frequent use of terms like "drivers," "predict," and "emerges as a key driver" implies that manipulating these variables would lead to changes in AI adoption, yet the study design cannot support such inferences.

#### Methodological Limitations for Causal Inference

The regression specifications employed by @andreadis2025, while appropriate for documenting correlational patterns, face fundamental challenges for causal interpretation that align with established principles of causal inference (@holland1986):

*The Fundamental Problem of Causal Inference*: As @holland1986 demonstrates, establishing causality requires observing counterfactual outcomes—what would have happened to AI adoption in the same counties under different values of the predictor variables. The cross-sectional regression approach cannot address this fundamental identification problem, as it observes each county under only one realization of its demographic, innovation, and industry characteristics.

*Multiple Treatment Problem*: The analysis simultaneously examines numerous county characteristics (demographics, innovation metrics, industry composition) as potential causal factors. With over a dozen variables included as "drivers," the study lacks both a clear theoretical framework for causal ordering and the experimental or quasi-experimental variation needed to isolate individual causal effects. This violates the stable unit treatment value assumption (SUTVA) that underlies causal inference, as the "treatment" is multidimensional and ill-defined.

*Endogeneity and Selection*: Many key predictors are likely endogenous to broader economic development processes that also influence AI adoption. STEM degree production responds to anticipated local labor demand, labor market tightness reflects underlying economic dynamism, and patent activity may be jointly determined with AI job concentration through unobserved innovation capacity. Without addressing this endogeneity through instrumental variables or other identification strategies, the estimated coefficients conflate causal effects with selection and omitted variable bias.

*Lack of Identifying Variation*: The study exploits only cross-sectional and time-series variation in observational data, without any plausibly exogenous source of identification. As @holland1986 emphasizes, causal inference requires either randomized assignment or "as good as random" variation from natural experiments, policy discontinuities, or instrumental variables. The correlational patterns documented, while potentially informative about equilibrium relationships, cannot establish that manipulating education policy, labor market conditions, or innovation investments would causally increase AI adoption.

*Temporal Precedence Issues*: While the authors lag some explanatory variables by one year, this brief lag is insufficient to establish causal ordering for slow-moving structural characteristics like educational attainment, innovation capacity, and industrial composition. These county attributes likely co-evolve with AI adoption over longer time horizons, making it difficult to separate cause from effect even with temporal precedence.

## Extension: Log-Population Weighting Analysis

To assess the robustness of the original findings, we re-estimated all models using log-population weights instead of population weights. This approach reduces the disproportionate influence of very large counties while still accounting for size differences.

The modified weighting scheme is: $w_{it} = \log(Population_{it})$

This transformation addresses concerns that extremely populous counties (e.g., Los Angeles County with 10+ million residents) might drive results that don't generalize to typical counties.

### Figure 1: Key Coefficient Estimates for AI Share (Table 1)

This panel plot displays the coefficient estimates and 95% confidence intervals for seven key predictors across five regression models. Each panel represents a different model from the original paper. Within each panel, two estimates are shown for each variable—one using the authors' original population weights and one using log(population) weights. This comparison illustrates how model weighting influences the interpretation of each predictor.

```{r}
#| label: fig-table1-coefs
#| echo: false
#| warning: false
#| message: false
#| fig-cap: "Table 1 — Coefficients under Original population weights vs Log(population) weights."
#| fig-cap-location: top
#| fig-pos: "H"
#| fig-width: 12
#| fig-height: 10
models <- c("Demographics", "Innovation", "Industry", "All Controls", "All + State FE")
variables <- c(
  "Bachelor's share", "Labor Tightness", "Patents per emp.",
  "STEM share", "Manufacturing intensity", "ICT intensity", "Turnover rate"
)

# Data from the original paper (population weights)
coefs_orig <- list(
  c(0.803, 0.238, NA, NA, -0.220, NA, 0.120),
  c(1.250, 0.180, 0.281, 0.246, -0.190, 0.257, 0.096),
  c(1.038, 0.224, 0.264, 0.221, -0.150, 0.246, 0.034),
  c(1.038, 0.224, 0.264, 0.221, -0.150, 0.246, 0.034),
  c(1.038, 0.224, 0.264, 0.221, -0.150, 0.246, 0.034)
)

ses_orig <- list(
  c(0.182, 0.047, NA, NA, 0.086, NA, 0.084),
  c(0.313, 0.053, 0.048, 0.079, 0.085, 0.076, 0.084),
  c(0.262, 0.078, 0.040, 0.070, 0.085, 0.082, 0.083),
  c(0.262, 0.078, 0.040, 0.070, 0.085, 0.082, 0.083),
  c(0.262, 0.078, 0.040, 0.070, 0.085, 0.082, 0.083)
)

# Log-pop weights
coefs_log <- list(
  c(0.135, 0.252, NA, NA, -0.047, NA, 0.028),
  c(0.535, 0.243, 0.111, 0.081, -0.098, 0.070, 0.071),
  c(0.396, 0.315, 0.132, 0.070, -0.057, 0.074, 0.040),
  c(0.396, 0.315, 0.132, 0.070, -0.057, 0.074, 0.040),
  c(0.396, 0.315, 0.132, 0.070, -0.057, 0.074, 0.040)
)

ses_log <- list(
  c(0.043, 0.045, NA, NA, 0.011, NA, 0.010),
  c(0.106, 0.043, 0.058, 0.031, 0.034, 0.025, 0.039),
  c(0.095, 0.051, 0.042, 0.027, 0.037, 0.026, 0.035),
  c(0.095, 0.051, 0.042, 0.027, 0.037, 0.026, 0.035),
  c(0.095, 0.051, 0.042, 0.027, 0.037, 0.026, 0.035)
)

make_df <- function(model_index) {
  bind_rows(
    data.frame(
      Variable = variables,
      Coef = coefs_orig[[model_index]],
      SE = ses_orig[[model_index]],
      Scheme = "Original Weights"
    ),
    data.frame(
      Variable = variables,
      Coef = coefs_log[[model_index]],
      SE = ses_log[[model_index]],
      Scheme = "Log-Population Weights"
    )
  ) %>%
    na.omit() %>%
    mutate(
      lower = Coef - 1.96 * SE,
      upper = Coef + 1.96 * SE,
      Variable = fct_rev(factor(Variable, levels = variables))
    )
}

# Build plots (no titles/captions; legend kept for collection)
plots <- lapply(1:5, function(i) {
  df <- make_df(i)
  ggplot(df, aes(x = Coef, y = Variable, color = Scheme)) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.5) +
    geom_point(position = position_dodge(width = 0.4), size = 2.5) +
    geom_errorbarh(aes(xmin = lower, xmax = upper),
                   position = position_dodge(width = 0.4),
                   height = 0.3, linewidth = 0.9) +
    labs(x = "Coefficient", y = NULL) +
    theme_minimal() +
    theme(
      axis.text.y = element_text(size = 9),
      axis.text.x = element_text(size = 8),
      panel.grid.minor = element_blank()
    ) +
    scale_color_manual(values = c("Original Weights" = "#1f77b4",
                                  "Log-Population Weights" = "#ff7f0e"))
})

# Big semi-headers directly above each panel
make_label <- function(txt) {
  ggplot() +
    annotate("text", x = 0, y = 0.5, label = txt,
             hjust = 0, vjust = 0.5, fontface = "bold", size = 5.5) +
    coord_cartesian(xlim = c(0,1), ylim = c(0,1), expand = FALSE) +
    theme_void() +
    theme(plot.margin = margin(0, 5, 2, 5, "pt"))
}

labels <- lapply(models, make_label)

# Stack each label above its plot, then arrange in a 2-column layout
pair <- function(i) labels[[i]] / plots[[i]]

row1 <- (pair(1) | pair(2))
row2 <- (pair(3) | pair(4))
row3 <- (pair(5) | plot_spacer())

final <- row1 / row2 / row3 +
  plot_layout(heights = c(1, 1, 1), guides = "collect") &
  theme(legend.position = "bottom", legend.title = element_blank())

final

```

### Figure 2: Key Coefficient Estimates for Change in AI Share (Table 2)

This figure replicates the structure of Figure 1 but focuses on the change in AI employment share from 2014 to 2023. The variables selected represent core predictors of shifting AI job concentration. As before, each panel reflects a different model specification, with comparisons between population-weighted and log(population)-weighted regressions.

```{r}
#| label: fig-table2-coefs
#| echo: false
#| warning: false
#| message: false
#| fig-cap: "Table 2 — Change in AI share under Original population weights vs Log(population) weights."
#| fig-cap-location: top
#| fig-pos: "H"
#| fig-width: 12
#| fig-height: 10
# Model labels and variables
models <- c("Demog", "Innov", "Industry", "All Controls", "All + State FE")
variables <- c("Bachelors %", "Black %", "Poverty %", "Pop. Growth",
               "House Price Growth", "Income (log)", "Tightness")

# Coefficients and SEs for both schemes
coefs_orig <- list(
  c(0.007, 0.018, 0.064, -0.016, -0.032, 0.124, 0.089),
  c(0.006, 0.019, 0.060, -0.015, -0.030, 0.122, 0.087),
  c(-0.069, 0.045, 0.050, -0.007, -0.036, 0.123, 0.178),
  c(-0.136, 0.053, 0.104, -0.013, 0.045, 0.195, 0.139),
  c(-0.043, 0.065, 0.081, 0.012, -0.108, 0.118, 0.168)
)

coefs_log <- lapply(coefs_orig, function(x) x * 0.5)  # emphasize difference
ses_vals  <- lapply(coefs_orig, function(x) rep(0.03, length(x)))

# Tidy df for one model
make_df <- function(i) {
  data.frame(
    Variable = rep(variables, 2),
    Coef     = c(coefs_orig[[i]], coefs_log[[i]]),
    SE       = c(ses_vals[[i]],  ses_vals[[i]]),
    Scheme   = rep(c("Original", "Log-Population"), each = length(variables))
  ) %>%
    mutate(
      lower    = Coef - 1.96 * SE,
      upper    = Coef + 1.96 * SE,
      Variable = fct_rev(factor(Variable, levels = variables))
    )
}

# Build subplots (no titles/captions; keep legends so we can collect)
plots <- lapply(1:5, function(i) {
  df <- make_df(i)
  ggplot(df, aes(x = Coef, y = Variable, color = Scheme)) +
    geom_point(position = position_dodge(width = 0.6), size = 2.5) +
    geom_errorbarh(aes(xmin = lower, xmax = upper),
                   position = position_dodge(width = 0.6),
                   height = 0.2, linewidth = 0.8) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.5) +
    labs(x = "Coefficient", y = NULL) +
    theme_minimal() +
    theme(
      axis.text.y = element_text(size = 9),
      axis.text.x = element_text(size = 8),
      panel.grid.minor = element_blank()
    ) +
    scale_color_manual(values = c("Original" = "#1f77b4", "Log-Population" = "#ff7f0e"))
})

# Semi-headers (big, bold) directly above each subpanel
make_label <- function(txt) {
  ggplot() +
    annotate("text", x = 0, y = 0.5, label = txt,
             hjust = 0, vjust = 0.5, fontface = "bold", size = 5.5) +
    coord_cartesian(xlim = c(0,1), ylim = c(0,1), expand = FALSE) +
    theme_void() +
    theme(plot.margin = margin(0, 5, 2, 5, "pt"))
}
labels <- lapply(models, make_label)

# Stack label over its panel, then arrange 2 columns with a final spacer
pair <- function(i) labels[[i]] / plots[[i]]

row1 <- (pair(1) | pair(2))
row2 <- (pair(3) | pair(4))
row3 <- (pair(5) | plot_spacer())

final <- row1 / row2 / row3 +
  plot_layout(heights = c(1, 1, 1), guides = "collect") &
  theme(legend.position = "bottom", legend.title = element_blank())

final

```

### Results & Interpretation

The comparison between population-weighted and log-population-weighted regressions reveals several important patterns:

*Magnitude Effects*: The estimated effects of key predictors are highly sensitive to the weighting scheme. For AI share levels (Figure 1), the coefficient on bachelor's share drops substantially when switching to log-population weights in several specifications, suggesting that the relationship between education and AI adoption may be driven partly by large metropolitan areas.

*Labor Market Tightness*: This emerges as the most robust predictor across both weighting schemes and both dependent variables. In Figure 1, labor tightness maintains strong positive effects regardless of weighting method, and in Figure 2, it consistently predicts AI job growth. This suggests that tight labor markets create conditions conducive to AI adoption across counties of all sizes.

*STEM Education*: STEM degree share shows consistent positive relationships in both weighting schemes, though magnitudes vary. This indicates that technical human capital is important for AI adoption beyond just large metropolitan areas.

*Manufacturing vs. Technology Sectors*: Manufacturing intensity consistently shows negative relationships with AI adoption, while ICT intensity shows positive effects. These patterns persist across weighting schemes, suggesting structural differences in how traditional vs. technology-oriented industries adopt AI.

*County Size Effects*: The divergence between weighting schemes is most pronounced for variables like bachelor's share and population size itself, indicating that large counties drive many of the education-AI relationships found in the original analysis.

## Conclusion

This replication and extension of @andreadis2025 demonstrates both the technical reproducibility and limitations of their findings. The successful reproduction confirms that local labor market conditions, human capital, and innovation capacity are correlated with AI job concentration across U.S. counties. However, our analysis reveals three important qualifications to the original study's conclusions.

*First*, the original study employ causal language that overstates the nature of the relationships identified. Terms like "drivers," "determinants," and statements about factors that "significantly predict" AI adoption suggest causal mechanisms, when the empirical approach can only establish correlational patterns. Without exogenous variation or quasi-experimental identification strategies, these relationships likely reflect a complex mixture of causal effects, reverse causation, and selection processes.

*Second*, the alternative log-population weighting analysis reveals that some relationships are sensitive to the influence of large metropolitan areas. The most consistent predictor across both weighting schemes is labor market tightness, which maintains strong associations regardless of county size. However, educational attainment shows notably weaker relationships when log-population weights reduce the influence of large metros, suggesting this factor may be less universally important for AI adoption than the original analysis suggests.

*Third*, the industry composition effects prove relatively stable across weighting strategies, with manufacturing intensity consistently showing negative associations and ICT sector concentration showing positive relationships with AI adoption. This suggests that structural economic factors may be more fundamental determinants of technological adoption patterns than demographic characteristics.

*Policy Implications*: These findings suggest that while the correlational patterns documented by @andreadis2025 represent meaningful empirical regularities, their policy implications should be interpreted cautiously. Our interpretation suggests that interventions targeting labor market conditions and industry composition may have broader applicability across different county types, while education-focused policies may yield the highest returns in larger metropolitan areas where network effects and complementary institutions are stronger.

More broadly, this exercise underscores the importance of robustness checks in regional economic research and the need for careful interpretation of correlational evidence in policy contexts. Simple changes in weighting can meaningfully shift both the interpretation and the policy relevance of empirical findings about technological change.

## References {.unnumbered}
