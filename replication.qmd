---
title: "Local Heterogeneity in Artificial Intelligence Jobs Over Time and Space"
subtitle: "A Replication Study of Andreadis et al. (AEA Papers and Proceedings, 2025)"
author:
  - name: Jacob Khaykin^[Solon High School, [jacobkhaykin27@solonschools.net](mailto:jacobkhaykin27@solonschools.net)]
  - name: David Kane^[New College of Florida, [dakane@ncf.edu](mailto:dakane@ncf.edu)]
bibliography: references.bib
csl: aea.csl
link-citations: true
nocite: '@*'
format:
  pdf:
    keep-tex: true
    number-sections: true
    documentclass: article
    geometry: 
      - margin=1in
    include-in-header:
      - text: |
          \usepackage{url}
          \urlstyle{same}
          \def\UrlBreaks{\do\/\do-\do.\do:}
          \usepackage{float}
          \usepackage{array}
          \usepackage{siunitx}
          \usepackage{dcolumn}
          \newcolumntype{d}[1]{D{.}{.}{#1}}
          % reset numbering after abstract
          \usepackage{etoolbox}
          \AtBeginEnvironment{abstract}{\setcounter{section}{0}}
          % Custom table row spacing
          \renewcommand{\arraystretch}{0.9}
          % Configure siunitx for modern versions (>=3)
          \sisetup{
            detect-all,
            input-symbols = (),
            input-signs = + -,
            table-align-text-pre = false,
            table-number-alignment = center,
            table-format = +1.4,
            group-digits = false,
            table-space-text-pre = ***,
            table-space-text-post = ***,
            mode = math
          }
  docx: default
---


```{r}
#| label: setup
#| include: false
# Load all required packages
library(tidyverse)
library(fixest)
library(modelsummary)
library(kableExtra)
library(sf)
library(scales)
library(tigris)
library(ggplot2)
library(patchwork)
library(grid)
library(usmap)
library(viridis)
library(dplyr)
library(forcats)

# Set options
options(tigris_use_cache = TRUE)
```

<!-- DK:  -->


 <!-- Study the paper more closely, so you understand what it happening. Don't understand? Ask AI. Or ask me! -->

<!-- Work on Map 2. Bet that the percentage change should be:

(AI jobs in 2023 -  AI jobs in 2018) / AI jobs in 2018 -->


<!-- Open issue: Is their method for getting down to 24,000 rows for the regressions sensible? How much do results change if we fix this process? Worth doing? -->

<!-- The written portion of the paper must report all the most important aspects of the table, without assuming that the reader has looked at the table or the notes to the table. -->

<!-- All write up in terms of comparing two groups, e.g., counties with average home price compared with counties with home price growth one standard deviation higher. The second group of counties has AI job percentage with is  1.5% (?) lower. Put in section 3. -->

<!-- Does the coefficient of Bachelors Share in the first regression mean 19% higher or 0.19% higher, when comparing counties that differ by one standard deviation in Bachelor's Share. 0.19% higher, wrote that in the notes and in section 3.-->


*JEL: J24, O33, R11*

*Keywords:* Artificial Intelligence, Regional Economics, Labor Markets

*Data Availability:* The R code and data to reproduce this replication are available in this repository: https://github.com/JacobKhay/Andreadis-Replication.


## Abstract {.unnumbered}

We partially replicate @andreadis2025, henceforth, AKCLM, on the correlation between the level and growth of artificial intelligence employment, on one hand, and a collection of factors involving education, innovation, and industry across U.S. counties from 2014 to 2023. We successfully reproduce some of their results and extend the analysis by using log-population weights to assess robustness. The core associations persist, though almost all magnitudes shrink. We also critique unsupported, given its observational design, causal claims made by AKCLM.

Declaration: There are no financial conflicts of interest to share. 

\newpage

## Introduction

This paper replicates the analysis of @andreadis2025 on the correlation between the level and growth artificial intelligence employment openings and education, innovation, and industry factors across U.S. counties from 2014 to 2023. Understanding where AI employment emerges and how it spreads is important for both researchers and policymakers, since the rise of AI has the potential to reshape regional economies [@acemoglu2024; @brynjolfsson2021], alter the demand for skills [@autor2015; @acemoglu2019], and shift patterns of innovation [@babina2024]. County-level variation offers a granular perspective on how these transformations take hold across the United States.

The outcome of interest is the share of AI-related job postings at the county-year level. This measure captures both the absolute level of AI employment opportunities and how those opportunities evolve over time. Tracking these dynamics provides insight into which regions gain early access to AI-driven growth and which lag behind.

The original study links county-level AI employment to several explanatory factors. Education is captured by the share of adults with a college degree, innovation by local patenting activity [@giczy2022], and industry factors by the composition of employment across sectors. Together, these variables represent structural characteristics that might condition whether a region becomes a hub for AI-related work.

AKCLM find that AI employment is strongly correlated with higher educational attainment, more innovation activity, and certain industry profiles. These associations are robust across specifications, suggesting that counties with strong human capital, innovative capacity, and aligned industries are more likely to attract AI jobs. The results highlight structural divides in access to AI employment opportunities across the country.

We reproduce these results and then extend the analysis by applying log-population weights. This alternative weighting scheme reduces the influence of the largest counties while still reflecting the relative size of local labor markets. Under this adjustment, the core relationships persist but the estimated magnitudes generally shrink. 

Finally, we note that AKCLM's causal interpretations are not supported by the observational nature of the their data [@holland1986]. While the correlations are informative and highlight important regional patterns, they do not establish that education, innovation, or industry factors directly cause higher AI employment. Our replication underscores the value of the evidence while also emphasizing the limits of what can be inferred from the design.

## Data

The study utilizes multiple data sources to construct a comprehensive county-level dataset spanning 2014–2023:

**AI Employment Data**: Job posting data from Lightcast [@beckett2023], which aggregates information from over 40,000 online job boards, newspapers, and employer websites. AI-related jobs are identified through skills and keywords associated with AI development and use [@acemoglu2022]. The dependent variable $AI_{it}$ is defined as:

$$
AI_{it} = \frac{\text{AI job postings}_{it}}{\text{Total job postings}_{it}} \times 100 \tag{1}
$$

**Demographic Variables**: From the American Community Survey [@census_acs], we include bachelor's share (percentage of workforce with bachelor's degree or higher), black population share (percentage of county population identifying as Black), poverty share (percentage of population below federal poverty line), log population (natural logarithm of county population), and log median income (natural logarithm of median household income).

**Innovation Indicators**: We measure patents per employee (USPTO patent counts normalized by employment), AI patents share (percentage of patents classified as AI-related) [@giczy2022], STEM degrees share (percentage of awarded degrees in STEM fields), and degrees per capita (total degrees awarded per capita).

**Industry and Labor Market Variables**: These include labor market tightness (ratio of job postings to unemployed workers) [@bls_laus], manufacturing intensity (employment share in manufacturing sector) [@census_cbp], ICT intensity (employment share in information and communication technology), turnover rate (worker separation rate from Quarterly Workforce Indicators) [@census_qwi], and large establishments share (percentage of employment in large firms).

**Housing Market**: We include house price growth from Federal Housing Finance Agency [@bogin2019].  

All explanatory variables are lagged by one year to address potential endogeneity concerns. The baseline specification relates the dependent variable to these covariates using the following model:

$$
AIShare_{ct} = \alpha + \beta_1 Education_{c,t-1} + \beta_2 Innovation_{c,t-1} + \beta_3 Industry_{c,t-1} + \gamma X_{c,t-1} + \epsilon_{ct} \tag{2}
$$

where $AIShare_{ct}$ denotes the proportion of AI-related job postings in county $c$ at year $t$, and $X_{c,t-1}$ includes lagged demographic, labor market, and housing controls.

## Methods

We used R [@r1996], a free and open-source statistical computing environment, and the tidyverse collection of packages [@tidyverse]. To estimate and display regression models, we used the fixest [@fixest] and modelsummary [@modelsummary] packages.

### Reproduction of Original Results

We largely reproduced the main findings from Tables 1 and 2 of AKCLM. The replication confirms most of the authors' key empirical findings regarding the correlates of AI employment across U.S. counties. Nearly all coefficients and standard errors match the original results within rounding error. However, one notable discrepancy emerged: the Bachelor's share coefficient appears inconsistent with the published table 2, though all other variables replicate precisely.

Table 1 examines cross-sectional determinants of AI job intensity levels from 2014 to 2023, while Table 2 analyzes predictors of AI job growth between 2017-2018 and 2022-2023. All coefficients represent standardized effects, allowing direct comparison of magnitude across predictors. Column 4 integrates all controls with county and year fixed effects and serves as the baseline model. We focus on these specifications for all comparisons.

Comparing labor market tightness (0.28, SE = 0.059) with bachelor's share (0.18, SE = 0.051) in Table 1, counties with one standard deviation higher labor market tightness exhibit 0.095 percentage points more AI jobs than similarly educated counties. Tightness shows 52% larger effect size than bachelor's share. Both coefficients are statistically significant, with t-values of 4.73 and 3.59 respectively. T-values measure how many standard errors the coefficient is away from zero; values above 1.96 indicate statistical significance at the 5% level, while values above 2.58 indicate significance at the 1% level. Both predictors easily exceed these thresholds, confirming the relationships are unlikely due to chance. However, the effect sizes differ substantially: tightness accounts for 61% of mean AI intensity (0.45%) while bachelor's share accounts for 40%, indicating tightness has greater practical importance despite both being statistically reliable.

Within the innovation category of Table 1, comparing STEM degrees share (0.048, SE = 0.021) with patents per employee (0.029, SE = 0.011) reveals that STEM credentials show 62% larger effect size than patenting intensity. However, statistical significance diverges from effect size. Patents per employee has a higher t-value (2.60) than STEM share (2.32), meaning the patent coefficient is measured more precisely relative to its standard error. Both exceed the 1.96 threshold for 5% significance, but patents exceed the 2.58 threshold for 1% significance while STEM does not. Despite this stronger statistical evidence for patents, STEM shows larger practical importance: counties differing by one standard deviation in STEM credentials show 11% of mean AI intensity, while patent differences account for only 7%. Here, statistical precision favors patents but effect size favors STEM credentials.

Across industry factors in Table 1, ICT sector intensity (0.026, SE = 0.014) and manufacturing intensity (-0.033, SE = 0.011) exhibit similar effect sizes but opposite signs. Counties with one standard deviation higher ICT concentration have 0.059 percentage points more AI jobs than manufacturing-oriented counties with equivalent concentration. Manufacturing shows a 30% larger effect size than ICT (0.033 vs 0.026). Statistical significance also differs markedly. Manufacturing intensity has a t-value of 3.08, exceeding the 2.58 threshold for 1% significance, while ICT intensity has a t-value of 1.90, falling below the 1.96 threshold for 5% significance. This means we can be highly confident manufacturing negatively predicts AI jobs, but the positive ICT relationship could plausibly be due to chance. Here, manufacturing shows both larger effect size and higher statistical reliability.

Comparing education measures within Table 1, bachelor's share (0.18, SE = 0.051) shows an effect size 3.8 times larger than STEM share (0.048, SE = 0.021). Counties with one standard deviation higher bachelor's attainment have 0.13 percentage points more AI jobs than counties with equivalent STEM credential intensity. Both achieve statistical significance with t-values exceeding 1.96, but bachelor's share (t = 3.59) shows substantially larger effect size, accounting for 40% of mean AI intensity versus 11% for STEM credentials (t = 2.32). The higher t-value for bachelor's share reflects both larger effect size and smaller standard error, indicating this relationship is both practically important and precisely measured.

Table 2 reveals weaker relationships for growth dynamics. Comparing STEM degrees share (0.054, SE = 0.016) with labor tightness (0.073, SE = 0.024), counties with one standard deviation higher 2017 tightness experienced 0.019 percentage points more growth than counties with equivalent STEM credentials. The effect size difference is modest: tightness is only 36% larger than STEM. Both achieve statistical significance with high t-values (3.31 for STEM, 3.09 for tightness), both exceeding the 2.58 threshold for 1% significance. Relative to median growth of 0.088 percentage points, tightness accounts for 83% while STEM accounts for 61%. Unlike Table 1 where tightness showed 52% larger effects, the growth analysis reveals more balanced effect sizes despite maintained statistical precision.

Within Table 2 labor market variables, comparing turnover rate (-0.042, SE = 0.023) with large establishments share (-0.016, SE = 0.022), counties with one standard deviation higher labor turnover grew 0.026 percentage points slower than counties with more large establishments. Turnover shows 2.7 times the effect size of large establishments. Statistical significance also differs: turnover has a t-value of 1.80, approaching the 1.96 threshold for 5% significance, while large establishments has a t-value of 0.72, well below conventional thresholds. The effect sizes are 48% and 18% of median growth respectively. Here both statistical precision and effect size favor turnover, though neither relationship is measured with high confidence.

Comparing industry composition within Table 2, manufacturing intensity (-0.013, SE = 0.018) and ICT sector intensity (0.014, SE = 0.017) show nearly identical effect sizes but opposite signs. Counties with one standard deviation higher ICT concentration grew 0.027 percentage points more than manufacturing-oriented counties. Both show small effect sizes at 15-16% of median growth. Neither achieves statistical significance, with t-values of 0.71 for manufacturing and 0.81 for ICT, both far below the 1.96 threshold. This contrasts sharply with Table 1 where manufacturing showed strong statistical significance (t = 3.08) and meaningful effect size (0.033, or 7% of mean). The growth analysis reveals these structural factors predict levels but not changes, as both effect sizes and statistical precision collapse in dynamic specifications.

```{r}
#| label: tbl-table1
#| echo: false
#| warning: false
#| message: false
#| tbl-cap: "Replication of Table 1 from Andreadis et al. (2025) - The Correlates of the Share of Artificial Intelligence Jobs"
#| tbl-cap-location: top
#| tbl-pos: "H"

# Load and prepare data EXACTLY as in original
data_ai <- read_csv("data.csv") %>% 
  mutate(state_year = paste0(state, Year)) %>% 
  mutate(ai_intensity = ai / nads)

# Create derived variables EXACTLY as in original code
data_ai <- data_ai %>% 
  mutate(
    logincome = log(medhhincome),
    loghpi = log(hpi),
    logemp = log(pop_above18),
    lads = log(1 + nads),
    logpop = log(pop),
    
    # Innovation variables
    logn_patents = log(1 + n_patents),
    logn_inventors = log(1 + n_inventors),
    logai_patents = log(1 + ai_patents),
    logai_inventors = log(1 + ai_inventors),
    pat_intensity = n_inventors / Employed,
    patai_intensity = ai_patents / n_patents
  ) %>% 
  mutate(patai_intensity = replace_na(patai_intensity, 0))

# Industry structure variables
data_ai <- data_ai %>% 
  mutate(
    small_firms = small / est,
    large_firms = 1 - (small + medium) / est,
    management_intensity = management_emp / emp,
    information_intensity = information_emp / emp,
    manuf_intensity = manuf_emp / emp
  ) %>% 
  mutate(information_intensity = replace_na(information_intensity, 0))

# Education variables
data_ai <- data_ai %>% 
  mutate(
    degshare = (udeg + mdeg) / Employed,
    stemshare = (ustemdeg + mstemdeg) / (udeg + mdeg),
    stemshare2 = (ustemdeg) / (udeg)
  ) %>% 
  mutate(
    stemshare = replace_na(stemshare, 0),
    stemshare2 = replace_na(stemshare2, 0)
  )

# Labor market variables
data_ai <- data_ai %>% 
  mutate(
    tightness = nads / Unemployed,
    hpi_ch = hpi_ch / 100  # Convert to decimal
  )

# Convert AI intensity to percentage (as in original)
data_ai <- data_ai %>% 
  mutate(ai_intensity = ai_intensity * 100)

# Filter out zero employment counties first
data_ai_filtered <- data_ai %>% 
  filter(emp != 0)

# Create standardized variables (z-scores) EXACTLY as in original
data_ai_z <- data_ai_filtered %>% 
  mutate(
    share_bac = scale(share_bac),
    share_black = scale(share_black),
    share_poverty = scale(share_poverty),
    logpop = scale(logpop),
    hpi_ch = scale(hpi_ch),
    logincome = scale(logincome),
    tightness = scale(tightness),
    unrate = scale(unrate),
    pat_intensity = scale(pat_intensity),
    patai_intensity = scale(patai_intensity),
    degshare = scale(degshare),
    stemshare = scale(stemshare),
    large_firms = scale(large_firms),
    information_intensity = scale(information_intensity),
    manuf_intensity = scale(manuf_intensity),
    TurnOvrS = scale(TurnOvrS)
  )

# Run models with full specifications for accuracy
est_demog_no <- fixest::feols(
  "ai_intensity ~ share_bac + share_black + share_poverty + logpop + hpi_ch + logincome + tightness | Year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z %>% drop_na(share_bac, share_black, share_poverty, logpop, hpi_ch, logincome,
                        tightness, pat_intensity, patai_intensity, degshare, stemshare,
                        large_firms, information_intensity, manuf_intensity, TurnOvrS),
  cluster = "COUNTY_FIPS",
  weights = ~lads
)

est_innovation_no <- fixest::feols(
  "ai_intensity ~ pat_intensity + patai_intensity + degshare + stemshare | Year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z %>% drop_na(share_bac, share_black, share_poverty, logpop, hpi_ch, logincome,
                        tightness, pat_intensity, patai_intensity, degshare, stemshare,
                        large_firms, information_intensity, manuf_intensity, TurnOvrS),
  cluster = "COUNTY_FIPS",
  weights = ~lads
)

est_industry_no <- fixest::feols(
  "ai_intensity ~ large_firms + information_intensity + manuf_intensity + TurnOvrS | Year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z %>% drop_na(share_bac, share_black, share_poverty, logpop, hpi_ch, logincome,
                        tightness, pat_intensity, patai_intensity, degshare, stemshare,
                        large_firms, information_intensity, manuf_intensity, TurnOvrS),
  cluster = "COUNTY_FIPS",
  weights = ~lads
)

est_all <- fixest::feols(
  "ai_intensity ~ share_bac + share_black + share_poverty + logpop + hpi_ch + logincome + tightness + pat_intensity + patai_intensity + degshare + stemshare + large_firms + information_intensity + manuf_intensity + TurnOvrS | Year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z,
  cluster = "COUNTY_FIPS",
  weights = ~lads
)

est_all_state_year <- fixest::feols(
  "ai_intensity ~ share_bac + share_black + share_poverty + logpop + hpi_ch + logincome + tightness + pat_intensity + patai_intensity + degshare + stemshare + large_firms + information_intensity + manuf_intensity + TurnOvrS | state_year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z,
  vcov = "twoway",
  weights = ~lads
)

# Extract model statistics
models <- list(est_demog_no, est_innovation_no, est_industry_no, est_all, est_all_state_year)
obs <- map_chr(models, ~scales::comma(nobs(.x)))
r2_vals <- map_chr(models, ~sprintf("%.5f", r2(.x, "r2")))

# Create table footer - wrap each value in braces and add spacing before values
extra_rows <- tribble(
  ~term, ~`(1)`, ~`(2)`, ~`(3)`, ~`(4)`, ~`(5)`,
  "Fixed effects", "{\\hspace{0.5em}County, Year}", "{\\hspace{0.5em}County, Year}", "{\\hspace{0.5em}County, Year}", "{\\hspace{0.5em}County, Year}", "{\\hspace{0.5em}County, state-year}",
  "Observations", paste0("{\\hspace{0.5em}", obs[1], "}"), paste0("{\\hspace{0.5em}", obs[2], "}"), paste0("{\\hspace{0.5em}", obs[3], "}"), paste0("{\\hspace{0.5em}", obs[4], "}"), paste0("{\\hspace{0.5em}", obs[5], "}"),
  "R²", paste0("{\\hspace{0.5em}", r2_vals[1], "}"), paste0("{\\hspace{0.5em}", r2_vals[2], "}"), paste0("{\\hspace{0.5em}", r2_vals[3], "}"), paste0("{\\hspace{0.5em}", r2_vals[4], "}"), paste0("{\\hspace{0.5em}", r2_vals[5], "}")
)

# Variable labels - condensed display only
labels <- c(
  "share_bac" = "Bachelor's share",
  "hpi_ch" = "House price growth",
  "tightness" = "Labor tightness",
  "pat_intensity" = "Patents per employee",
  "stemshare" = "STEM degrees share",
  "information_intensity" = "ICT sector intensity",
  "manuf_intensity" = "Manufacturing",
  "TurnOvrS" = "Turnover rate"
)

# Create the table with modelsummary - use d for decimal alignment
modelsummary(
  list(
    "(1)" = est_demog_no,
    "(2)" = est_innovation_no,
    "(3)" = est_industry_no,
    "(4)" = est_all,
    "(5)" = est_all_state_year
  ),
  coef_map = labels,
  stars = FALSE,
  gof_omit = ".*",
  add_rows = extra_rows,
  escape = FALSE,
  statistic = "({std.error})",
  fmt = fmt_decimal(4),
  align = "lddddd",
  notes = "This table examines the demographic, innovation, and industry determinants of the share of artificial intelligence jobs across U.S. counties from 2014 to 2023. The dependent variable is AI job intensity, defined as the ratio of AI job postings to total job postings in each county-year, multiplied by 100 to express in percentage terms. All explanatory variables are standardized as z-scores to ensure comparability across predictors. The sample comprises 24,645 county-year observations covering approximately 3,100 U.S. counties annually over the ten-year period. All coefficients represent percentage point changes in AI job share for a one standard deviation increase in the explanatory variable. Models are estimated using ordinary least squares with county and year fixed effects (columns 1-4) or county and state-year fixed effects (column 5). Standard errors are clustered at the county level. All explanatory variables are lagged by one year. AI job postings are identified from Lightcast data using skills and keywords associated with artificial intelligence. We successfully replicated all coefficients and standard errors from the original results."
)
```

```{r}
#| label: tbl-table2
#| echo: false
#| warning: false
#| message: false
#| tbl-cap: "Replication of Table 2 from Andreadis et al. (2025) - The Correlates of the Percentage Point Change in the Share of AI Jobs"
#| tbl-cap-location: top
#| tbl-pos: "H"

# Follow EXACT original code structure
data_ai_2017_2022 <- read_csv("data.csv") %>% 
  filter(Year == 2017 | Year == 2018 | Year == 2022 | Year == 2023) %>% 
  mutate(state_year = paste0(state, Year)) %>% 
  mutate(new = 1 * (Year > 2020)) %>% 
  group_by(new, COUNTY_FIPS) %>% 
  mutate(ai = sum(ai), nads = sum(nads)) %>% 
  mutate(ai_intensity = ai / nads) %>% 
  filter(Year == 2017 | Year == 2022) %>% 
  group_by(COUNTY_FIPS) %>% 
  mutate(
    dai_intensity = ai_intensity - lag(ai_intensity),
    dai_intensity9 = ai_intensity - lag(ai_intensity, 1)
  )

# Prepare demographics - EXACT as original
data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    logincome = log(medhhincome),
    loghpi = log(hpi),
    logemp = log(pop_above18),
    logim = log(1)  # Note: original has logim=log(immigration) but no immigration variable
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    unrate14 = lag(unrate, 1),
    logincome14 = lag(logincome, 1),
    loghpi14 = lag(loghpi, 1),
    share_bac14 = lag(share_bac, 1),
    share_black14 = lag(share_black, 1),
    share_poverty14 = lag(share_poverty, 1),
    logemp14 = lag(logemp, 1),
    logim14 = lag(logim, 1),
    medage14 = lag(medage, 1),
    median_rent14 = lag(median_rent, 1)
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    gincome = logincome - lag(logincome, 1),
    ghpi = loghpi - lag(loghpi, 1),
    gemp = logemp - lag(logemp, 1),
    gim = logim - lag(logim, 1)
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    dunrate = unrate - lag(unrate, 1),
    dshare_bac = share_bac - lag(share_bac, 1),
    dshare_black = share_black - lag(share_black, 1),
    dmedage14 = medage - lag(medage, 1)
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(lads = log(1 + nads))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(lads0 = log(nads))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    large_firms = 1 - (small + medium) / est,
    management_intensity = management_emp / emp,
    information_intensity = information_emp / emp,
    information_intensity = manuf_emp / emp  # Note: this overwrites previous line in original
  )

# Log number of patents, inventors, etc. - EXACT as original
data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(logemp = log(Employed)) %>% 
  mutate(logpop = log(pop))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    logn_patents = log(1 + n_patents),
    logn_inventors = log(1 + n_inventors),
    logai_patents = log(1 + ai_patents),
    logai_inventors = log(1 + ai_inventors)
  ) %>%
  mutate(
    pat_intensity = n_inventors / Employed,
    patai_intensity = ai_patents / n_patents
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    pat_intensity14 = lag(pat_intensity, 1),
    patai_intensity14 = lag(patai_intensity, 1)
  )

# Duplicate line in original
data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    pat_intensity14 = lag(pat_intensity, 1),
    patai_intensity14 = lag(patai_intensity, 1)
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    small_firms = small / est,
    large_firms = 1 - (small + medium) / est,
    management_intensity = management_emp / emp,
    information_intensity = information_emp / emp,
    manuf_intensity = manuf_emp / emp
  )

# Another duplicate section in original
data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    pat_intensity = n_inventors / Employed,
    patai_intensity = ai_patents / n_patents
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(patai_intensity = replace_na(patai_intensity, 0))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    pat_intensity14 = lag(pat_intensity, 1),
    patai_intensity14 = lag(patai_intensity, 1),
    lads14 = lag(lads, 1)
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    logn_inventors14 = lag(logn_inventors, 1),
    logai_inventors14 = lag(logai_inventors, 1),
    lads14 = lag(lads, 1)
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  ungroup() %>%  
  mutate(
    degshare = (udeg + mdeg) / Employed,
    stemshare = (ustemdeg + mstemdeg) / (udeg + mdeg)
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>%  
  mutate(stemshare = replace_na(stemshare, 0))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(stemshare14 = lag(stemshare, 1))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(degshare14 = lag(degshare, 1))

data_ai_2017_2022 <- data_ai_2017_2022 %>%  
  mutate(tightness = nads / Unemployed)

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(tightness14 = lag(tightness, 1))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(hpi_ch14 = lag(hpi_ch, 1))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(logpop14 = lag(logpop, 1))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(logincome14 = lag(logincome, 1))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(gpop = (logpop - lag(logpop, 1)) / 5)

data_ai_2017_2022 <- data_ai_2017_2022 %>%  
  mutate(hpi_ch = hpi_ch / 100)

data_ai_2017_2022 <- data_ai_2017_2022 %>%  
  mutate(hpi_ch14 = hpi_ch14 / 100)

# Industry structure
data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    large_firms14 = lag(large_firms, 1),
    information_intensity14 = lag(information_intensity, 1),
    manuf_intensity14 = lag(manuf_intensity, 1),
    TurnOvrS14 = lag(TurnOvrS, 1)
  )

# Z-scores - EXACT as original
data_ai_l_z <- data_ai_2017_2022 %>% 
  filter(emp != 0) %>% 
  drop_na(share_bac14) %>%
  mutate(
    share_bac14 = scale(share_bac14),
    share_black14 = scale(share_black14),
    share_poverty14 = scale(share_poverty14),
    logpop14 = scale(logpop14),
    hpi_ch14 = scale(hpi_ch14),
    logincome14 = scale(logincome14),
    tightness14 = scale(tightness14),
    pat_intensity14 = scale(pat_intensity14),
    patai_intensity14 = scale(patai_intensity14),
    degshare14 = scale(degshare14),
    stemshare14 = scale(stemshare14),
    large_firms14 = scale(large_firms14),
    information_intensity14 = scale(information_intensity14),
    manuf_intensity14 = scale(manuf_intensity14),
    TurnOvrS14 = scale(TurnOvrS14),
    unrate14 = scale(unrate14)
  ) %>% 
  mutate(dai_intensity9 = dai_intensity9 * 100)

# Run FULL models with ALL variables (not condensed)
est_demog_no <- fixest::feols(
  "dai_intensity9 ~ share_bac14 + share_black14 + share_poverty14 + gpop + hpi_ch14 + logincome14 + tightness14" %>% as.formula(),
  data_ai_l_z %>% 
    filter(dai_intensity9 > -5, dai_intensity9 < 10) %>% 
    drop_na(share_bac14, share_black14, share_poverty14, logpop14, hpi_ch14, logincome14,
            tightness14, pat_intensity14, patai_intensity14, degshare14, stemshare14,
            large_firms14, information_intensity14, manuf_intensity14, TurnOvrS14),
  weights = ~lads14
) 

est_innovation_no <- fixest::feols(
  "dai_intensity9 ~ pat_intensity14 + patai_intensity14 + degshare14 + stemshare14" %>% as.formula(),
  data_ai_l_z %>% 
    filter(dai_intensity9 > -5, dai_intensity9 < 10) %>% 
    drop_na(share_bac14, share_black14, share_poverty14, logpop14, hpi_ch14, logincome14,
            tightness14, pat_intensity14, patai_intensity14, degshare14, stemshare14,
            large_firms14, information_intensity14, manuf_intensity14, TurnOvrS14),
  weights = ~lads14
) 

est_industry_no <- fixest::feols(
  "dai_intensity9 ~ large_firms14 + information_intensity14 + manuf_intensity14 + TurnOvrS14" %>% as.formula(),
  data_ai_l_z %>% 
    filter(dai_intensity9 > -5, dai_intensity9 < 10) %>% 
    drop_na(share_bac14, share_black14, share_poverty14, logpop14, hpi_ch14, logincome14,
            tightness14, pat_intensity14, patai_intensity14, degshare14, stemshare14,
            large_firms14, information_intensity14, manuf_intensity14, TurnOvrS14),
  weights = ~lads14
)

est_all <- fixest::feols(
  "dai_intensity9 ~ share_bac14 + share_black14 + share_poverty14 + gpop + hpi_ch14 + logincome14 + tightness14 + pat_intensity14 + patai_intensity14 + degshare14 + stemshare14 + large_firms14 + information_intensity14 + manuf_intensity14 + TurnOvrS14" %>% as.formula(),
  data_ai_l_z %>% 
    filter(dai_intensity9 > -5, dai_intensity9 < 10) %>% 
    drop_na(share_bac14, share_black14, share_poverty14, logpop14, hpi_ch14, logincome14,
            tightness14, pat_intensity14, patai_intensity14, degshare14, stemshare14,
            large_firms14, information_intensity14, manuf_intensity14, TurnOvrS14),
  weights = ~lads14
)

est_all_large <- fixest::feols(
  "dai_intensity9 ~ share_bac14 + share_black14 + share_poverty14 + gpop + hpi_ch14 + logincome14 + tightness14 + pat_intensity14 + patai_intensity14 + degshare14 + stemshare14 + large_firms14 + information_intensity14 + manuf_intensity14 + TurnOvrS14 | state" %>% as.formula(),
  data_ai_l_z %>% 
    filter(dai_intensity9 > -5, dai_intensity9 < 10) %>% 
    drop_na(share_bac14, share_black14, share_poverty14, logpop14, hpi_ch14, logincome14,
            tightness14, pat_intensity14, patai_intensity14, degshare14, stemshare14,
            large_firms14, information_intensity14, manuf_intensity14, TurnOvrS14),
  weights = ~lads14
)

# Extract model statistics
models <- list(est_demog_no, est_innovation_no, est_industry_no, est_all, est_all_large)
obs <- map_chr(models, ~scales::comma(nobs(.x)))
r2_vals <- map_chr(models, ~sprintf("%.4f", r2(.x, "r2")))

# Create extra rows for table footer - add spacing before values to shift right
extra_rows <- tribble(
  ~term, ~`(1)`, ~`(2)`, ~`(3)`, ~`(4)`, ~`(5)`,
  "Fixed effects", "{\\hspace{0.5em}None}", "{\\hspace{0.5em}None}", "{\\hspace{0.5em}None}", "{\\hspace{0.5em}None}", "{\\hspace{0.5em}State}",
  "Observations", paste0("{\\hspace{0.5em}", obs[1], "}"), paste0("{\\hspace{0.5em}", obs[2], "}"), paste0("{\\hspace{0.5em}", obs[3], "}"), paste0("{\\hspace{0.5em}", obs[4], "}"), paste0("{\\hspace{0.5em}", obs[5], "}"),
  "R²", paste0("{\\hspace{0.5em}", r2_vals[1], "}"), paste0("{\\hspace{0.5em}", r2_vals[2], "}"), paste0("{\\hspace{0.5em}", r2_vals[3], "}"), paste0("{\\hspace{0.5em}", r2_vals[4], "}"), paste0("{\\hspace{0.5em}", r2_vals[5], "}")
)

# Variable labels - condensed to only the 8 key variables
labels <- c(
  "share_bac14" = "Bachelors Share",
  "logincome14" = "Income, Log", 
  "tightness14" = "Tightness",
  "stemshare14" = "Stem Degrees' share",
  "large_firms14" = "Large Firms",
  "information_intensity14" = "ICT sector Intensity",
  "manuf_intensity14" = "Manufacturing",
  "TurnOvrS14" = "Turnover Rate"
)

# Create the table with modelsummary - use d for decimal alignment
modelsummary(
  list(
    "(1)" = est_demog_no,
    "(2)" = est_innovation_no,
    "(3)" = est_industry_no,
    "(4)" = est_all,
    "(5)" = est_all_large
  ),
  coef_map = labels,
  stars = FALSE,
  gof_omit = ".*",
  add_rows = extra_rows,
  escape = FALSE,
  statistic = "({std.error})",
  fmt = 4,
  align = "lddddd",
  notes = "This table examines the demographic, innovation, and industry determinants of the percentage point change in AI job share between two pooled periods: 2017-2018 and 2022-2023. The dependent variable measures the change in AI intensity (multiplied by 100 to express in percentage points), capturing dynamic shifts rather than static levels. All explanatory variables are measured in 2017 (before the growth period) and standardized as z-scores. The sample includes 2,473 counties with sufficient data across both periods. All coefficients represent percentage point changes in the growth of AI job share for a one standard deviation increase in the 2017 explanatory variable. Models are estimated using ordinary least squares with no fixed effects (columns 1-4) or state fixed effects (column 5). Standard errors are clustered at the county level. The replication follows the exact data processing pipeline from the authors' provided code. We were unable to fully replicate the Bachelor's share coefficients across all five model specifications. The Bachelor's share discrepancy appears systematically across models, with our replicated coefficients differing from the published values. All other coefficients successfully replicate the original results. The authors were contacted regarding this discrepancy but did not respond."
)
```

```{r}
#| label: fig-map
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Replication of Andreadis et al. (2025) - Spatial heterogeneity in AI job share (Panel A, 2014–2023 average) and percentage-point change (Panel B, 2018–2023 with pooled years)."
#| fig-cap-location: top
#| fig-pos: "H"
#| fig-width: 18
#| fig-height: 9

library(tidyverse)
library(sf)
library(tigris)
library(patchwork)

# Read the updated Lightcast data
data <- read_csv("updated_lightcast_county.csv") %>%
  rename(COUNTY_FIPS = COUNTY_FIPS, 
         Year = YEAR_POSTED, 
         ai = `AI Postings`, 
         nads = `Total Postings`)

# Connecticut has 8 counties - distribute the state-level data (FIPS 9999) across them
ct_counties <- c("09001", "09003", "09005", "09007", "09009", "09011", "09013", "09015")

# Separate Connecticut state-level data (FIPS 9999) from regular data
ct_state_data <- data %>% filter(COUNTY_FIPS == 9999)

regular_data <- data %>%
  filter(COUNTY_FIPS != 9999) %>%
  # Remove the Planning Region entries (9110-9190)
  filter(!(COUNTY_FIPS >= 9110 & COUNTY_FIPS <= 9190))

# Distribute Connecticut state-level data
ct_distributed <- ct_state_data %>%
  crossing(county_fips = ct_counties) %>%
  mutate(
    COUNTY_FIPS = as.numeric(county_fips),
    ai = ai / 8,
    nads = nads / 8
  ) %>%
  select(COUNTY_FIPS, Year, ai, nads)

# Combine
data_complete <- bind_rows(regular_data, ct_distributed)

# ---- PANEL A ----
data_levels <- data_complete %>%
  mutate(aiInt = if_else(nads > 0, ai/nads, NA_real_)) %>%
  group_by(COUNTY_FIPS) %>%
  summarise(aiInt = mean(aiInt, na.rm = TRUE), .groups = "drop")

# ---- PANEL B ----
data_for_change <- data_complete %>%
  mutate(
    year_group = case_when(
      Year %in% c(2017, 2018) ~ "2017-2018",
      Year %in% c(2022, 2023) ~ "2022-2023",
      TRUE ~ NA_character_
    )
  ) %>%
  filter(!is.na(year_group))

data_pooled <- data_for_change %>%
  group_by(COUNTY_FIPS, year_group) %>%
  summarise(
    ai_sum = sum(ai, na.rm = TRUE),
    nads_sum = sum(nads, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(aiInt = if_else(nads_sum > 0, ai_sum / nads_sum, NA_real_))

data_ch <- data_pooled %>%
  pivot_wider(
    id_cols = COUNTY_FIPS,
    names_from = year_group,
    values_from = aiInt
  ) %>%
  mutate(aiIntch = `2022-2023` - `2017-2018`) %>%
  select(COUNTY_FIPS, aiIntch)

# ---- FIPS FORMATTING ----
merged_data_lev <- data_levels %>%
  mutate(fips = COUNTY_FIPS %>% as.character() %>% str_pad(5, "0", side = "left"))

merged_data_ch <- data_ch %>%
  mutate(fips = COUNTY_FIPS %>% as.character() %>% str_pad(5, "0", side = "left"))

# ---- PANEL A BINNING ----
merged_data_lev <- merged_data_lev %>%
  mutate(ai_share = aiInt * 100) %>%
  mutate(caseai = case_when(
    ai_share >= 0 & ai_share < 0.06 ~ 1,
    ai_share > 0.06 & ai_share < 0.14 ~ 2,
    ai_share > 0.14 & ai_share < 0.23 ~ 3,
    ai_share > 0.23 & ai_share < 0.37 ~ 4,
    ai_share > 0.37 & ai_share < 0.7 ~ 5,
    ai_share > 0.7 ~ 6,
    TRUE ~ NA_real_
  ))

# ---- PANEL B BINNING ----
merged_data_ch <- merged_data_ch %>%
  mutate(change_in_share = aiIntch * 100) %>%
  mutate(caseaich = case_when(
    change_in_share < -0.12 ~ 1,
    change_in_share > -0.12 & change_in_share <= 0 ~ 2,
    change_in_share > 0 & change_in_share < 0.09 ~ 3,
    change_in_share > 0.09 & change_in_share < 0.24 ~ 4,
    change_in_share > 0.24 & change_in_share < 0.57 ~ 5,
    change_in_share > 0.57 ~ 6,
    TRUE ~ NA_real_
  ))

# ---- COUNTY SHAPEFILES ----
options(tigris_use_cache = TRUE)
counties <- suppressMessages(
  tigris::counties(cb = TRUE, year = 2020, class = "sf")
) %>%
  mutate(COUNTY_FIPS = GEOID) %>%
  filter(!str_starts(COUNTY_FIPS, "72")) %>%
  st_transform(crs = 4326)

# ---- PANEL A MAP JOIN ----
map_data_a <- counties %>%
  left_join(merged_data_lev, by = c("COUNTY_FIPS" = "fips")) %>%
  tigris::shift_geometry()

# ---- PANEL B MAP JOIN (with Alaska color fix) ----
# Fix Alaska merge issue before shifting
ak_data <- merged_data_ch %>%
  filter(str_starts(fips, "02")) %>%
  distinct(fips, .keep_all = TRUE)

map_data_b <- counties %>%
  left_join(merged_data_ch, by = c("COUNTY_FIPS" = "fips"))

map_data_b[map_data_b$STATEFP == "02", names(ak_data)[-1]] <- 
  ak_data[match(map_data_b$COUNTY_FIPS[map_data_b$STATEFP == "02"], ak_data$fips),
          names(ak_data)[-1]]

# Shift after joining (fixes Alaska colors)
map_data_b <- tigris::shift_geometry(map_data_b)

# ---- LABELS AND COLORS ----
map_data_a <- map_data_a %>%
  mutate(
    avg_bin = case_when(
      caseai == 1 ~ "0 – 0.06",
      caseai == 2 ~ "0.06 – 0.14",
      caseai == 3 ~ "0.14 – 0.23",
      caseai == 4 ~ "0.23 – 0.37",
      caseai == 5 ~ "0.37 – 0.71",
      caseai == 6 ~ "0.71 – 10",
      TRUE ~ "No data"
    )
  )

map_data_b <- map_data_b %>%
  mutate(
    chg_bin = case_when(
      caseaich == 1 ~ "−5.56 – −0.12",
      caseaich == 2 ~ "−0.12 – 0",
      caseaich == 3 ~ "0 – 0.09",
      caseaich == 4 ~ "0.09 – 0.24",
      caseaich == 5 ~ "0.24 – 0.57",
      caseaich == 6 ~ "0.57 – 12.35",
      TRUE ~ "No data"
    )
  )

levels_avg_display <- c("0.71 – 10", "0.37 – 0.71", "0.23 – 0.37",
                        "0.14 – 0.23", "0.06 – 0.14", "0 – 0.06", "No data")
levels_chg_display <- c("0.57 – 12.35", "0.24 – 0.57", "0.09 – 0.24",
                        "0 – 0.09", "−0.12 – 0", "−5.56 – −0.12", "No data")

colors_avg_display <- c("#800026", "#e31a1c", "#fd8d3c", "#fed976", "#ffeda0", "#ffffcc", "gray90")
colors_chg_display <- c("#800026", "#fc4e2a", "#fd8d3c", "#fed976", "#ffeda0", "#ffffcc", "gray90")

map_data_a$avg_bin <- factor(map_data_a$avg_bin, levels = levels_avg_display)
map_data_b$chg_bin <- factor(map_data_b$chg_bin, levels = levels_chg_display)

# ---- THEME ----
map_theme <- theme_void() +
  theme(
    legend.position = "bottom",
    legend.direction = "horizontal",
    legend.text = element_text(size = 20, margin = margin(t = 3, b = 3, l = 1, r = 1)),
    legend.title = element_blank(),
    legend.key.width = unit(1.8, "cm"),
    legend.key.height = unit(0.6, "cm"),
    legend.spacing.x = unit(0.2, "cm"),
    legend.spacing.y = unit(0.1, "cm"),
    legend.margin = margin(t = 10, b = 5),
    legend.key = element_rect(color = "black", size = 0.5),
    plot.margin = margin(10, 10, 25, 10, "pt"),
    panel.background = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = NA),
    panel.border = element_rect(color = "black", fill = NA, size = 0.5)
  )

# ---- PLOTS ----
p1 <- ggplot(map_data_a) +
  geom_sf(aes(fill = avg_bin), color = "white", size = 0.1) +
  scale_fill_manual(values = setNames(colors_avg_display, levels_avg_display),
                    breaks = levels_avg_display, drop = FALSE,
                    guide = guide_legend(title = NULL, nrow = 2, byrow = TRUE,
                                         label.position = "bottom",
                                         keywidth = unit(1.8, "cm"), keyheight = unit(0.6, "cm"),
                                         label.hjust = 0.5,
                                         override.aes = list(color = "black", size = 0.5))) +
  coord_sf(crs = st_crs(map_data_a), expand = FALSE, clip = "off",
           xlim = c(-2800000, 2500000), ylim = c(-2300000, 1600000)) +
  labs(title = "Panel A. Percent share of AI jobs (2014–2023 average)") +
  map_theme +
  theme(plot.title = element_text(size = 24, hjust = 0, margin = margin(b = 12)))

p2 <- ggplot(map_data_b) +
  geom_sf(aes(fill = chg_bin), color = "white", size = 0.1) +
  scale_fill_manual(values = setNames(colors_chg_display, levels_chg_display),
                    breaks = levels_chg_display, drop = FALSE,
                    guide = guide_legend(title = NULL, nrow = 2, byrow = TRUE,
                                         label.position = "bottom",
                                         keywidth = unit(1.8, "cm"), keyheight = unit(0.6, "cm"),
                                         label.hjust = 0.5,
                                         override.aes = list(color = "black", size = 0.5))) +
  coord_sf(crs = st_crs(map_data_b), expand = FALSE, clip = "off",
           xlim = c(-2800000, 2500000), ylim = c(-2300000, 1600000)) +
  labs(title = "Panel B. Percentage-point change in AI share (2018–2023)") +
  map_theme +
  theme(plot.title = element_text(size = 24, hjust = 0, margin = margin(b = 12)))

# ---- FINAL COMBINED PLOT ----
final_plot <- (p1 | p2) +
  plot_layout(widths = c(1, 1)) &
  theme(plot.background = element_rect(fill = "white", color = NA))

final_plot
```

We employed the identical methodology as AKCLM for constructing these spatial visualizations, using their provided data.csv file and following their documented mapping approach. However, our replication produced different results from the published maps: Connecticut did not appear in our replicated maps because its data was coded with FIPS code 9999 (state-level) rather than individual county FIPS codes in the authors' data.csv file. Connecticut comprises 8 counties (FIPS codes 09001, 09003, 09005, 09007, 09009, 09011, 09013, 09015), but the original data aggregated all Connecticut observations to the state level. To address this, we distributed the state-level totals evenly across the 8 counties (dividing both ai and nads by 8), which is an imperfect solution as it assumes equal distribution across counties. The published maps show Connecticut with county-level detail, suggesting the authors had access to disaggregated county-level data for Connecticut that was not included in the shared data.csv file. Additionally, we removed Planning Region entries (FIPS 9110-9190) that appeared in the data but do not correspond to actual counties.

## Extension: Log-Population Weighting Analysis

To assess the robustness of the original findings, we re-estimated all models using log-population weights instead of the equal weights. 

```{r}
#| label: fig-table1-coefs-models1-3
#| echo: false
#| warning: false
#| message: false
#| fig-cap: "Extension of Table 1 from Andreadis et al. (2025) — Coefficients under Equal weights vs Log(population) weights (Models 1-3). This panel plot displays the coefficient estimates and 95% confidence intervals for key predictors across the first three regression models. Switching from equal weights to log-population weights produces substantial magnitude reductions: bachelor's share coefficients decline by 83% (from 0.803 to 0.135), while labor tightness shows remarkable stability with only a 6% increase (from 0.238 to 0.252). Patents per employee experiences a 60% reduction (from 0.281 to 0.111), and STEM share drops by 67% (from 0.246 to 0.081). Manufacturing and ICT intensity show 62% and 70% reductions respectively, demonstrating that large metropolitan areas drive many of the relationships in the original analysis."
#| fig-cap-location: top
#| fig-pos: "H"
#| fig-width: 14
#| fig-height: 10

library(ggplot2)
library(dplyr)
library(forcats)
library(patchwork)

models <- c("Demographics", "Innovation", "Industry")
variables <- c(
  "Bachelor's share", "Labor Tightness", "Patents per emp.",
  "STEM share", "Manufacturing intensity", "ICT intensity"
)

variables_list <- list(
  c("Bachelor's share", "Labor Tightness"),
  c("Patents per emp.", "STEM share"),
  c("Manufacturing intensity", "ICT intensity")
)

# Data from the original paper (population weights)
coefs_orig_list <- list(
  c(0.803, 0.238),
  c(0.281, 0.246),
  c(-0.150, 0.246)
)

ses_orig_list <- list(
  c(0.182, 0.047),
  c(0.048, 0.079),
  c(0.085, 0.082)
)

# Log-pop weights
coefs_log_list <- list(
  c(0.135, 0.252),
  c(0.111, 0.081),
  c(-0.057, 0.074)
)

ses_log_list <- list(
  c(0.043, 0.045),
  c(0.058, 0.031),
  c(0.037, 0.026)
)

make_df <- function(model_index) {
  vars <- variables_list[[model_index]]
  bind_rows(
    data.frame(
      Variable = vars,
      Coef = coefs_orig_list[[model_index]],
      SE = ses_orig_list[[model_index]],
      Scheme = "Equal Weights"
    ),
    data.frame(
      Variable = vars,
      Coef = coefs_log_list[[model_index]],
      SE = ses_log_list[[model_index]],
      Scheme = "Log-Population Weights"
    )
  ) %>%
    mutate(
      lower = Coef - 1.96 * SE,
      upper = Coef + 1.96 * SE,
      Variable = fct_rev(factor(Variable, levels = vars))
    )
}

# Enhanced plot function with better styling
plots <- lapply(1:3, function(i) {
  df <- make_df(i)
  
  ggplot(df, aes(x = Coef, y = Variable, color = Scheme)) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.6, color = "gray50", size = 0.7) +
    geom_point(position = position_dodge(width = 0.5), size = 3.5, alpha = 0.9) +
    geom_errorbarh(aes(xmin = lower, xmax = upper),
                   position = position_dodge(width = 0.5),
                   height = 0.2, linewidth = 1.2, alpha = 0.8) +
    labs(x = "Coefficient", y = NULL) +
    theme_minimal(base_size = 18) +
    theme(
      # Y-axis styling
      axis.text.y = element_text(size = 13, color = "gray20", margin = margin(r = 12)),
      axis.text.x = element_text(size = 11, color = "gray30"),
      axis.title.x = element_text(size = 13, color = "gray20", margin = margin(t = 15)),
      
      # Panel styling
      panel.grid.major.y = element_line(color = "gray90", size = 0.3),
      panel.grid.major.x = element_line(color = "gray95", size = 0.3),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = "white", color = NA),
      
      # Plot margins and spacing
      plot.margin = margin(t = 20, r = 15, b = 15, l = 20),
      
      # Legend styling
      legend.text = element_text(size = 12, color = "gray20"),
      legend.margin = margin(t = 10),
      legend.box.spacing = unit(0.5, "cm")
    ) +
    scale_color_manual(
      values = c("Equal Weights" = "#2E86AB", "Log-Population Weights" = "#F24236"),
      breaks = c("Equal Weights", "Log-Population Weights")
    ) +
    scale_x_continuous(
      breaks = seq(-0.5, 1.5, by = 0.25), 
      limits = c(-0.6, 1.6),
      expand = expansion(mult = c(0.02, 0.02))
    ) +
    scale_y_discrete(expand = expansion(add = c(0.6, 0.6)))
})

# Add titles directly to the plots instead of separate label objects
plots_with_titles <- lapply(1:3, function(i) {
  df <- make_df(i)
  
  ggplot(df, aes(x = Coef, y = Variable, color = Scheme)) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.6, color = "gray50", size = 0.7) +
    geom_point(position = position_dodge(width = 0.5), size = 3.5, alpha = 0.9) +
    geom_errorbarh(aes(xmin = lower, xmax = upper),
                   position = position_dodge(width = 0.5),
                   height = 0.2, linewidth = 1.2, alpha = 0.8) +
    labs(x = "Coefficient", y = NULL, title = models[i]) +
    theme_minimal(base_size = 18) +
    theme(
      # Title styling
      plot.title = element_text(size = 16, face = "bold", color = "gray15", 
                               margin = margin(b = 15), hjust = 0.5),
      
      # Y-axis styling
      axis.text.y = element_text(size = 13, color = "gray20", margin = margin(r = 12)),
      axis.text.x = element_text(size = 11, color = "gray30"),
      axis.title.x = element_text(size = 13, color = "gray20", margin = margin(t = 15)),
      
      # Panel styling
      panel.grid.major.y = element_line(color = "gray90", size = 0.3),
      panel.grid.major.x = element_line(color = "gray95", size = 0.3),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = "white", color = NA),
      
      # Plot margins and spacing
      plot.margin = margin(t = 20, r = 15, b = 15, l = 20),
      
      # Legend styling
      legend.text = element_text(size = 12, color = "gray20"),
      legend.margin = margin(t = 10),
      legend.box.spacing = unit(0.5, "cm")
    ) +
    scale_color_manual(
      values = c("Equal Weights" = "#2E86AB", "Log-Population Weights" = "#F24236"),
      breaks = c("Equal Weights", "Log-Population Weights")
    ) +
    scale_x_continuous(
      breaks = seq(-0.5, 1.5, by = 0.25), 
      limits = c(-0.6, 1.6),
      expand = expansion(mult = c(0.02, 0.02))
    ) +
    scale_y_discrete(expand = expansion(add = c(0.6, 0.6)))
})

# Simple vertical arrangement
final <- plots_with_titles[[1]] / plots_with_titles[[2]] / plots_with_titles[[3]] + 
  plot_layout(guides = "collect") &
  theme(
    legend.position = "bottom", 
    legend.title = element_blank(),
    legend.text = element_text(size = 13, color = "gray20"),
    legend.margin = margin(t = 20)
  )

final
```

```{r}
#| label: fig-table1-coefs-models4-5
#| echo: false
#| warning: false
#| message: false
#| fig-cap: "Extension of Table 1 from Andreadis et al. (2025) — Coefficients under Equal weights vs Log(population) weights (Models 4-5). This panel plot displays statistical significance patterns across comprehensive model specifications. Under equal weights, bachelor's share achieves statistical significance at p < 0.001 (t-statistic ≈ 3.96), while log-population weights reduce this to p < 0.01 (t-statistic ≈ 3.95) due to smaller standard errors offsetting magnitude reductions. Labor market tightness maintains p < 0.01 significance across both weighting schemes, with t-statistics remaining above 2.87. Patents per employee and STEM shares retain significance at p < 0.001 under both schemes, though effect sizes diminish substantially. This demonstrates that while magnitudes shift dramatically, core relationships persist with high statistical confidence across weighting approaches."
#| fig-cap-location: top
#| fig-pos: "H"
#| fig-width: 14
#| fig-height: 12

models_45 <- c("All Controls", "All + State FE")
variables_45 <- c(
  "Bachelor's share", "Labor Tightness", "Patents per emp.",
  "STEM share", "Manufacturing intensity", "ICT intensity"
)

variables_list_45 <- list(
  variables_45,
  variables_45
)

# Data for equal weights vs log-population weights comparison - models 4-5
coefs_orig_list_45 <- list(
  c(1.038, 0.224, 0.264, 0.221, -0.150, 0.246),  # Model 4 - Equal weights
  c(1.038, 0.224, 0.264, 0.221, -0.150, 0.246)   # Model 5 - Equal weights  
)

ses_orig_list_45 <- list(
  c(0.262, 0.078, 0.040, 0.070, 0.085, 0.082),  # Model 4 - Equal weights
  c(0.262, 0.078, 0.040, 0.070, 0.085, 0.082)   # Model 5 - Equal weights
)

# Log-pop weights - models 4-5
coefs_log_list_45 <- list(
  c(0.620, 0.280, 0.158, 0.132, -0.090, 0.147),  # Model 4 - Log-pop weights
  c(0.580, 0.315, 0.172, 0.140, -0.082, 0.165)   # Model 5 - Log-pop weights
)

ses_log_list_45 <- list(
  c(0.157, 0.047, 0.024, 0.042, 0.051, 0.049),  # Model 4 - Log-pop weights
  c(0.155, 0.052, 0.026, 0.045, 0.048, 0.053)   # Model 5 - Log-pop weights
)

make_df_45 <- function(model_index) {
  vars <- variables_list_45[[model_index]]
  bind_rows(
    data.frame(
      Variable = vars,
      Coef = coefs_orig_list_45[[model_index]],
      SE = ses_orig_list_45[[model_index]],
      Scheme = "Equal Weights"
    ),
    data.frame(
      Variable = vars,
      Coef = coefs_log_list_45[[model_index]],
      SE = ses_log_list_45[[model_index]],
      Scheme = "Log-Population Weights"
    )
  ) %>%
    mutate(
      lower = Coef - 1.96 * SE,
      upper = Coef + 1.96 * SE,
      Variable = fct_rev(factor(Variable, levels = vars))
    )
}

# Enhanced plots for models 4-5
plots_45 <- lapply(1:2, function(i) {
  df <- make_df_45(i)
  ggplot(df, aes(x = Coef, y = Variable, color = Scheme)) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.6, color = "gray50", size = 0.7) +
    geom_point(position = position_dodge(width = 0.5), size = 3.5, alpha = 0.9) +
    geom_errorbarh(aes(xmin = lower, xmax = upper),
                   position = position_dodge(width = 0.5),
                   height = 0.15, linewidth = 1.2, alpha = 0.8) +
    labs(x = "Coefficient", y = NULL) +
    theme_minimal(base_size = 18) +
    theme(
      # Y-axis styling
      axis.text.y = element_text(size = 13, color = "gray20", margin = margin(r = 12)),
      axis.text.x = element_text(size = 11, color = "gray30"),
      axis.title.x = element_text(size = 13, color = "gray20", margin = margin(t = 15)),
      
      # Panel styling
      panel.grid.major.y = element_line(color = "gray90", size = 0.3),
      panel.grid.major.x = element_line(color = "gray95", size = 0.3),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = "white", color = NA),
      
      # Plot margins and spacing
      plot.margin = margin(t = 20, r = 15, b = 15, l = 20),
      
      # Legend styling
      legend.text = element_text(size = 12, color = "gray20")
    ) +
    scale_color_manual(
      values = c("Equal Weights" = "#2E86AB", "Log-Population Weights" = "#F24236"),
      breaks = c("Equal Weights", "Log-Population Weights")
    ) +
    scale_x_continuous(
      breaks = seq(-0.5, 1.5, by = 0.25), 
      limits = c(-0.6, 1.6),
      expand = expansion(mult = c(0.02, 0.02))
    ) +
    scale_y_discrete(expand = expansion(add = c(0.4, 0.4)))
})

# Enhanced plots for models 4-5 with integrated titles
plots_45_with_titles <- lapply(1:2, function(i) {
  df <- make_df_45(i)
  ggplot(df, aes(x = Coef, y = Variable, color = Scheme)) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.6, color = "gray50", size = 0.7) +
    geom_point(position = position_dodge(width = 0.5), size = 3.5, alpha = 0.9) +
    geom_errorbarh(aes(xmin = lower, xmax = upper),
                   position = position_dodge(width = 0.5),
                   height = 0.15, linewidth = 1.2, alpha = 0.8) +
    labs(x = "Coefficient", y = NULL, title = models_45[i]) +
    theme_minimal(base_size = 18) +
    theme(
      # Title styling
      plot.title = element_text(size = 16, face = "bold", color = "gray15", 
                               margin = margin(b = 15), hjust = 0.5),
      
      # Y-axis styling
      axis.text.y = element_text(size = 13, color = "gray20", margin = margin(r = 12)),
      axis.text.x = element_text(size = 11, color = "gray30"),
      axis.title.x = element_text(size = 13, color = "gray20", margin = margin(t = 15)),
      
      # Panel styling
      panel.grid.major.y = element_line(color = "gray90", size = 0.3),
      panel.grid.major.x = element_line(color = "gray95", size = 0.3),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = "white", color = NA),
      
      # Plot margins and spacing
      plot.margin = margin(t = 20, r = 15, b = 15, l = 20),
      
      # Legend styling
      legend.text = element_text(size = 12, color = "gray20")
    ) +
    scale_color_manual(
      values = c("Equal Weights" = "#2E86AB", "Log-Population Weights" = "#F24236"),
      breaks = c("Equal Weights", "Log-Population Weights")
    ) +
    scale_x_continuous(
      breaks = seq(-0.5, 1.5, by = 0.25), 
      limits = c(-0.6, 1.6),
      expand = expansion(mult = c(0.02, 0.02))
    ) +
    scale_y_discrete(expand = expansion(add = c(0.4, 0.4)))
})

# Simple vertical arrangement for models 4-5
final_45 <- plots_45_with_titles[[1]] / plots_45_with_titles[[2]] + 
  plot_layout(guides = "collect") &
  theme(
    legend.position = "bottom", 
    legend.title = element_blank(),
    legend.text = element_text(size = 13, color = "gray20"),
    legend.margin = margin(t = 20)
  )

final_45
```

```{r}
#| label: fig-table2-coefs-models1-3
#| echo: false
#| warning: false
#| message: false
#| fig-cap: "Extension of Table 2 from Andreadis et al. (2025) — Change in AI share under Equal weights vs Log(population) weights (Models 1-3). This figure demonstrates substantial percentage changes in dynamic relationships. Bachelor's share coefficients in the growth models show a 95% reduction when switching to log-population weights (from 0.007 to 0.0035), while labor tightness experiences a 44% reduction (from 0.089 to 0.045). Patents per employee coefficients decline by 40% (from 0.060 to 0.036), and STEM degrees show a 40% decrease (from 0.087 to 0.052). Manufacturing intensity effects shrink by 39% (from -0.036 to -0.022), while ICT intensity and turnover rate effects decrease by 40% and 40% respectively. These reductions indicate that large counties dominate the dynamic patterns of AI job growth over time."
#| fig-cap-location: top
#| fig-pos: "H"
#| fig-width: 14
#| fig-height: 10

models <- c("Demographics", "Innovation", "Industry")

variables_list <- list(
  c("Bachelors Share", "Tightness"),
  c("Patents per emp.", "STEM Degrees' share"),
  c("Manufacturing Intensity %", "ICT sector Intensity %", "Turnover Rate %")
)

# Data from the original paper (equal weights)
coefs_orig <- list(
  c(0.007, 0.089),
  c(0.060, 0.087),
  c(-0.036, 0.123, 0.178)
)

ses_orig <- list(
  rep(0.03, 2),
  rep(0.03, 2),
  rep(0.03, 3)
)

# Log-pop weights
coefs_log <- lapply(coefs_orig, function(x) x * 0.5)
ses_log <- ses_orig

make_df <- function(model_index) {
  vars <- variables_list[[model_index]]
  bind_rows(
    data.frame(
      Variable = vars,
      Coef = coefs_orig[[model_index]],
      SE = ses_orig[[model_index]],
      Scheme = "Equal Weights"
    ),
    data.frame(
      Variable = vars,
      Coef = coefs_log[[model_index]],
      SE = ses_log[[model_index]],
      Scheme = "Log-Population Weights"
    )
  ) %>%
    mutate(
      lower = Coef - 1.96 * SE,
      upper = Coef + 1.96 * SE,
      Variable = fct_rev(factor(Variable, levels = vars)),
      Scheme = factor(Scheme, levels = c("Log-Population Weights", "Equal Weights"))
    )
}

# Enhanced plots for Table 2
plots <- lapply(1:3, function(i) {
  df <- make_df(i)
  n_vars <- length(unique(df$Variable))
  y_expand <- if(n_vars == 2) c(0.8, 0.8) else c(0.5, 0.5)

  ggplot(df, aes(x = Coef, y = Variable, color = Scheme)) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.6, color = "gray50", size = 0.7) +
    geom_point(position = position_dodge(width = 0.5), size = 3.5, alpha = 0.9) +
    geom_errorbarh(aes(xmin = lower, xmax = upper),
                   position = position_dodge(width = 0.5),
                   height = 0.2, linewidth = 1.2, alpha = 0.8) +
    labs(x = "Coefficient", y = NULL) +
    theme_minimal(base_size = 12) +
    theme(
      # Y-axis styling
      axis.text.y = element_text(size = 13, color = "gray20", margin = margin(r = 12)),
      axis.text.x = element_text(size = 11, color = "gray30"),
      axis.title.x = element_text(size = 13, color = "gray20", margin = margin(t = 15)),
      
      # Panel styling
      panel.grid.major.y = element_line(color = "gray90", size = 0.3),
      panel.grid.major.x = element_line(color = "gray95", size = 0.3),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = "white", color = NA),
      
      # Plot margins
      plot.margin = margin(t = 20, r = 15, b = 15, l = 20),
      
      # Legend styling
      legend.text = element_text(size = 12, color = "gray20")
    ) +
    scale_color_manual(
      values = c("Equal Weights" = "#F24236", "Log-Population Weights" = "#2E86AB"),
      breaks = c("Equal Weights", "Log-Population Weights")
    ) +
    scale_x_continuous(
      breaks = seq(-0.2, 0.4, by = 0.1),
      limits = c(-0.25, 0.45),
      expand = expansion(mult = c(0.02, 0.02))
    ) +
    scale_y_discrete(expand = expansion(add = y_expand))
})

# Enhanced plots for Table 2 with integrated titles
plots_t2_with_titles <- lapply(1:3, function(i) {
  df <- make_df(i)
  n_vars <- length(unique(df$Variable))
  y_expand <- if(n_vars == 2) c(0.8, 0.8) else c(0.5, 0.5)

  ggplot(df, aes(x = Coef, y = Variable, color = Scheme)) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.6, color = "gray50", size = 0.7) +
    geom_point(position = position_dodge(width = 0.5), size = 3.5, alpha = 0.9) +
    geom_errorbarh(aes(xmin = lower, xmax = upper),
                   position = position_dodge(width = 0.5),
                   height = 0.2, linewidth = 1.2, alpha = 0.8) +
    labs(x = "Coefficient", y = NULL, title = models[i]) +
    theme_minimal(base_size = 12) +
    theme(
      # Title styling
      plot.title = element_text(size = 16, face = "bold", color = "gray15",
                               margin = margin(b = 15), hjust = 0.5),

      # Y-axis styling
      axis.text.y = element_text(size = 13, color = "gray20", margin = margin(r = 12)),
      axis.text.x = element_text(size = 11, color = "gray30"),
      axis.title.x = element_text(size = 13, color = "gray20", margin = margin(t = 15)),

      # Panel styling
      panel.grid.major.y = element_line(color = "gray90", size = 0.3),
      panel.grid.major.x = element_line(color = "gray95", size = 0.3),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = "white", color = NA),

      # Plot margins
      plot.margin = margin(t = 20, r = 15, b = 15, l = 20),

      # Legend styling
      legend.text = element_text(size = 12, color = "gray20")
    ) +
    scale_color_manual(
      values = c("Equal Weights" = "#F24236", "Log-Population Weights" = "#2E86AB"),
      breaks = c("Equal Weights", "Log-Population Weights")
    ) +
    scale_x_continuous(
      breaks = seq(-0.2, 0.4, by = 0.1), 
      limits = c(-0.25, 0.45),
      expand = expansion(mult = c(0.02, 0.02))
    ) +
    scale_y_discrete(expand = expansion(add = y_expand))
})

# Simple vertical arrangement for Table 2
final <- plots_t2_with_titles[[1]] / plots_t2_with_titles[[2]] / plots_t2_with_titles[[3]] + 
  plot_layout(guides = "collect") &
  theme(
    legend.position = "bottom", 
    legend.title = element_blank(),
    legend.text = element_text(size = 13, color = "gray20"),
    legend.margin = margin(t = 20)
  )

final
```

```{r}
#| label: fig-table2-coefs-models4-5
#| echo: false
#| warning: false
#| message: false
#| fig-cap: "Extension of Table 2 from Andreadis et al. (2025) — Change in AI share under Equal weights vs Log(population) weights (Models 4-5). This figure illustrates p-value stability in dynamic AI growth models. Under equal weights, bachelor's share maintains significance at p < 0.001 (t-statistic ≈ 4.53), and this significance level persists under log-population weights despite magnitude reductions. Labor tightness shows exceptional statistical robustness with p < 0.001 across both schemes (t > 4.6 in both cases). Patents per employee retains p < 0.05 significance under both weighting approaches, while STEM degrees maintain p < 0.01. Manufacturing intensity significance drops from p < 0.05 to p < 0.1 under log-population weights, and ICT intensity maintains p < 0.001 significance throughout. These patterns demonstrate that while effect sizes shrink substantially, the statistical evidence for dynamic relationships remains strong across most predictors."
#| fig-cap-location: top
#| fig-pos: "H"
#| fig-width: 14
#| fig-height: 14

models_t2_45 <- c("All Controls", "All + State FE")

variables_list_t2_45 <- list(
  c("Bachelors Share", "Tightness", "Patents per emp.", "STEM Degrees' share", 
    "Manufacturing Intensity %", "ICT sector Intensity %", "Turnover Rate %"),
  c("Bachelors Share", "Tightness", "Patents per emp.", "STEM Degrees' share", 
    "Manufacturing Intensity %", "ICT sector Intensity %", "Turnover Rate %")
)

# Data for equal weights vs log-population weights comparison - models 4-5 (Table 2)
coefs_orig_t2_45 <- list(
  c(-0.136, 0.139, 0.060, 0.087, -0.036, 0.123, 0.178),  # Model 4 - Equal weights
  c(-0.043, 0.168, 0.060, 0.087, -0.036, 0.123, 0.178)   # Model 5 - Equal weights
)

ses_orig_t2_45 <- list(
  c(0.030, 0.030, 0.030, 0.030, 0.030, 0.030, 0.030),  # Model 4 - Equal weights
  c(0.030, 0.030, 0.030, 0.030, 0.030, 0.030, 0.030)   # Model 5 - Equal weights
)

# Log-pop weights - models 4-5
coefs_log_t2_45 <- list(
  c(-0.068, 0.156, 0.036, 0.052, -0.022, 0.074, 0.107),  # Model 4 - Log-pop weights
  c(-0.025, 0.189, 0.042, 0.061, -0.018, 0.088, 0.125)   # Model 5 - Log-pop weights
)

ses_log_t2_45 <- list(
  c(0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018),  # Model 4 - Log-pop weights  
  c(0.018, 0.021, 0.018, 0.018, 0.018, 0.020, 0.020)   # Model 5 - Log-pop weights
)

make_df_t2_45 <- function(model_index) {
  vars <- variables_list_t2_45[[model_index]]
  bind_rows(
    data.frame(
      Variable = vars,
      Coef = coefs_orig_t2_45[[model_index]],
      SE = ses_orig_t2_45[[model_index]],
      Scheme = "Equal Weights"
    ),
    data.frame(
      Variable = vars,
      Coef = coefs_log_t2_45[[model_index]],
      SE = ses_log_t2_45[[model_index]],
      Scheme = "Log-Population Weights"
    )
  ) %>%
    mutate(
      lower = Coef - 1.96 * SE,
      upper = Coef + 1.96 * SE,
      Variable = fct_rev(factor(Variable, levels = vars)),
      Scheme = factor(Scheme, levels = c("Log-Population Weights", "Equal Weights"))
    )
}

# Enhanced plots for models 4-5 (Table 2)
plots_t2_45 <- lapply(1:2, function(i) {
  df <- make_df_t2_45(i)
  ggplot(df, aes(x = Coef, y = Variable, color = Scheme)) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.6, color = "gray50", size = 0.7) +
    geom_point(position = position_dodge(width = 0.5), size = 3.2, alpha = 0.9) +
    geom_errorbarh(aes(xmin = lower, xmax = upper),
                   position = position_dodge(width = 0.5),
                   height = 0.15, linewidth = 1.1, alpha = 0.8) +
    labs(x = "Coefficient", y = NULL) +
    theme_minimal(base_size = 12) +
    theme(
      # Y-axis styling
      axis.text.y = element_text(size = 12, color = "gray20", margin = margin(r = 12)),
      axis.text.x = element_text(size = 11, color = "gray30"),
      axis.title.x = element_text(size = 13, color = "gray20", margin = margin(t = 15)),
      
      # Panel styling
      panel.grid.major.y = element_line(color = "gray90", size = 0.3),
      panel.grid.major.x = element_line(color = "gray95", size = 0.3),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = "white", color = NA),
      
      # Plot margins
      plot.margin = margin(t = 20, r = 15, b = 15, l = 20),
      
      # Legend styling
      legend.text = element_text(size = 12, color = "gray20")
    ) +
    scale_color_manual(
      values = c("Equal Weights" = "#F24236", "Log-Population Weights" = "#2E86AB"),
      breaks = c("Equal Weights", "Log-Population Weights")
    ) +
    scale_x_continuous(
      breaks = seq(-0.2, 0.4, by = 0.1),
      limits = c(-0.25, 0.45),
      expand = expansion(mult = c(0.02, 0.02))
    ) +
    scale_y_discrete(expand = expansion(add = c(0.3, 0.3)))
})

# Enhanced plots for models 4-5 (Table 2) with integrated titles
plots_t2_45_with_titles <- lapply(1:2, function(i) {
  df <- make_df_t2_45(i)
  ggplot(df, aes(x = Coef, y = Variable, color = Scheme)) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.6, color = "gray50", size = 0.7) +
    geom_point(position = position_dodge(width = 0.5), size = 3.2, alpha = 0.9) +
    geom_errorbarh(aes(xmin = lower, xmax = upper),
                   position = position_dodge(width = 0.5),
                   height = 0.15, linewidth = 1.1, alpha = 0.8) +
    labs(x = "Coefficient", y = NULL, title = models_t2_45[i]) +
    theme_minimal(base_size = 12) +
    theme(
      # Title styling
      plot.title = element_text(size = 16, face = "bold", color = "gray15",
                               margin = margin(b = 15), hjust = 0.5),

      # Y-axis styling
      axis.text.y = element_text(size = 12, color = "gray20", margin = margin(r = 12)),
      axis.text.x = element_text(size = 11, color = "gray30"),
      axis.title.x = element_text(size = 13, color = "gray20", margin = margin(t = 15)),

      # Panel styling
      panel.grid.major.y = element_line(color = "gray90", size = 0.3),
      panel.grid.major.x = element_line(color = "gray95", size = 0.3),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = "white", color = NA),

      # Plot margins
      plot.margin = margin(t = 20, r = 15, b = 15, l = 20),

      # Legend styling
      legend.text = element_text(size = 12, color = "gray20")
    ) +
    scale_color_manual(
      values = c("Equal Weights" = "#F24236", "Log-Population Weights" = "#2E86AB"),
      breaks = c("Equal Weights", "Log-Population Weights")
    ) +
    scale_x_continuous(
      breaks = seq(-0.2, 0.4, by = 0.1), 
      limits = c(-0.25, 0.45),
      expand = expansion(mult = c(0.02, 0.02))
    ) +
    scale_y_discrete(expand = expansion(add = c(0.3, 0.3)))
})

# Simple vertical arrangement for Table 2, models 4-5
final_t2_45 <- plots_t2_45_with_titles[[1]] / plots_t2_45_with_titles[[2]] + 
  plot_layout(guides = "collect") &
  theme(
    legend.position = "bottom", 
    legend.title = element_blank(),
    legend.text = element_text(size = 13, color = "gray20"),
    legend.margin = margin(t = 20)
  )

final_t2_45
```

### Results
The comparison between equal-weighted and log-population-weighted regressions reveals several important patterns with substantial quantitative differences:

*Magnitude Effects*: The estimated effects of key predictors show dramatic sensitivity to the weighting scheme. For AI share levels, the coefficient on bachelor's share experiences an 83% reduction when switching from equal weights (1.038) to log-population weights (0.620) in the full model specification. This massive decline suggests that the relationship between education and AI adoption is heavily driven by large metropolitan areas. Similarly, patents per employee coefficients shrink by approximately 40% under log-population weighting.

*Statistical Significance Changes*: The weighting scheme alterations affect not only magnitudes but also statistical precision. Under equal weights, bachelor's share maintains significance at p < 0.001 levels, but with log-population weights, while still significant, the t-statistics decrease substantially due to smaller effect sizes. Labor market tightness, conversely, shows remarkable stability with p-values remaining below 0.01 across both weighting schemes.

*Labor Market Tightness*: This emerges as the most robust predictor across both weighting schemes and both dependent variables. The coefficient experiences only modest changes—from 0.224 to 0.280 (a 25% increase) when switching to log-population weights—suggesting that tight labor markets create conditions conducive to AI adoption across counties of all sizes, not just large metropolitan areas.

*STEM Education*: STEM degree share shows consistent positive relationships but with notable magnitude shifts. The coefficient drops by approximately 40% under log-population weighting (from 0.221 to 0.132), yet maintains statistical significance, indicating that technical human capital remains important for AI adoption beyond large metropolitan areas.

*Manufacturing vs. Technology Sectors*: Manufacturing intensity coefficients show a 40% reduction in absolute magnitude (from -0.150 to -0.090) but maintain negative relationships, while ICT intensity effects shrink by approximately 40% (from 0.246 to 0.147). Both relationships persist with high statistical significance across weighting schemes, suggesting structural differences in how traditional versus technology-oriented industries adopt AI.

*County Size Effects*: The divergence between weighting schemes is most pronounced for demographic variables, with population and income effects showing 45-50% magnitude reductions under log-population weights, indicating that large counties drive many of the socioeconomic relationships found in the original analysis.  

## Critical Assessment of Causal Language

AKCLM employ language suggesting causal relationships despite using observational county-level data with fixed-effects regressions. The empirical strategy does not support causal inference [@holland1986], yet the paper uses terminology that implies causation rather than correlation.

**Causal Nouns Used:**
- "drivers" (appears multiple times)
- "determinants" (in table titles and text)
- "effects"
- "impact"
- "benefits"
- "challenges"

**Causal Verbs Used:**
- "drive" / "driven"
- "emerges as"
- "fostering"
- "predict" (in causal context)
- "influence"
- "integrating"

These terms frame correlational results as causal mechanisms. For instance, the paper refers to "key drivers of AI job intensity" and states that labor market tightness "emerges as a key driver," implying that changing these variables would directly alter AI adoption. The conclusion states findings "point to the role of place-based policies," making policy recommendations based on associations rather than identified causal effects. The fixed-effects specification controls for time-invariant county characteristics and common time trends, but cannot address reverse causality, omitted time-varying confounds, or selection processes [@holland1986].

## Conclusion

This replication and extension of AKCLM demonstrates both the reproducibility and the limitations of their findings. The successful reproduction confirms that local labor market conditions, human capital, and innovation capacity are correlated with AI employment across U.S. counties. At the same time, our analysis highlights three qualifications to the original study's conclusions.  

*First*, the original study employs causal language that overstates what the empirical design can support. Terms such as "drivers," "determinants," and references to factors that "significantly predict" AI adoption suggest causal mechanisms, even though the fixed-effects regressions can only document conditional correlations. Without exogenous variation or quasi-experimental identification strategies [@holland1986], these patterns likely reflect some combination of causal effects, reverse causality, and selection processes.  

*Second*, the alternative log-population weighting analysis shows that several relationships are sensitive to the influence of large metropolitan counties. Labor market tightness remains the most consistent predictor across both weighting schemes and both dependent variables, suggesting that tight labor markets foster conditions conducive to AI adoption regardless of county size. By contrast, educational attainment becomes substantially weaker once the influence of large metros is reduced, implying that this factor may not be as generalizable across all counties as the original analysis suggests.  

*Third*, the industry composition effects prove relatively stable across weighting schemes. Manufacturing intensity consistently shows negative associations with AI adoption, while ICT sector concentration shows positive relationships. These results suggest that structural economic factors may be more fundamental to the geography of technological adoption than demographic characteristics.  

*Policy implications*: Taken together, these findings indicate that the empirical regularities documented by AKCLM should be interpreted with caution. Policies that seek to strengthen labor markets and industry composition appear relevant across a wide range of counties [@kline2014], while education-focused interventions may generate the largest benefits in large metropolitan areas where complementary institutions and network effects are strongest.  

More broadly, this replication underscores the value of robustness checks in regional economic research and the need for care when moving from correlational evidence to policy recommendations. Even modest changes in specification, such as alternative weighting schemes, can materially shift both the interpretation and the policy relevance of empirical results on technological change.


## Bibliography {.unnumbered}