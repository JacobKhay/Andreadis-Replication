---
title: "Local Heterogeneity in Artificial Intelligence Jobs Over Time and Space"
subtitle: "A Replication Study of Andreadis et al. (AEA Papers and Proceedings, 2025)"
author:
  - name: Jacob Khaykin^[Solon High School, [jacobkhaykin27@solonschools.net](mailto:jacobkhaykin27@solonschools.net)]
  - name: David Kane^[New College of Florida, [dakane@ncf.edu](mailto:dakane@ncf.edu)]
bibliography: references.bib
csl: aea.csl
link-citations: true
nocite: '@*'
format:
  pdf:
    keep-tex: true
    number-sections: true
    documentclass: article
    geometry: 
      - margin=1in
    include-in-header:
      - text: |
          \usepackage{url}
          \urlstyle{same}
          \def\UrlBreaks{\do\/\do-\do.\do:}
          \usepackage{float}
          \usepackage{array}
          \usepackage{siunitx}
          \usepackage{dcolumn}
          \newcolumntype{d}[1]{D{.}{.}{#1}}
          % reset numbering after abstract
          \usepackage{etoolbox}
          \AtBeginEnvironment{abstract}{\setcounter{section}{0}}
          % Custom table row spacing
          \renewcommand{\arraystretch}{0.9}
          % Configure siunitx for modern versions (>=3)
          \sisetup{
            detect-all,
            input-symbols = (),
            input-signs = + -,
            table-align-text-pre = false,
            table-number-alignment = center,
            table-format = +1.4,
            group-digits = false,
            table-space-text-pre = ***,
            table-space-text-post = ***,
            mode = math
          }
  docx: default
---


```{r}
#| label: setup
#| include: false
# Load all required packages
library(tidyverse)
library(fixest)
library(modelsummary)
library(kableExtra)
library(sf)
library(scales)
library(tigris)
library(ggplot2)
library(patchwork)
library(grid)
library(usmap)
library(viridis)
library(dplyr)
library(forcats)

# Set options
options(tigris_use_cache = TRUE)
```

<!-- DK:  -->


 <!-- Study the paper more closely, so you understand what it happening. Don't understand? Ask AI. Or ask me! -->

<!-- Work on Map 2. Bet that the percentage change should be:

(AI jobs in 2023 -  AI jobs in 2018) / AI jobs in 2018 -->


<!-- Open issue: Is their method for getting down to 24,000 rows for the regressions sensible? How much do results change if we fix this process? Worth doing? -->

<!-- The written portion of the paper must report all the most important aspects of the table, without assuming that the reader has looked at the table or the notes to the table. -->

<!-- All write up in terms of comparing two groups, e.g., counties with average home price compared with counties with home price growth one standard deviation higher. The second group of counties has AI job percentage with is  1.5% (?) lower. Put in section 3. -->

<!-- Does the coefficient of Bachelors Share in the first regression mean 19% higher or 0.19% higher, when comparing counties that differ by one standard deviation in Bachelor's Share. 0.19% higher, wrote that in the notes and in section 3. -->

<!-- Add text for Figure from paper and for our figures. -->


<!-- Dependent variable discussion should include both.  -->

<!-- Understand what the code is doing. AI can help! Also, the code for your extensions, should be clean and understandable. For example, you only download data.csv once, even though they download it lots of times. -->

<!-- DK: Mostly replace phrases like original study with AKCLM. -->


Understand 

*JEL: J24, O33, R11*

*Keywords:* Artificial Intelligence, Regional Economics, Labor Markets

*Data Availability:* The R code and data to reproduce this replication are available in this repository: https://github.com/JacobKhay/Andreadis-Replication.


## Abstract {.unnumbered}

We partially replicate @andreadis2025 on the correlation between the level and growth of artificial intelligence employment across U.S. counties from 2014 to 2023 and a collection of variables involving education, innovation, and industry. We successfully reproduce most of their results and extend their analysis by using log-population weights to assess robustness. The core associations persist. We also critique their unsupported causal claims.

Declaration: There are no financial conflicts of interest to share. 

\newpage

## Introduction

This paper replicates the analysis of @andreadis2025, henceforth AKCLM, on the correlation between the level and growth artificial intelligence employment openings and education, innovation, and industry factors across U.S. counties from 2014 to 2023. Understanding where AI employment emerges and how it spreads is important for both researchers and policymakers, since the rise of AI has the potential to reshape regional economies [@acemoglu2024; @brynjolfsson2021], alter the demand for skills [@autor2015; @acemoglu2019], and shift patterns of innovation [@babina2024]. County-level variation offers a granular perspective on how these transformations take hold across the United States.

<!-- DK: Either years desccribed explicitly in both or in neither. Should be both! And mention the US! -->

There are two outcome variables. First, the share of AI-related job postings at the county-year level captures the absolute level of AI employment opportunities in a given county and year. Second, the percentage point change in AI job share between 2017-2018 and 2022-2023 measures how AI employment growth varies across counties. Together, these outcomes distinguish between where AI jobs are concentrated (levels) and where AI adoption is accelerating most rapidly (growth). Tracking both dynamics provides insight into which regions gain early access to AI-driven employment and which experience the fastest expansion.

The original study links county-level AI employment to several explanatory factors. Education is captured by the share of adults with a college degree, innovation by local patenting activity [@giczy2022], and industry factors by the composition of employment across sectors. Together, these variables represent structural characteristics that might condition whether a region becomes a hub for AI-related work.

AKCLM find that both AI employment levels and growth are associated with higher educational attainment, more innovation activity, and certain industry profiles. For the level of AI jobs, bachelor's degree share, labor market tightness, and STEM education are positively associated, while manufacturing intensity shows a negative association. For AI job growth, STEM degrees and labor market tightness remain significantly correlated, while turnover rates show a negative relationship. These associations are robust across specifications, suggesting that counties with strong human capital, innovative capacity, and aligned industries experience both higher AI employment levels and faster AI adoption. The results highlight structural divides in access to AI employment opportunities across the country.

<!-- DK: More room to add stuff in this paragraph. Which variables? Which matter most? Added that here-->

We reproduce these results and then extend the analysis by using log-population weights. Under this adjustment, the core associations persist for both outcomes but the estimated magnitudes generally shrink. For AI job levels, labor market tightness shows the most robust association across weighting schemes, while bachelor's share shows the largest magnitude decline under log-population weights. For AI job growth, STEM degree share and labor market tightness show the most consistent correlations, though effect sizes diminish when larger counties receive less weight. 



Finally, we note that AKCLM's causal interpretations are not supported by the observational nature of the their data [@holland1986]. While the correlations are informative and highlight important regional patterns, they do not establish that education, innovation, or industry factors directly cause higher AI employment. Our replication underscores the value of the evidence while also emphasizing the limits of what can be inferred from the design.

## Data

AKCLM utilize multiple data sources to construct a comprehensive county-level dataset spanning 2014–2023:

*AI Employment Data*: Job posting data from Lightcast [@beckett2023], which aggregates information from over 40,000 online job boards, newspapers, and employer websites. AI-related jobs are identified through skills and keywords associated with AI development and use [@acemoglu2022]. The first dependent variable, AI job share $AI_{it}$, is defined as:

$$
AI_{it} = \frac{\text{AI job postings}_{it}}{\text{Total job postings}_{it}} \times 100 \tag{1}
$$

The second dependent variable captures the percentage point change in AI job share between the 2017-2018 average and the 2022-2023 average: $\Delta AI_i = \overline{AI}_{i,2022-2023} - \overline{AI}_{i,2017-2018}$.

*Demographic Variables*: From the American Community Survey [@census_acs], we include bachelor's share (percentage of workforce with bachelor's degree or higher), black population share (percentage of county population identifying as Black), poverty share (percentage of population below federal poverty line), log population (natural logarithm of county population), and log median income (natural logarithm of median household income).

*Innovation Indicators*: We measure patents per employee (USPTO patent counts normalized by employment), AI patents share (percentage of patents classified as AI-related) [@giczy2022], STEM degrees share (percentage of awarded degrees in STEM fields), and degrees per capita (total degrees awarded per capita).

*Industry and Labor Market Variables*: These include labor market tightness (ratio of job postings to unemployed workers) [@bls_laus], manufacturing intensity (employment share in manufacturing sector) [@census_cbp], ICT intensity (employment share in information and communication technology), turnover rate (worker separation rate from Quarterly Workforce Indicators) [@census_qwi], and large establishments share (percentage of employment in large firms).

*Housing Market*: We include house price growth from Federal Housing Finance Agency [@bogin2019].  

All explanatory variables are lagged by one year. A significant statistical issue arises from the construction of the dependent and independent variables in the original analysis. The dependent variable, AI intensity, is defined as AI job postings divided by total job postings, where total job postings appear in the denominator. Meanwhile, labor market tightness, one of the key independent variables, is constructed as job postings divided by unemployed workers, where job postings appear in the numerator. This creates denominator contamination or mechanical endogeneity, where both sides of the regression share the same underlying quantity (job postings), inducing a built-in correlation that exists purely through algebraic construction rather than economic causation. When total job postings increase, AI intensity can mechanically fall even if AI postings remain constant (due to the denominator effect), while labor market tightness mechanically increases (due to the numerator effect). This creates a spurious negative correlation between the dependent variable and the tightness regressor that has nothing to do with actual economic relationships, as the regression can pick up patterns driven by algebra rather than economics. This is a violation of OLS assumptions because the regressor embeds a component of the dependent variable's denominator, making them statistically non-independent by construction. This can bias coefficient estimates and overstate statistical significance. The issue is analogous to regressing $Y/Z$ on $X/Z$ where the shared $Z$ induces correlation even when $Y$ and $X$ are truly independent. Other independent variables (industry shares, patents per worker, education, income, turnover) do not share measurement components with the dependent variable's construction, so they remain valid in the standard OLS framework. This mechanical relationship should be interpreted with caution, as the observed correlation between AI intensity and labor market tightness may partially reflect shared measurement error rather than a true causal or correlational economic relationship.

## Replication

We used R [@r1996], a free and open-source statistical computing environment, and the tidyverse collection of packages [@tidyverse]. To estimate and display regression models, we used the fixest [@fixest] and modelsummary [@modelsummary] packages.

We attempted to reproduce Table 1, Table 2, amd Figure 1 from AKCLM. The replication confirms most of the authors' key empirical findings regarding the correlates of AI employment across U.S. counties, as described in Tables 1 and 2. Nearly all coefficients and standard errors match the original results. However, the coefficient and standard error for Bachelor's share is inconsistent with result published in Table 2. We can not reproduce the maps in Figure 1, not least because the AKCLM's data does not include values for counties in Connecticut.


```{r}
#| label: tbl-table1
#| echo: false
#| warning: false
#| message: false
#| tbl-cap: "Replication of Table 1 from Andreadis et al. (2025) - The Correlates of the Share of Artificial Intelligence Jobs"
#| tbl-cap-location: top
#| tbl-pos: "H"

# Load data
data_ai <- read_csv("data.csv") %>% 
  mutate(state_year = paste0(state, Year)) %>% 
  mutate(ai_intensity = ai / nads)

# Prepare demographics
data_ai <- data_ai %>% 
  mutate(logincome = log(medhhincome), loghpi = log(hpi), logemp = log(pop_above18))

data_ai <- data_ai %>% 
  mutate(lads = log(1 + nads))

data_ai <- data_ai %>% 
  mutate(lads0 = log(nads))

# Log number of patents, inventors, etc.
data_ai <- data_ai %>% 
  mutate(logemp = log(Employed)) %>% 
  mutate(logpop = log(pop))

data_ai <- data_ai %>% 
  mutate(
    logn_patents = log(1 + n_patents),
    logn_inventors = log(1 + n_inventors),
    logai_patents = log(1 + ai_patents),
    logai_inventors = log(1 + ai_inventors)
  ) %>%
  mutate(pat_intensity = n_inventors / Employed, patai_intensity = ai_patents / n_patents) %>% 
  mutate(patai_intensity = replace_na(patai_intensity, 0))

# Industry structure
data_ai <- data_ai %>% 
  mutate(
    small_firms = small / est,
    large_firms = 1 - (small + medium) / est,
    management_intensity = management_emp / emp,
    information_intensity = information_emp / emp,
    manuf_intensity = manuf_emp / emp
  )

data_ai <- data_ai %>% 
  mutate(information_intensity = replace_na(information_intensity, 0))

# Education variables
data_ai <- data_ai %>% 
  ungroup() %>%  
  mutate(
    degshare = (udeg + mdeg) / Employed,
    stemshare = (ustemdeg + mstemdeg) / (udeg + mdeg),
    stemshare2 = (ustemdeg) / (udeg)
  )

data_ai <- data_ai %>%  
  mutate(stemshare = replace_na(stemshare, 0), stemshare2 = replace_na(stemshare2, 0))

# Labor market variables
data_ai <- data_ai %>%  
  mutate(tightness = nads / Unemployed)

data_ai <- data_ai %>%  
  mutate(hpi_ch = hpi_ch / 100)

# Z-scores - VERSION ROBUST with as.numeric()
data_ai_z <- data_ai %>%
  filter(emp != 0) %>%
  mutate(
    pop_raw = pop,        # Save raw population for weighting
    logpop_raw = logpop,  # Save raw log(pop) for weighting
    logemp_raw = logemp,  # Save raw log(employment) for weighting
    share_bac = as.numeric(scale(share_bac)),
    share_black = as.numeric(scale(share_black)),
    share_poverty = as.numeric(scale(share_poverty)),
    logpop = as.numeric(scale(logpop)),
    hpi_ch = as.numeric(scale(hpi_ch)),
    logincome = as.numeric(scale(logincome)),
    tightness = as.numeric(scale(tightness)),
    unrate = as.numeric(scale(unrate)),
    pat_intensity = as.numeric(scale(pat_intensity)),
    patai_intensity = as.numeric(scale(patai_intensity)),
    degshare = as.numeric(scale(degshare)),
    stemshare = as.numeric(scale(stemshare)),
    large_firms = as.numeric(scale(large_firms)),
    information_intensity = as.numeric(scale(information_intensity)),
    manuf_intensity = as.numeric(scale(manuf_intensity)),
    TurnOvrS = as.numeric(scale(TurnOvrS))
  ) %>%
  mutate(ai_intensity = ai_intensity * 100)

# Run regressions
est_demog_no <- fixest::feols(
  "ai_intensity ~ share_bac + share_black + share_poverty + logpop + hpi_ch + logincome + tightness | Year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z %>% drop_na(share_bac, share_black, share_poverty, logpop, hpi_ch, logincome,
                        tightness, pat_intensity, patai_intensity, degshare, stemshare,
                        large_firms, information_intensity, manuf_intensity, TurnOvrS),
  cluster = "COUNTY_FIPS",
  weights = ~lads
)

est_innovation_no <- fixest::feols(
  "ai_intensity ~ pat_intensity + patai_intensity + degshare + stemshare | Year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z %>% drop_na(share_bac, share_black, share_poverty, logpop, hpi_ch, logincome,
                        tightness, pat_intensity, patai_intensity, degshare, stemshare,
                        large_firms, information_intensity, manuf_intensity, TurnOvrS),
  cluster = "COUNTY_FIPS",
  weights = ~lads
)

est_industry_no <- fixest::feols(
  "ai_intensity ~ large_firms + information_intensity + manuf_intensity + TurnOvrS | Year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z %>% drop_na(share_bac, share_black, share_poverty, logpop, hpi_ch, logincome,
                        tightness, pat_intensity, patai_intensity, degshare, stemshare,
                        large_firms, information_intensity, manuf_intensity, TurnOvrS),
  cluster = "COUNTY_FIPS",
  weights = ~lads
)

est_all <- fixest::feols(
  "ai_intensity ~ share_bac + share_black + share_poverty + logpop + hpi_ch + logincome + tightness + pat_intensity + patai_intensity + degshare + stemshare + large_firms + information_intensity + manuf_intensity + TurnOvrS | Year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z,
  cluster = "COUNTY_FIPS",
  weights = ~lads
)

est_all_state_year <- fixest::feols(
  "ai_intensity ~ share_bac + share_black + share_poverty + logpop + hpi_ch + logincome + tightness + pat_intensity + patai_intensity + degshare + stemshare + large_firms + information_intensity + manuf_intensity + TurnOvrS | state_year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z,
  vcov = "twoway",
  weights = ~lads
)

# Extract model statistics
models <- list(est_demog_no, est_innovation_no, est_industry_no, est_all, est_all_state_year)
obs <- map_chr(models, ~scales::comma(nobs(.x)))
r2_vals <- map_chr(models, ~sprintf("%.5f", r2(.x, "r2")))

# Create table footer
extra_rows <- tribble(
  ~term, ~`(1)`, ~`(2)`, ~`(3)`, ~`(4)`, ~`(5)`,
  "Fixed effects", "{\\hspace{0.5em}County, Year}", "{\\hspace{0.5em}County, Year}", "{\\hspace{0.5em}County, Year}", "{\\hspace{0.5em}County, Year}", "{\\hspace{0.5em}County, state-year}",
  "Observations", paste0("{\\hspace{0.5em}", obs[1], "}"), paste0("{\\hspace{0.5em}", obs[2], "}"), paste0("{\\hspace{0.5em}", obs[3], "}"), paste0("{\\hspace{0.5em}", obs[4], "}"), paste0("{\\hspace{0.5em}", obs[5], "}"),
  "R²", paste0("{\\hspace{0.5em}", r2_vals[1], "}"), paste0("{\\hspace{0.5em}", r2_vals[2], "}"), paste0("{\\hspace{0.5em}", r2_vals[3], "}"), paste0("{\\hspace{0.5em}", r2_vals[4], "}"), paste0("{\\hspace{0.5em}", r2_vals[5], "}")
)

# Variable labels
labels <- c(
  "share_bac" = "Bachelor's share",
  "hpi_ch" = "House price growth",
  "tightness" = "Labor tightness",
  "pat_intensity" = "Patents per employee",
  "stemshare" = "STEM degrees share",
  "information_intensity" = "ICT sector intensity",
  "manuf_intensity" = "Manufacturing",
  "TurnOvrS" = "Turnover rate"
)

# Create note text
table_notes <- paste(
  "This table examines the demographic, innovation, and industry determinants",
  "of the share of artificial intelligence jobs across U.S. counties from 2014 to 2023.",
  "The dependent variable is AI job intensity, defined as the ratio of AI job postings",
  "to total job postings in each county-year, multiplied by 100 to express in percentage terms.",
  "AI job postings are identified from Lightcast data using skills and keywords associated",
  "with artificial intelligence. All explanatory variables are standardized as z-scores",
  "to ensure comparability across predictors. The sample comprises county-year observations",
  "covering approximately 3,100 U.S. counties annually over the ten-year period.",
  "All coefficients represent percentage point changes in AI job share for a one standard",
  "deviation increase in the explanatory variable. Models are estimated using ordinary least",
  "squares with county and year fixed effects (columns 1-4) or county and state-year fixed",
  "effects (column 5). Standard errors are clustered at the county level. All explanatory",
  "variables are lagged by one year."
)

# Create the table
modelsummary(
  list(
    "(1)" = est_demog_no,
    "(2)" = est_innovation_no,
    "(3)" = est_industry_no,
    "(4)" = est_all,
    "(5)" = est_all_state_year
  ),
  coef_map = labels,
  stars = FALSE,
  gof_omit = ".*",
  add_rows = extra_rows,
  escape = FALSE,
  statistic = "({std.error})",
  fmt = fmt_decimal(4),
  align = "lddddd",
  notes = table_notes
)
```


### Table 1 

Table 1 examines cross-sectional determinants of AI job intensity levels from 2014 to 2023. All coefficients represent standardized effects, allowing direct comparison of magnitude across predictors. Column 4 integrates all controls with county and year fixed effects and serves as the baseline model. We focus on these specifications for all comparisons.

Comparing labor market tightness (0.28, p < 0.001) with bachelor's share (0.18, p < 0.001) in Table 1, counties with one standard deviation higher labor market tightness exhibit 0.095 percentage points more AI jobs than similarly educated counties. Tightness shows 52% larger effect size than bachelor's share. Both coefficients are highly statistically significant with p-values well below 0.001, confirming these relationships are extremely unlikely due to chance. However, the effect sizes differ substantially: tightness accounts for 61% of mean AI intensity (0.45%) while bachelor's share accounts for 40%, indicating tightness has greater practical importance despite both being statistically reliable.

Within the innovation category of Table 1, comparing STEM degrees share (0.048, p < 0.05) with patents per employee (0.029, p < 0.01) reveals that STEM credentials show 62% larger effect size than patenting intensity. However, statistical significance diverges from effect size. Patents per employee shows stronger statistical evidence with p < 0.01, while STEM share has p < 0.05, meaning the patent coefficient is measured more precisely. Despite this stronger statistical evidence for patents, STEM shows larger practical importance: counties differing by one standard deviation in STEM credentials show 11% of mean AI intensity, while patent differences account for only 7%. Here, statistical precision favors patents but effect size favors STEM credentials.

Across industry factors in Table 1, ICT sector intensity (0.026, p = 0.058) and manufacturing intensity (-0.033, p < 0.01) exhibit similar effect sizes but opposite signs. Counties with one standard deviation higher ICT concentration have 0.059 percentage points more AI jobs than manufacturing-oriented counties with equivalent concentration. Manufacturing shows a 30% larger effect size than ICT (0.033 vs 0.026). Statistical significance also differs markedly. Manufacturing intensity is highly significant (p < 0.01), while ICT intensity narrowly misses the conventional 5% threshold (p = 0.058). This means we can be highly confident manufacturing negatively predicts AI jobs, but the positive ICT relationship could plausibly be due to chance. Here, manufacturing shows both larger effect size and higher statistical reliability.

Comparing education measures within Table 1, bachelor's share (0.18, p < 0.001) shows an effect size 3.8 times larger than STEM share (0.048, p < 0.05). Counties with one standard deviation higher bachelor's attainment have 0.13 percentage points more AI jobs than counties with equivalent STEM credential intensity. Both achieve statistical significance, but bachelor's share shows substantially larger effect size, accounting for 40% of mean AI intensity versus 11% for STEM credentials. The stronger statistical evidence for bachelor's share (p < 0.001 vs p < 0.05) combined with larger effect size indicates this relationship is both practically important and precisely measured.


```{r}
#| label: tbl-table2
#| echo: false
#| warning: false
#| message: false
#| tbl-cap: "Replication of Table 2 from Andreadis et al. (2025) - The Correlates of the Percentage Point Change in the Share of AI Jobs"
#| tbl-cap-location: top
#| tbl-pos: "H"

# Follow EXACT original code structure
data_ai_2017_2022 <- read_csv("data.csv") %>% 
  filter(Year == 2017 | Year == 2018 | Year == 2022 | Year == 2023) %>% 
  mutate(state_year = paste0(state, Year)) %>% 
  mutate(new = 1 * (Year > 2020)) %>% 
  group_by(new, COUNTY_FIPS) %>% 
  mutate(ai = sum(ai), nads = sum(nads)) %>% 
  mutate(ai_intensity = ai / nads) %>% 
  filter(Year == 2017 | Year == 2022) %>% 
  group_by(COUNTY_FIPS) %>% 
  mutate(
    dai_intensity = ai_intensity - lag(ai_intensity),
    dai_intensity9 = ai_intensity - lag(ai_intensity, 1)
  )

# Prepare demographics - EXACT as original
data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    logincome = log(medhhincome),
    loghpi = log(hpi),
    logemp = log(pop_above18),
    logim = log(1)  # Note: original has logim=log(immigration) but no immigration variable
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    unrate14 = lag(unrate, 1),
    logincome14 = lag(logincome, 1),
    loghpi14 = lag(loghpi, 1),
    share_bac14 = lag(share_bac, 1),
    share_black14 = lag(share_black, 1),
    share_poverty14 = lag(share_poverty, 1),
    logemp14 = lag(logemp, 1),
    logim14 = lag(logim, 1),
    medage14 = lag(medage, 1),
    median_rent14 = lag(median_rent, 1)
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    gincome = logincome - lag(logincome, 1),
    ghpi = loghpi - lag(loghpi, 1),
    gemp = logemp - lag(logemp, 1),
    gim = logim - lag(logim, 1)
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    dunrate = unrate - lag(unrate, 1),
    dshare_bac = share_bac - lag(share_bac, 1),
    dshare_black = share_black - lag(share_black, 1),
    dmedage14 = medage - lag(medage, 1)
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(lads = log(1 + nads))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(lads0 = log(nads))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    large_firms = 1 - (small + medium) / est,
    management_intensity = management_emp / emp,
    information_intensity = information_emp / emp,
    information_intensity = manuf_emp / emp  # Note: this overwrites previous line in original
  )

# Log number of patents, inventors, etc. - EXACT as original
data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(logemp = log(Employed)) %>% 
  mutate(logpop = log(pop))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    logn_patents = log(1 + n_patents),
    logn_inventors = log(1 + n_inventors),
    logai_patents = log(1 + ai_patents),
    logai_inventors = log(1 + ai_inventors)
  ) %>%
  mutate(
    pat_intensity = n_inventors / Employed,
    patai_intensity = ai_patents / n_patents
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    pat_intensity14 = lag(pat_intensity, 1),
    patai_intensity14 = lag(patai_intensity, 1)
  )

# Duplicate line in original
data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    pat_intensity14 = lag(pat_intensity, 1),
    patai_intensity14 = lag(patai_intensity, 1)
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    small_firms = small / est,
    large_firms = 1 - (small + medium) / est,
    management_intensity = management_emp / emp,
    information_intensity = information_emp / emp,
    manuf_intensity = manuf_emp / emp
  )

# Another duplicate section in original
data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    pat_intensity = n_inventors / Employed,
    patai_intensity = ai_patents / n_patents
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(patai_intensity = replace_na(patai_intensity, 0))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    pat_intensity14 = lag(pat_intensity, 1),
    patai_intensity14 = lag(patai_intensity, 1),
    lads14 = lag(lads, 1)
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    logn_inventors14 = lag(logn_inventors, 1),
    logai_inventors14 = lag(logai_inventors, 1),
    lads14 = lag(lads, 1)
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  ungroup() %>%  
  mutate(
    degshare = (udeg + mdeg) / Employed,
    stemshare = (ustemdeg + mstemdeg) / (udeg + mdeg)
  )

data_ai_2017_2022 <- data_ai_2017_2022 %>%  
  mutate(stemshare = replace_na(stemshare, 0))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(stemshare14 = lag(stemshare, 1))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(degshare14 = lag(degshare, 1))

data_ai_2017_2022 <- data_ai_2017_2022 %>%  
  mutate(tightness = nads / Unemployed)

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(tightness14 = lag(tightness, 1))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(hpi_ch14 = lag(hpi_ch, 1))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(logpop14 = lag(logpop, 1))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(logincome14 = lag(logincome, 1))

data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(gpop = (logpop - lag(logpop, 1)) / 5)

data_ai_2017_2022 <- data_ai_2017_2022 %>%  
  mutate(hpi_ch = hpi_ch / 100)

data_ai_2017_2022 <- data_ai_2017_2022 %>%  
  mutate(hpi_ch14 = hpi_ch14 / 100)

# Industry structure
data_ai_2017_2022 <- data_ai_2017_2022 %>% 
  mutate(
    large_firms14 = lag(large_firms, 1),
    information_intensity14 = lag(information_intensity, 1),
    manuf_intensity14 = lag(manuf_intensity, 1),
    TurnOvrS14 = lag(TurnOvrS, 1)
  )

# Z-scores - EXACT as original
data_ai_l_z <- data_ai_2017_2022 %>% 
  filter(emp != 0) %>% 
  drop_na(share_bac14) %>%
  mutate(
    share_bac14 = scale(share_bac14),
    share_black14 = scale(share_black14),
    share_poverty14 = scale(share_poverty14),
    logpop14 = scale(logpop14),
    hpi_ch14 = scale(hpi_ch14),
    logincome14 = scale(logincome14),
    tightness14 = scale(tightness14),
    pat_intensity14 = scale(pat_intensity14),
    patai_intensity14 = scale(patai_intensity14),
    degshare14 = scale(degshare14),
    stemshare14 = scale(stemshare14),
    large_firms14 = scale(large_firms14),
    information_intensity14 = scale(information_intensity14),
    manuf_intensity14 = scale(manuf_intensity14),
    TurnOvrS14 = scale(TurnOvrS14),
    unrate14 = scale(unrate14)
  ) %>% 
  mutate(dai_intensity9 = dai_intensity9 * 100)

# Run FULL models with ALL variables (not condensed)
est_demog_no <- fixest::feols(
  "dai_intensity9 ~ share_bac14 + share_black14 + share_poverty14 + gpop + hpi_ch14 + logincome14 + tightness14" %>% as.formula(),
  data_ai_l_z %>% 
    filter(dai_intensity9 > -5, dai_intensity9 < 10) %>% 
    drop_na(share_bac14, share_black14, share_poverty14, logpop14, hpi_ch14, logincome14,
            tightness14, pat_intensity14, patai_intensity14, degshare14, stemshare14,
            large_firms14, information_intensity14, manuf_intensity14, TurnOvrS14),
  weights = ~lads14
) 

est_innovation_no <- fixest::feols(
  "dai_intensity9 ~ pat_intensity14 + patai_intensity14 + degshare14 + stemshare14" %>% as.formula(),
  data_ai_l_z %>% 
    filter(dai_intensity9 > -5, dai_intensity9 < 10) %>% 
    drop_na(share_bac14, share_black14, share_poverty14, logpop14, hpi_ch14, logincome14,
            tightness14, pat_intensity14, patai_intensity14, degshare14, stemshare14,
            large_firms14, information_intensity14, manuf_intensity14, TurnOvrS14),
  weights = ~lads14
) 

est_industry_no <- fixest::feols(
  "dai_intensity9 ~ large_firms14 + information_intensity14 + manuf_intensity14 + TurnOvrS14" %>% as.formula(),
  data_ai_l_z %>% 
    filter(dai_intensity9 > -5, dai_intensity9 < 10) %>% 
    drop_na(share_bac14, share_black14, share_poverty14, logpop14, hpi_ch14, logincome14,
            tightness14, pat_intensity14, patai_intensity14, degshare14, stemshare14,
            large_firms14, information_intensity14, manuf_intensity14, TurnOvrS14),
  weights = ~lads14
)

est_all <- fixest::feols(
  "dai_intensity9 ~ share_bac14 + share_black14 + share_poverty14 + gpop + hpi_ch14 + logincome14 + tightness14 + pat_intensity14 + patai_intensity14 + degshare14 + stemshare14 + large_firms14 + information_intensity14 + manuf_intensity14 + TurnOvrS14" %>% as.formula(),
  data_ai_l_z %>% 
    filter(dai_intensity9 > -5, dai_intensity9 < 10) %>% 
    drop_na(share_bac14, share_black14, share_poverty14, logpop14, hpi_ch14, logincome14,
            tightness14, pat_intensity14, patai_intensity14, degshare14, stemshare14,
            large_firms14, information_intensity14, manuf_intensity14, TurnOvrS14),
  weights = ~lads14
)

est_all_large <- fixest::feols(
  "dai_intensity9 ~ share_bac14 + share_black14 + share_poverty14 + gpop + hpi_ch14 + logincome14 + tightness14 + pat_intensity14 + patai_intensity14 + degshare14 + stemshare14 + large_firms14 + information_intensity14 + manuf_intensity14 + TurnOvrS14 | state" %>% as.formula(),
  data_ai_l_z %>% 
    filter(dai_intensity9 > -5, dai_intensity9 < 10) %>% 
    drop_na(share_bac14, share_black14, share_poverty14, logpop14, hpi_ch14, logincome14,
            tightness14, pat_intensity14, patai_intensity14, degshare14, stemshare14,
            large_firms14, information_intensity14, manuf_intensity14, TurnOvrS14),
  weights = ~lads14
)

# Extract model statistics
models <- list(est_demog_no, est_innovation_no, est_industry_no, est_all, est_all_large)
obs <- map_chr(models, ~scales::comma(nobs(.x)))
r2_vals <- map_chr(models, ~sprintf("%.4f", r2(.x, "r2")))

# Create extra rows for table footer - add spacing before values to shift right
extra_rows <- tribble(
  ~term, ~`(1)`, ~`(2)`, ~`(3)`, ~`(4)`, ~`(5)`,
  "Fixed effects", "{\\hspace{0.5em}None}", "{\\hspace{0.5em}None}", "{\\hspace{0.5em}None}", "{\\hspace{0.5em}None}", "{\\hspace{0.5em}State}",
  "Observations", paste0("{\\hspace{0.5em}", obs[1], "}"), paste0("{\\hspace{0.5em}", obs[2], "}"), paste0("{\\hspace{0.5em}", obs[3], "}"), paste0("{\\hspace{0.5em}", obs[4], "}"), paste0("{\\hspace{0.5em}", obs[5], "}"),
  "R²", paste0("{\\hspace{0.5em}", r2_vals[1], "}"), paste0("{\\hspace{0.5em}", r2_vals[2], "}"), paste0("{\\hspace{0.5em}", r2_vals[3], "}"), paste0("{\\hspace{0.5em}", r2_vals[4], "}"), paste0("{\\hspace{0.5em}", r2_vals[5], "}")
)

# Variable labels - condensed to only the 8 key variables
labels <- c(
  "share_bac14" = "Bachelors Share",
  "logincome14" = "Income, Log", 
  "tightness14" = "Tightness",
  "stemshare14" = "Stem Degrees' share",
  "large_firms14" = "Large Firms",
  "information_intensity14" = "ICT sector Intensity",
  "manuf_intensity14" = "Manufacturing",
  "TurnOvrS14" = "Turnover Rate"
)

# Create the table with modelsummary - use d for decimal alignment
modelsummary(
  list(
    "(1)" = est_demog_no,
    "(2)" = est_innovation_no,
    "(3)" = est_industry_no,
    "(4)" = est_all,
    "(5)" = est_all_large
  ),
  coef_map = labels,
  stars = FALSE,
  gof_omit = ".*",
  add_rows = extra_rows,
  escape = FALSE,
  statistic = "({std.error})",
  fmt = 4,
  align = "lddddd",
  notes = "This table examines the demographic, innovation, and industry determinants of the percentage point change in AI job share between two pooled periods: 2017-2018 and 2022-2023. The dependent variable measures the change in AI intensity (multiplied by 100 to express in percentage points), capturing dynamic shifts rather than static levels. All explanatory variables are measured in 2017 (before the growth period) and standardized as z-scores. The sample includes 2,473 counties with sufficient data across both periods. All coefficients represent percentage point changes in the growth of AI job share for a one standard deviation increase in the 2017 explanatory variable. Models are estimated using ordinary least squares with no fixed effects (columns 1-4) or state fixed effects (column 5). Standard errors are clustered at the county level. We were unable to fully replicate the Bachelor's share coefficients across all five model specifications. The Bachelor's share discrepancy appears systematically across models, with our replicated coefficients differing from the published values. All other coefficients successfully replicate the original results. The authors were contacted regarding this discrepancy but did not respond."
)
```



### Table 2

Table 2 analyzes predictors of AI job growth between 2017-2018 and 2022-2023. All coefficients represent standardized effects, with explanatory variables measured in 2017. Column 4 includes all controls and serves as the baseline model for our comparisons.

Comparing STEM degrees share (0.054, p < 0.001) with labor tightness (0.073, p < 0.01), counties with one standard deviation higher 2017 tightness experienced 0.019 percentage points more growth than counties with equivalent STEM credentials. The effect size difference is modest: tightness is only 36% larger than STEM. Both achieve high statistical significance with p-values below 0.01. Relative to median growth of 0.088 percentage points, tightness accounts for 83% while STEM accounts for 61%. Unlike Table 1 where tightness showed 52% larger effects, the growth analysis reveals more balanced effect sizes despite maintained statistical precision.

Within Table 2 labor market variables, comparing turnover rate (-0.042, p = 0.072) with large establishments share (-0.016, p = 0.47), counties with one standard deviation higher labor turnover grew 0.026 percentage points slower than counties with more large establishments. Turnover shows 2.7 times the effect size of large establishments. Statistical significance also differs: turnover approaches conventional significance (p = 0.072), while large establishments shows no statistical evidence (p = 0.47). The effect sizes are 48% and 18% of median growth respectively. Here both statistical precision and effect size favor turnover, though neither relationship is measured with high confidence.

Comparing industry composition within Table 2, manufacturing intensity (-0.013, p = 0.48) and ICT sector intensity (0.014, p = 0.42) show nearly identical effect sizes but opposite signs. Counties with one standard deviation higher ICT concentration grew 0.027 percentage points more than manufacturing-oriented counties. Both show small effect sizes at 15-16% of median growth. Neither achieves statistical significance, with p-values well above conventional thresholds. This contrasts sharply with Table 1 where manufacturing showed strong statistical significance (p < 0.01) and meaningful effect size (0.033, or 7% of mean). The growth analysis reveals these structural factors predict levels but not changes, as both effect sizes and statistical precision collapse in dynamic specifications.



```{r}
#| label: fig-map
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "This figure shows the spatial heterogeneity in AI job share across US counties. Panel A shows the average share of AI-related job postings from 2014 to 2023, revealing concentration in technology hubs like Santa Clara County, California (8.19 percent), Fairfax County, Virginia (6.97 percent), and San Francisco County, California (6.34 percent). Panel B shows percentage-point changes between 2018 and 2023, with fastest growth in unexpected locations including Maries, Missouri (12.35 percentage points), Hughes, South Dakota (10.43 percentage points), and Osage, Michigan (9.82 percentage points). Connecticut appears with N/A values because the authors' data file did not contain data for conneticut. Additional visual discrepancies exist between our replicated maps and the published maps, which could reflect a copy-paste error when the authors merged data files. The authors were contacted regarding this discrepancy but did not respond."
#| fig-cap-location: bottom
#| fig-pos: "H"
#| fig-width: 18
#| fig-height: 9

library(tidyverse)
library(sf)
library(tigris)
library(patchwork)

# Read the updated Lightcast data
data <- read_csv("updated_lightcast_county.csv") %>%
  rename(COUNTY_FIPS = COUNTY_FIPS, 
         Year = YEAR_POSTED, 
         ai = `AI Postings`, 
         nads = `Total Postings`)

# Remove Connecticut state-level data (FIPS 9999) and Planning Region entries
data_complete <- data %>%
  filter(COUNTY_FIPS != 9999) %>%
  filter(!(COUNTY_FIPS >= 9110 & COUNTY_FIPS <= 9190))

# ---- PANEL A ----
data_levels <- data_complete %>%
  mutate(aiInt = if_else(nads > 0, ai/nads, NA_real_)) %>%
  group_by(COUNTY_FIPS) %>%
  summarise(aiInt = mean(aiInt, na.rm = TRUE), .groups = "drop")

# ---- PANEL B ----
data_for_change <- data_complete %>%
  mutate(
    year_group = case_when(
      Year %in% c(2017, 2018) ~ "2017-2018",
      Year %in% c(2022, 2023) ~ "2022-2023",
      TRUE ~ NA_character_
    )
  ) %>%
  filter(!is.na(year_group))

data_pooled <- data_for_change %>%
  group_by(COUNTY_FIPS, year_group) %>%
  summarise(
    ai_sum = sum(ai, na.rm = TRUE),
    nads_sum = sum(nads, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(aiInt = if_else(nads_sum > 0, ai_sum / nads_sum, NA_real_))

data_ch <- data_pooled %>%
  pivot_wider(
    id_cols = COUNTY_FIPS,
    names_from = year_group,
    values_from = aiInt
  ) %>%
  mutate(aiIntch = `2022-2023` - `2017-2018`) %>%
  select(COUNTY_FIPS, aiIntch)

# ---- FIPS FORMATTING ----
merged_data_lev <- data_levels %>%
  mutate(fips = COUNTY_FIPS %>% as.character() %>% str_pad(5, "0", side = "left"))

merged_data_ch <- data_ch %>%
  mutate(fips = COUNTY_FIPS %>% as.character() %>% str_pad(5, "0", side = "left"))

# ---- PANEL A BINNING ----
merged_data_lev <- merged_data_lev %>%
  mutate(ai_share = aiInt * 100) %>%
  mutate(caseai = case_when(
    ai_share >= 0 & ai_share < 0.06 ~ 1,
    ai_share > 0.06 & ai_share < 0.14 ~ 2,
    ai_share > 0.14 & ai_share < 0.23 ~ 3,
    ai_share > 0.23 & ai_share < 0.37 ~ 4,
    ai_share > 0.37 & ai_share < 0.7 ~ 5,
    ai_share > 0.7 ~ 6,
    TRUE ~ NA_real_
  ))

# ---- PANEL B BINNING ----
merged_data_ch <- merged_data_ch %>%
  mutate(change_in_share = aiIntch * 100) %>%
  mutate(caseaich = case_when(
    change_in_share < -0.12 ~ 1,
    change_in_share > -0.12 & change_in_share <= 0 ~ 2,
    change_in_share > 0 & change_in_share < 0.09 ~ 3,
    change_in_share > 0.09 & change_in_share < 0.24 ~ 4,
    change_in_share > 0.24 & change_in_share < 0.57 ~ 5,
    change_in_share > 0.57 ~ 6,
    TRUE ~ NA_real_
  ))

# ---- COUNTY SHAPEFILES ----
options(tigris_use_cache = TRUE)
counties <- suppressMessages(
  tigris::counties(cb = TRUE, year = 2020, class = "sf")
) %>%
  mutate(COUNTY_FIPS = GEOID) %>%
  filter(!str_starts(COUNTY_FIPS, "72")) %>%
  st_transform(crs = 4326)

# ---- PANEL A MAP JOIN ----
map_data_a <- counties %>%
  left_join(merged_data_lev, by = c("COUNTY_FIPS" = "fips")) %>%
  tigris::shift_geometry()

# ---- PANEL B MAP JOIN (with Alaska color fix) ----
# Fix Alaska merge issue before shifting
ak_data <- merged_data_ch %>%
  filter(str_starts(fips, "02")) %>%
  distinct(fips, .keep_all = TRUE)

map_data_b <- counties %>%
  left_join(merged_data_ch, by = c("COUNTY_FIPS" = "fips"))

map_data_b[map_data_b$STATEFP == "02", names(ak_data)[-1]] <- 
  ak_data[match(map_data_b$COUNTY_FIPS[map_data_b$STATEFP == "02"], ak_data$fips),
          names(ak_data)[-1]]

# Shift after joining (fixes Alaska colors)
map_data_b <- tigris::shift_geometry(map_data_b)

# ---- LABELS AND COLORS ----
map_data_a <- map_data_a %>%
  mutate(
    avg_bin = case_when(
      caseai == 1 ~ "0 – 0.06",
      caseai == 2 ~ "0.06 – 0.14",
      caseai == 3 ~ "0.14 – 0.23",
      caseai == 4 ~ "0.23 – 0.37",
      caseai == 5 ~ "0.37 – 0.71",
      caseai == 6 ~ "0.71 – 10",
      TRUE ~ "No data"
    )
  )

map_data_b <- map_data_b %>%
  mutate(
    chg_bin = case_when(
      caseaich == 1 ~ "−5.56 – −0.12",
      caseaich == 2 ~ "−0.12 – 0",
      caseaich == 3 ~ "0 – 0.09",
      caseaich == 4 ~ "0.09 – 0.24",
      caseaich == 5 ~ "0.24 – 0.57",
      caseaich == 6 ~ "0.57 – 12.35",
      TRUE ~ "No data"
    )
  )

levels_avg_display <- c("0.71 – 10", "0.37 – 0.71", "0.23 – 0.37",
                        "0.14 – 0.23", "0.06 – 0.14", "0 – 0.06", "No data")
levels_chg_display <- c("0.57 – 12.35", "0.24 – 0.57", "0.09 – 0.24",
                        "0 – 0.09", "−0.12 – 0", "−5.56 – −0.12", "No data")

colors_avg_display <- c("#800026", "#e31a1c", "#fd8d3c", "#fed976", "#ffeda0", "#ffffcc", "gray90")
colors_chg_display <- c("#800026", "#fc4e2a", "#fd8d3c", "#fed976", "#ffeda0", "#ffffcc", "gray90")

map_data_a$avg_bin <- factor(map_data_a$avg_bin, levels = levels_avg_display)
map_data_b$chg_bin <- factor(map_data_b$chg_bin, levels = levels_chg_display)

# ---- THEME ----
map_theme <- theme_void() +
  theme(
    legend.position = "bottom",
    legend.direction = "horizontal",
    legend.text = element_text(size = 20, margin = margin(t = 3, b = 3, l = 1, r = 1)),
    legend.title = element_blank(),
    legend.key.width = unit(1.8, "cm"),
    legend.key.height = unit(0.6, "cm"),
    legend.spacing.x = unit(0.2, "cm"),
    legend.spacing.y = unit(0.1, "cm"),
    legend.margin = margin(t = 10, b = 5),
    legend.key = element_rect(color = "black", size = 0.5),
    plot.margin = margin(10, 10, 25, 10, "pt"),
    panel.background = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = NA),
    panel.border = element_rect(color = "black", fill = NA, size = 0.5)
  )

# ---- PLOTS ----
p1 <- ggplot(map_data_a) +
  geom_sf(aes(fill = avg_bin), color = "white", size = 0.1) +
  scale_fill_manual(values = setNames(colors_avg_display, levels_avg_display),
                    breaks = levels_avg_display, drop = FALSE,
                    guide = guide_legend(title = NULL, nrow = 2, byrow = TRUE,
                                         label.position = "bottom",
                                         keywidth = unit(1.8, "cm"), keyheight = unit(0.6, "cm"),
                                         label.hjust = 0.5,
                                         override.aes = list(color = "black", size = 0.5))) +
  coord_sf(crs = st_crs(map_data_a), expand = FALSE, clip = "off",
           xlim = c(-2800000, 2500000), ylim = c(-2300000, 1600000)) +
  labs(title = "Panel A. Percent share of AI jobs (2014–2023 average)") +
  map_theme +
  theme(plot.title = element_text(size = 24, hjust = 0, margin = margin(b = 12)))

p2 <- ggplot(map_data_b) +
  geom_sf(aes(fill = chg_bin), color = "white", size = 0.1) +
  scale_fill_manual(values = setNames(colors_chg_display, levels_chg_display),
                    breaks = levels_chg_display, drop = FALSE,
                    guide = guide_legend(title = NULL, nrow = 2, byrow = TRUE,
                                         label.position = "bottom",
                                         keywidth = unit(1.8, "cm"), keyheight = unit(0.6, "cm"),
                                         label.hjust = 0.5,
                                         override.aes = list(color = "black", size = 0.5))) +
  coord_sf(crs = st_crs(map_data_b), expand = FALSE, clip = "off",
           xlim = c(-2800000, 2500000), ylim = c(-2300000, 1600000)) +
  labs(title = "Panel B. Percentage-point change in AI share (2018–2023)") +
  map_theme +
  theme(plot.title = element_text(size = 24, hjust = 0, margin = margin(b = 12)))

# ---- FINAL COMBINED PLOT ----
final_plot <- (p1 | p2) +
  plot_layout(widths = c(1, 1)) &
  theme(plot.background = element_rect(fill = "white", color = NA))

# Add overall title
final_plot <- final_plot +
  plot_annotation(
    title = "Figure 1: Replication of Andreadis et al. (2025) - Spatial Heterogeneity in the Share of and Change of AI Jobs",
    theme = theme(plot.title = element_text(size = 26, hjust = 0.5, margin = margin(b = 15)))
  )

final_plot
```



### Figure 1

Figure 1 displays the spatial distribution and evolution of AI job adoption across US counties from 2014 to 2023. Panel A shows average AI job shares, while Panel B captures growth dynamics between 2018 and 2023.

Comparing the geographic concentration patterns between coastal tech hubs and inland regions in Panel A, Santa Clara County, California (8.19 percent) shows 35.6 times higher AI job share than the national median (0.23 percent). Similarly, Fairfax County, Virginia (6.97 percent) and San Francisco County, California (6.34 percent) exhibit AI shares 30.3 and 27.6 times the median respectively. These coastal metropolitan areas demonstrate extreme concentration, while most rural and inland counties show minimal AI activity (0–0.14 percent range). The geographic disparity reveals that AI job markets cluster in established technology ecosystems, with the top counties accounting for disproportionate shares of national AI employment demand. This concentration pattern suggests that human capital, innovation infrastructure, and industry composition create self-reinforcing advantages for tech hubs.

Within Panel B growth patterns, comparing unexpected high-growth counties with traditional tech centers reveals surprising shifts. Maries, Missouri experienced 12.35 percentage points growth, Hughes, South Dakota grew 10.43 percentage points, and Osage, Michigan increased 9.82 percentage points between 2018 and 2023. These growth rates far exceed those in established tech hubs, suggesting geographic diffusion driven by remote work adoption following pandemic lockdowns. Counties with initially high AI shares continued strengthening their absolute positions, but the highest growth rates appeared in suburban and lower-cost regions. This divergence between levels (Panel A) and changes (Panel B) indicates that while concentration persists, the marginal growth shifted toward areas with lower housing costs and remote-friendly environments. The median growth of 0.088 percentage points masks substantial heterogeneity, with the ninetieth percentile showing 0.93 percentage points growth compared to tenth percentile decline of 0.29 percentage points.

Comparing Panel A concentration patterns with Panel B growth dynamics reveals that initial advantages compound over time. Counties in the highest AI share category (0.71–10 percent in Panel A) show moderate absolute growth in Panel B, while their percentage increases remain substantial. Conversely, counties with zero AI activity in Panel A largely remain unchanged in Panel B, showing minimal growth (0–0.09 percentage points range). This persistence suggests path dependency in AI adoption, where counties lacking initial conditions for AI jobs struggle to enter the market even during rapid national expansion. The contrast highlights regional disparities that Tables 1 and 2 explain through education, innovation, and labor market tightness differentials.


## Extension: Log-Population Weighting Analysis

To assess the robustness of the original findings, we re-estimated all models using log-population weights. This weighting scheme gives more weight to populous counties compared to equal weighting (where each county receives the same weight regardless of population size). 

```{r}
#| label: fig-table1-coefs-models1-3
#| echo: false
#| warning: false
#| message: false
#| fig-cap: "Extension of Table 1 from Andreadis et al. (2025) — Coefficients under Equal weights vs Log-Population weights (Models 1-3)"
#| fig-cap-location: top
#| fig-pos: "H"
#| fig-width: 14
#| fig-height: 10

library(ggplot2)
library(dplyr)
library(forcats)
library(patchwork)

# Run equal-weighted regressions (no weights) for comparison (Models 1-3)
est_demog_no <- fixest::feols(
  "ai_intensity ~ share_bac + share_black + share_poverty + logpop + hpi_ch + logincome + tightness | Year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z %>% drop_na(share_bac, share_black, share_poverty, logpop, hpi_ch, logincome,
                        tightness, pat_intensity, patai_intensity, degshare, stemshare,
                        large_firms, information_intensity, manuf_intensity, TurnOvrS),
  cluster = "COUNTY_FIPS"
)

est_innovation_no <- fixest::feols(
  "ai_intensity ~ pat_intensity + patai_intensity + degshare + stemshare | Year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z %>% drop_na(share_bac, share_black, share_poverty, logpop, hpi_ch, logincome,
                        tightness, pat_intensity, patai_intensity, degshare, stemshare,
                        large_firms, information_intensity, manuf_intensity, TurnOvrS),
  cluster = "COUNTY_FIPS"
)

est_industry_no <- fixest::feols(
  "ai_intensity ~ large_firms + information_intensity + manuf_intensity + TurnOvrS | Year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z %>% drop_na(share_bac, share_black, share_poverty, logpop, hpi_ch, logincome,
                        tightness, pat_intensity, patai_intensity, degshare, stemshare,
                        large_firms, information_intensity, manuf_intensity, TurnOvrS),
  cluster = "COUNTY_FIPS"
)

# Run log-population weighted regressions (Models 1-3)
est_demog_logpop <- fixest::feols(
  "ai_intensity ~ share_bac + share_black + share_poverty + logpop + hpi_ch + logincome + tightness | Year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z %>% drop_na(share_bac, share_black, share_poverty, logpop, hpi_ch, logincome,
                        tightness, pat_intensity, patai_intensity, degshare, stemshare,
                        large_firms, information_intensity, manuf_intensity, TurnOvrS),
  cluster = "COUNTY_FIPS",
  weights = ~logpop_raw
)

est_innovation_logpop <- fixest::feols(
  "ai_intensity ~ pat_intensity + patai_intensity + degshare + stemshare | Year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z %>% drop_na(share_bac, share_black, share_poverty, logpop, hpi_ch, logincome,
                        tightness, pat_intensity, patai_intensity, degshare, stemshare,
                        large_firms, information_intensity, manuf_intensity, TurnOvrS),
  cluster = "COUNTY_FIPS",
  weights = ~logpop_raw
)

est_industry_logpop <- fixest::feols(
  "ai_intensity ~ large_firms + information_intensity + manuf_intensity + TurnOvrS | Year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z %>% drop_na(share_bac, share_black, share_poverty, logpop, hpi_ch, logincome,
                        tightness, pat_intensity, patai_intensity, degshare, stemshare,
                        large_firms, information_intensity, manuf_intensity, TurnOvrS),
  cluster = "COUNTY_FIPS",
  weights = ~logpop_raw
)

models <- c("Demographics", "Innovation", "Industry")
variables <- c(
  "Bachelor's share", "Labor Tightness", "Patents per emp.",
  "STEM share", "Manufacturing intensity", "ICT intensity"
)

variables_list <- list(
  c("Bachelor's share", "Labor Tightness"),
  c("Patents per emp.", "STEM share"),
  c("Manufacturing intensity", "ICT intensity")
)

# Extract coefficients and SEs from original (lads-weighted) regressions (Models 1-3)
coefs_orig_list <- list(
  c(coef(est_demog_no)["share_bac"], coef(est_demog_no)["tightness"]),
  c(coef(est_innovation_no)["pat_intensity"], coef(est_innovation_no)["stemshare"]),
  c(coef(est_industry_no)["manuf_intensity"], coef(est_industry_no)["information_intensity"])
)

ses_orig_list <- list(
  c(se(est_demog_no)["share_bac"], se(est_demog_no)["tightness"]),
  c(se(est_innovation_no)["pat_intensity"], se(est_innovation_no)["stemshare"]),
  c(se(est_industry_no)["manuf_intensity"], se(est_industry_no)["information_intensity"])
)

# Extract coefficients and SEs from log-pop weighted regressions (Models 1-3)
coefs_log_list <- list(
  c(coef(est_demog_logpop)["share_bac"], coef(est_demog_logpop)["tightness"]),
  c(coef(est_innovation_logpop)["pat_intensity"], coef(est_innovation_logpop)["stemshare"]),
  c(coef(est_industry_logpop)["manuf_intensity"], coef(est_industry_logpop)["information_intensity"])
)

ses_log_list <- list(
  c(se(est_demog_logpop)["share_bac"], se(est_demog_logpop)["tightness"]),
  c(se(est_innovation_logpop)["pat_intensity"], se(est_innovation_logpop)["stemshare"]),
  c(se(est_industry_logpop)["manuf_intensity"], se(est_industry_logpop)["information_intensity"])
)

make_df <- function(model_index) {
  vars <- variables_list[[model_index]]
  bind_rows(
    data.frame(
      Variable = vars,
      Coef = coefs_orig_list[[model_index]],
      SE = ses_orig_list[[model_index]],
      Scheme = "Equal Weights"
    ),
    data.frame(
      Variable = vars,
      Coef = coefs_log_list[[model_index]],
      SE = ses_log_list[[model_index]],
      Scheme = "Log-Population Weights"
    )
  ) %>%
    mutate(
      lower = Coef - 1.96 * SE,
      upper = Coef + 1.96 * SE,
      Variable = fct_rev(factor(Variable, levels = vars))
    )
}

# Enhanced plot function with better styling
plots <- lapply(1:3, function(i) {
  df <- make_df(i)
  
  ggplot(df, aes(x = Coef, y = Variable, color = Scheme)) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.6, color = "gray50", size = 0.7) +
    geom_point(position = position_dodge(width = 0.5), size = 3.5, alpha = 0.9) +
    geom_errorbarh(aes(xmin = lower, xmax = upper),
                   position = position_dodge(width = 0.5),
                   height = 0.2, linewidth = 1.2, alpha = 0.8) +
    labs(x = "Coefficient", y = NULL) +
    theme_minimal(base_size = 18) +
    theme(
      # Y-axis styling
      axis.text.y = element_text(size = 13, color = "gray20", margin = margin(r = 12)),
      axis.text.x = element_text(size = 11, color = "gray30"),
      axis.title.x = element_text(size = 13, color = "gray20", margin = margin(t = 15)),
      
      # Panel styling
      panel.grid.major.y = element_line(color = "gray90", size = 0.3),
      panel.grid.major.x = element_line(color = "gray95", size = 0.3),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = "white", color = NA),
      
      # Plot margins and spacing
      plot.margin = margin(t = 20, r = 15, b = 15, l = 20),
      
      # Legend styling
      legend.text = element_text(size = 12, color = "gray20"),
      legend.margin = margin(t = 10),
      legend.box.spacing = unit(0.5, "cm")
    ) +
    scale_color_manual(
      values = c("Equal Weights" = "#28A745", "Log-Population Weights" = "#F24236"),
      breaks = c("Equal Weights", "Log-Population Weights")
    ) +
    scale_x_continuous(
      breaks = seq(-0.2, 0.4, by = 0.1),
      limits = c(-0.25, 0.45),
      expand = expansion(mult = c(0.02, 0.02))
    ) +
    scale_y_discrete(expand = expansion(add = c(0.6, 0.6)))
})

# Add titles directly to the plots instead of separate label objects
plots_with_tags <- lapply(1:3, function(i) {
  df <- make_df(i)

  ggplot(df, aes(x = Coef, y = Variable, color = Scheme)) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.6, color = "gray50", size = 0.7) +
    geom_point(position = position_dodge(width = 0.5), size = 3.5, alpha = 0.9) +
    geom_errorbarh(aes(xmin = lower, xmax = upper),
                   position = position_dodge(width = 0.5),
                   height = 0.2, linewidth = 1.2, alpha = 0.8) +
    labs(x = "Coefficient", y = NULL, tag = models[i]) +
    theme_minimal(base_size = 18) +
    theme(
      # Tag styling (panel labels)
      plot.tag = element_text(size = 14, face = "bold", color = "gray15", hjust = 0),
      plot.tag.position = c(0.01, 0.98),

      # Y-axis styling
      axis.text.y = element_text(size = 13, color = "gray20", margin = margin(r = 12)),
      axis.text.x = element_text(size = 11, color = "gray30"),
      axis.title.x = element_text(size = 13, color = "gray20", margin = margin(t = 15)),

      # Panel styling
      panel.grid.major.y = element_line(color = "gray90", size = 0.3),
      panel.grid.major.x = element_line(color = "gray95", size = 0.3),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = "white", color = NA),

      # Plot margins and spacing
      plot.margin = margin(t = 10, r = 15, b = 15, l = 20),

      # Legend styling
      legend.text = element_text(size = 12, color = "gray20"),
      legend.margin = margin(t = 10),
      legend.box.spacing = unit(0.5, "cm")
    ) +
    scale_color_manual(
      values = c("Equal Weights" = "#28A745", "Log-Population Weights" = "#F24236"),
      breaks = c("Equal Weights", "Log-Population Weights")
    ) +
    scale_x_continuous(
      breaks = seq(-0.2, 0.4, by = 0.1),
      limits = c(-0.25, 0.45),
      expand = expansion(mult = c(0.02, 0.02))
    ) +
    scale_y_discrete(expand = expansion(add = c(0.6, 0.6)))
})

# Simple vertical arrangement
final <- plots_with_tags[[1]] / plots_with_tags[[2]] / plots_with_tags[[3]] +
  plot_layout(guides = "collect") &
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.text = element_text(size = 13, color = "gray20"),
    legend.margin = margin(t = 20)
  )

final
```

*Notes:* This figure compares coefficient estimates from equal-weighted regressions (where each county receives the same weight) versus log-population-weighted regressions (where counties are weighted by log(population)) for Models 1-3 of Table 1 from Andreadis et al. (2025). These models examine AI job share (the percentage of job postings that are AI-related) as the outcome variable. Model 1 (Demographics) includes bachelor's share and labor market tightness. Model 2 (Innovation) includes patents per employee and STEM degree share. Model 3 (Industry) includes manufacturing intensity and ICT sector intensity. All models include county and year fixed effects. The confidence intervals shown represent 95% confidence levels (±1.96 standard errors). Blue points indicate equal-weighted estimates, while red points indicate log-population-weighted estimates.

The comparison between equal weights and log-population weights across Models 1-3 reveals modest differences in coefficient magnitudes, suggesting that the correlations documented in Table 1 are relatively robust to weighting schemes. In the Demographics panel, bachelor's share and labor market tightness show nearly identical coefficients under both weighting approaches, with overlapping confidence intervals indicating that educational attainment and labor market conditions correlate with AI intensity similarly across counties regardless of population size.

The Innovation panel demonstrates the same pattern: patents per employee and STEM degree share maintain consistent relationships with AI intensity under both weighting schemes. The narrow gap between blue (equal weights) and red (log-population weights) point estimates suggests that innovation capacity correlates with AI adoption in both small and large counties, rather than being driven predominantly by metropolitan areas.

Similarly, the Industry panel shows manufacturing intensity and ICT sector intensity with nearly overlapping coefficients across weighting schemes. Manufacturing's negative association with AI intensity and ICT's positive association persist whether we weight counties equally or by population, indicating these industry composition effects are not artifacts of giving disproportionate influence to large urban counties. The stability of these relationships across weighting approaches strengthens confidence that the Table 1 findings reflect genuine associations rather than statistical artifacts of how observations are weighted.

```{r}
#| label: fig-table1-coefs-models4-5
#| echo: false
#| warning: false
#| message: false
#| fig-cap: "Extension of Table 1 from Andreadis et al. (2025) — Coefficients under Equal weights vs Log-Population weights (Models 4-5)"
#| fig-cap-location: top
#| fig-pos: "H"
#| fig-width: 14
#| fig-height: 12

# Run equal-weighted regressions (no weights) for comparison (Models 4-5)
est_all <- fixest::feols(
  "ai_intensity ~ share_bac + share_black + share_poverty + logpop + hpi_ch + logincome + tightness + pat_intensity + patai_intensity + degshare + stemshare + large_firms + information_intensity + manuf_intensity + TurnOvrS | Year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z,
  cluster = "COUNTY_FIPS"
)

est_all_state_year <- fixest::feols(
  "ai_intensity ~ share_bac + share_black + share_poverty + logpop + hpi_ch + logincome + tightness + pat_intensity + patai_intensity + degshare + stemshare + large_firms + information_intensity + manuf_intensity + TurnOvrS | state_year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z,
  vcov = "twoway"
)

# Run log-population weighted regressions (Models 4-5)
est_all_logpop <- fixest::feols(
  "ai_intensity ~ share_bac + share_black + share_poverty + logpop + hpi_ch + logincome + tightness + pat_intensity + patai_intensity + degshare + stemshare + large_firms + information_intensity + manuf_intensity + TurnOvrS | Year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z,
  cluster = "COUNTY_FIPS",
  weights = ~logpop_raw
)

est_all_state_year_logpop <- fixest::feols(
  "ai_intensity ~ share_bac + share_black + share_poverty + logpop + hpi_ch + logincome + tightness + pat_intensity + patai_intensity + degshare + stemshare + large_firms + information_intensity + manuf_intensity + TurnOvrS | state_year + COUNTY_FIPS" %>% as.formula(),
  data_ai_z,
  vcov = "twoway",
  weights = ~logpop_raw
)

models_45 <- c("All Controls", "All + State FE")
variables_45 <- c(
  "Bachelor's share", "Black share", "Poverty share", "Labor Tightness",
  "Patents per emp.", "STEM share", "Manufacturing intensity", "ICT intensity"
)

variables_list_45 <- list(
  variables_45,
  variables_45
)

# Extract coefficients and SEs from original (lads-weighted) regressions (Models 4-5)
coefs_orig_list_45 <- list(
  c(coef(est_all)["share_bac"], coef(est_all)["share_black"], coef(est_all)["share_poverty"],
    coef(est_all)["tightness"], coef(est_all)["pat_intensity"], coef(est_all)["stemshare"],
    coef(est_all)["manuf_intensity"], coef(est_all)["information_intensity"]),
  c(coef(est_all_state_year)["share_bac"], coef(est_all_state_year)["share_black"],
    coef(est_all_state_year)["share_poverty"], coef(est_all_state_year)["tightness"],
    coef(est_all_state_year)["pat_intensity"], coef(est_all_state_year)["stemshare"],
    coef(est_all_state_year)["manuf_intensity"], coef(est_all_state_year)["information_intensity"])
)

ses_orig_list_45 <- list(
  c(se(est_all)["share_bac"], se(est_all)["share_black"], se(est_all)["share_poverty"],
    se(est_all)["tightness"], se(est_all)["pat_intensity"], se(est_all)["stemshare"],
    se(est_all)["manuf_intensity"], se(est_all)["information_intensity"]),
  c(se(est_all_state_year)["share_bac"], se(est_all_state_year)["share_black"],
    se(est_all_state_year)["share_poverty"], se(est_all_state_year)["tightness"],
    se(est_all_state_year)["pat_intensity"], se(est_all_state_year)["stemshare"],
    se(est_all_state_year)["manuf_intensity"], se(est_all_state_year)["information_intensity"])
)

# Extract coefficients and SEs from log-pop weighted regressions (Models 4-5)
coefs_log_list_45 <- list(
  c(coef(est_all_logpop)["share_bac"], coef(est_all_logpop)["share_black"],
    coef(est_all_logpop)["share_poverty"], coef(est_all_logpop)["tightness"],
    coef(est_all_logpop)["pat_intensity"], coef(est_all_logpop)["stemshare"],
    coef(est_all_logpop)["manuf_intensity"], coef(est_all_logpop)["information_intensity"]),
  c(coef(est_all_state_year_logpop)["share_bac"], coef(est_all_state_year_logpop)["share_black"],
    coef(est_all_state_year_logpop)["share_poverty"], coef(est_all_state_year_logpop)["tightness"],
    coef(est_all_state_year_logpop)["pat_intensity"], coef(est_all_state_year_logpop)["stemshare"],
    coef(est_all_state_year_logpop)["manuf_intensity"], coef(est_all_state_year_logpop)["information_intensity"])
)

ses_log_list_45 <- list(
  c(se(est_all_logpop)["share_bac"], se(est_all_logpop)["share_black"],
    se(est_all_logpop)["share_poverty"], se(est_all_logpop)["tightness"],
    se(est_all_logpop)["pat_intensity"], se(est_all_logpop)["stemshare"],
    se(est_all_logpop)["manuf_intensity"], se(est_all_logpop)["information_intensity"]),
  c(se(est_all_state_year_logpop)["share_bac"], se(est_all_state_year_logpop)["share_black"],
    se(est_all_state_year_logpop)["share_poverty"], se(est_all_state_year_logpop)["tightness"],
    se(est_all_state_year_logpop)["pat_intensity"], se(est_all_state_year_logpop)["stemshare"],
    se(est_all_state_year_logpop)["manuf_intensity"], se(est_all_state_year_logpop)["information_intensity"])
)

make_df_45 <- function(model_index) {
  vars <- variables_list_45[[model_index]]
  bind_rows(
    data.frame(
      Variable = vars,
      Coef = coefs_orig_list_45[[model_index]],
      SE = ses_orig_list_45[[model_index]],
      Scheme = "Equal Weights"
    ),
    data.frame(
      Variable = vars,
      Coef = coefs_log_list_45[[model_index]],
      SE = ses_log_list_45[[model_index]],
      Scheme = "Log-Population Weights"
    )
  ) %>%
    mutate(
      lower = Coef - 1.96 * SE,
      upper = Coef + 1.96 * SE,
      Variable = fct_rev(factor(Variable, levels = vars))
    )
}

# Enhanced plots for models 4-5
plots_45 <- lapply(1:2, function(i) {
  df <- make_df_45(i)
  ggplot(df, aes(x = Coef, y = Variable, color = Scheme)) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.6, color = "gray50", size = 0.7) +
    geom_point(position = position_dodge(width = 0.5), size = 3.5, alpha = 0.9) +
    geom_errorbarh(aes(xmin = lower, xmax = upper),
                   position = position_dodge(width = 0.5),
                   height = 0.15, linewidth = 1.2, alpha = 0.8) +
    labs(x = "Coefficient", y = NULL) +
    theme_minimal(base_size = 18) +
    theme(
      # Y-axis styling
      axis.text.y = element_text(size = 13, color = "gray20", margin = margin(r = 12)),
      axis.text.x = element_text(size = 11, color = "gray30"),
      axis.title.x = element_text(size = 13, color = "gray20", margin = margin(t = 15)),
      
      # Panel styling
      panel.grid.major.y = element_line(color = "gray90", size = 0.3),
      panel.grid.major.x = element_line(color = "gray95", size = 0.3),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = "white", color = NA),
      
      # Plot margins and spacing
      plot.margin = margin(t = 20, r = 15, b = 15, l = 20),
      
      # Legend styling
      legend.text = element_text(size = 12, color = "gray20")
    ) +
    scale_color_manual(
      values = c("Equal Weights" = "#28A745", "Log-Population Weights" = "#F24236"),
      breaks = c("Equal Weights", "Log-Population Weights")
    ) +
    scale_x_continuous(
      breaks = seq(-0.2, 0.4, by = 0.1), 
      limits = c(-0.25, 0.45),
      expand = expansion(mult = c(0.02, 0.02))
    ) +
    scale_y_discrete(expand = expansion(add = c(0.4, 0.4)))
})

# Enhanced plots for models 4-5 with panel tags
plots_45_with_tags <- lapply(1:2, function(i) {
  df <- make_df_45(i)
  ggplot(df, aes(x = Coef, y = Variable, color = Scheme)) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.6, color = "gray50", size = 0.7) +
    geom_point(position = position_dodge(width = 0.5), size = 3.5, alpha = 0.9) +
    geom_errorbarh(aes(xmin = lower, xmax = upper),
                   position = position_dodge(width = 0.5),
                   height = 0.15, linewidth = 1.2, alpha = 0.8) +
    labs(x = "Coefficient", y = NULL, tag = models_45[i]) +
    theme_minimal(base_size = 18) +
    theme(
      # Tag styling (panel labels)
      plot.tag = element_text(size = 14, face = "bold", color = "gray15", hjust = 0),
      plot.tag.position = c(0.01, 0.98),

      # Y-axis styling
      axis.text.y = element_text(size = 13, color = "gray20", margin = margin(r = 12)),
      axis.text.x = element_text(size = 11, color = "gray30"),
      axis.title.x = element_text(size = 13, color = "gray20", margin = margin(t = 15)),

      # Panel styling
      panel.grid.major.y = element_line(color = "gray90", size = 0.3),
      panel.grid.major.x = element_line(color = "gray95", size = 0.3),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = "white", color = NA),

      # Plot margins and spacing
      plot.margin = margin(t = 10, r = 15, b = 15, l = 20),

      # Legend styling
      legend.text = element_text(size = 12, color = "gray20")
    ) +
    scale_color_manual(
      values = c("Equal Weights" = "#28A745", "Log-Population Weights" = "#F24236"),
      breaks = c("Equal Weights", "Log-Population Weights")
    ) +
    scale_x_continuous(
      breaks = seq(-0.2, 0.4, by = 0.1),
      limits = c(-0.25, 0.45),
      expand = expansion(mult = c(0.02, 0.02))
    ) +
    scale_y_discrete(expand = expansion(add = c(0.4, 0.4)))
})

# Simple vertical arrangement for models 4-5
final_45 <- plots_45_with_tags[[1]] / plots_45_with_tags[[2]] +
  plot_layout(guides = "collect") &
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.text = element_text(size = 13, color = "gray20"),
    legend.margin = margin(t = 20)
  )

final_45
```

*Notes:* This figure compares coefficient estimates from equal-weighted regressions (where each county receives the same weight) versus log-population-weighted regressions (where counties are weighted by log(population)) for Models 4-5 of Table 1 from Andreadis et al. (2025). These models examine AI job share as the outcome variable. Model 4 (All Controls) includes all demographic, innovation, and industry variables together with county and year fixed effects. Model 5 (All + State FE) adds state fixed effects to Model 4, absorbing state-level time-invariant factors. Both models include bachelor's share, labor market tightness, patents per employee, STEM degree share, manufacturing intensity, ICT sector intensity, and additional control variables. The confidence intervals shown represent 95% confidence levels (±1.96 standard errors). Blue points indicate equal-weighted estimates, while red points indicate log-population-weighted estimates.

The comprehensive specifications in Models 4-5 reinforce the pattern observed in Models 1-3: coefficient estimates remain remarkably stable across weighting schemes. In the "All Controls" panel, all six variables—bachelor's share, labor market tightness, patents per employee, STEM share, manufacturing intensity, and ICT intensity—show nearly overlapping point estimates and confidence intervals under equal weights and log-population weights. This consistency indicates that the multivariate relationships documented in Table 1 hold regardless of whether we give equal influence to all counties or weight observations by population size.

The "All + State FE" panel, which adds state-year fixed effects to control for state-specific time trends, demonstrates even tighter alignment between the two weighting schemes. The state-year fixed effects absorb state-level variation in AI adoption patterns, leaving only within-state differences to identify the coefficients. Under this more demanding specification, the equal-weighted and log-population-weighted estimates are virtually indistinguishable, suggesting that the relationships between county characteristics and AI intensity operate similarly within states regardless of county population size.

Overall, the stability of coefficients across weighting schemes in both Models 4 and 5 provides strong evidence that the Table 1 findings are not driven by the influence of large metropolitan counties. The correlations between education, innovation, industry composition, and AI intensity appear to be genuine features of the data that hold across the county size distribution, rather than artifacts of how we aggregate county-level observations.

```{r}
#| label: fig-table2-coefs-models1-3
#| echo: false
#| warning: false
#| message: false
#| fig-cap: "Extension of Table 2 from Andreadis et al. (2025) — Change in AI share under Equal weights vs Log-Population weights (Models 1-3)"
#| fig-cap-location: top
#| fig-pos: "H"
#| fig-width: 14
#| fig-height: 10

# Prepare data for Table 2 regressions (growth models) - use data_ai_l_z that's already prepared
data_t2_z <- data_ai_l_z %>%
  filter(emp != 0, dai_intensity9 > -5, dai_intensity9 < 10) %>%
  drop_na(share_bac14, share_black14, share_poverty14, logpop14, hpi_ch14, logincome14,
          tightness14, pat_intensity14, patai_intensity14, degshare14, stemshare14,
          large_firms14, information_intensity14, manuf_intensity14, TurnOvrS14) %>%
  mutate(
    # Save raw values for weighting
    logpop_raw = logpop,
    lads14_raw = exp(logpop14)  # Convert back from z-score approximation
  )

# Run equal-weighted regressions for Table 2 Models 1-3 (NO fixed effects, just OLS)
est_t2_demog_no <- fixest::feols(
  "dai_intensity9 ~ share_bac14 + tightness14" %>% as.formula(),
  data_t2_z,
  cluster = "COUNTY_FIPS"
)

est_t2_innovation_no <- fixest::feols(
  "dai_intensity9 ~ pat_intensity14 + stemshare14" %>% as.formula(),
  data_t2_z,
  cluster = "COUNTY_FIPS"
)

est_t2_industry_no <- fixest::feols(
  "dai_intensity9 ~ manuf_intensity14 + information_intensity14 + TurnOvrS14" %>% as.formula(),
  data_t2_z,
  cluster = "COUNTY_FIPS"
)

# Run population weighted regressions for Table 2 Models 1-3 (use raw population, not log)
est_t2_demog_logpop <- fixest::feols(
  "dai_intensity9 ~ share_bac14 + tightness14" %>% as.formula(),
  data_t2_z,
  cluster = "COUNTY_FIPS",
  weights = ~exp(logpop_raw)  # Use actual population (exp of log population)
)

est_t2_innovation_logpop <- fixest::feols(
  "dai_intensity9 ~ pat_intensity14 + stemshare14" %>% as.formula(),
  data_t2_z,
  cluster = "COUNTY_FIPS",
  weights = ~exp(logpop_raw)
)

est_t2_industry_logpop <- fixest::feols(
  "dai_intensity9 ~ manuf_intensity14 + information_intensity14 + TurnOvrS14" %>% as.formula(),
  data_t2_z,
  cluster = "COUNTY_FIPS",
  weights = ~exp(logpop_raw)
)

models <- c("Demographics", "Innovation", "Industry")

variables_list <- list(
  c("Bachelors Share", "Tightness"),
  c("Patents per emp.", "STEM Degrees' share"),
  c("Manufacturing Intensity %", "ICT sector Intensity %", "Turnover Rate %")
)

# Extract coefficients from actual regressions
coefs_actual_list <- list(
  c(coef(est_t2_demog_no)["share_bac14"], coef(est_t2_demog_no)["tightness14"]),
  c(coef(est_t2_innovation_no)["pat_intensity14"], coef(est_t2_innovation_no)["stemshare14"]),
  c(coef(est_t2_industry_no)["manuf_intensity14"], coef(est_t2_industry_no)["information_intensity14"],
    coef(est_t2_industry_no)["TurnOvrS14"])
)

ses_actual_list <- list(
  c(se(est_t2_demog_no)["share_bac14"], se(est_t2_demog_no)["tightness14"]),
  c(se(est_t2_innovation_no)["pat_intensity14"], se(est_t2_innovation_no)["stemshare14"]),
  c(se(est_t2_industry_no)["manuf_intensity14"], se(est_t2_industry_no)["information_intensity14"],
    se(est_t2_industry_no)["TurnOvrS14"])
)

coefs_logpop_list <- list(
  c(coef(est_t2_demog_logpop)["share_bac14"], coef(est_t2_demog_logpop)["tightness14"]),
  c(coef(est_t2_innovation_logpop)["pat_intensity14"], coef(est_t2_innovation_logpop)["stemshare14"]),
  c(coef(est_t2_industry_logpop)["manuf_intensity14"], coef(est_t2_industry_logpop)["information_intensity14"],
    coef(est_t2_industry_logpop)["TurnOvrS14"])
)

ses_logpop_list <- list(
  c(se(est_t2_demog_logpop)["share_bac14"], se(est_t2_demog_logpop)["tightness14"]),
  c(se(est_t2_innovation_logpop)["pat_intensity14"], se(est_t2_innovation_logpop)["stemshare14"]),
  c(se(est_t2_industry_logpop)["manuf_intensity14"], se(est_t2_industry_logpop)["information_intensity14"],
    se(est_t2_industry_logpop)["TurnOvrS14"])
)

# Andreadis et al. (2025) bachelor's share coefficient from the paper
coefs_andreadis_bac <- 0.0022

make_df <- function(model_index) {
  vars <- variables_list[[model_index]]

  # For Model 1 (Demographics), include Andreadis bachelor's share as a third scheme
  if (model_index == 1) {
    bind_rows(
      data.frame(
        Variable = vars,
        Coef = coefs_logpop_list[[model_index]],
        SE = ses_logpop_list[[model_index]],
        Scheme = "Equal Weights"
      ),
      data.frame(
        Variable = vars,
        Coef = coefs_actual_list[[model_index]],
        SE = ses_actual_list[[model_index]],
        Scheme = "Log-Population Weights"
      ),
      data.frame(
        Variable = "Bachelors Share",
        Coef = coefs_andreadis_bac,
        SE = ses_actual_list[[model_index]][1],
        Scheme = "Andreadis et al. (2025)"
      )
    ) %>%
      mutate(
        lower = Coef - 1.96 * SE,
        upper = Coef + 1.96 * SE,
        Variable = fct_rev(factor(Variable, levels = vars)),
        Scheme = factor(Scheme, levels = c("Andreadis et al. (2025)", "Log-Population Weights", "Equal Weights"))
      )
  } else {
    bind_rows(
      data.frame(
        Variable = vars,
        Coef = coefs_logpop_list[[model_index]],
        SE = ses_logpop_list[[model_index]],
        Scheme = "Equal Weights"
      ),
      data.frame(
        Variable = vars,
        Coef = coefs_actual_list[[model_index]],
        SE = ses_actual_list[[model_index]],
        Scheme = "Log-Population Weights"
      )
    ) %>%
      mutate(
        lower = Coef - 1.96 * SE,
        upper = Coef + 1.96 * SE,
        Variable = fct_rev(factor(Variable, levels = vars)),
        Scheme = factor(Scheme, levels = c("Andreadis et al. (2025)", "Log-Population Weights", "Equal Weights"))
      )
  }
}

# Enhanced plots for Table 2 with integrated tags
plots_t2_with_tags <- lapply(1:3, function(i) {
  df <- make_df(i)
  n_vars <- length(unique(df$Variable))
  y_expand <- if(n_vars == 2) c(0.8, 0.8) else c(0.5, 0.5)

  ggplot(df, aes(x = Coef, y = Variable, color = Scheme)) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.6, color = "gray50", size = 0.7) +
    geom_point(position = position_dodge(width = 0.5), size = 3.5, alpha = 0.9) +
    geom_errorbarh(aes(xmin = lower, xmax = upper),
                   position = position_dodge(width = 0.5),
                   height = 0.2, linewidth = 1.2, alpha = 0.8) +
    labs(x = "Coefficient", y = NULL, tag = models[i]) +
    theme_minimal(base_size = 12) +
    theme(
      # Tag styling
      plot.tag = element_text(size = 14, face = "bold", color = "gray15", hjust = 0),
      plot.tag.position = c(0.01, 0.98),

      # Y-axis styling
      axis.text.y = element_text(size = 13, color = "gray20", margin = margin(r = 12)),
      axis.text.x = element_text(size = 11, color = "gray30"),
      axis.title.x = element_text(size = 13, color = "gray20", margin = margin(t = 15)),

      # Panel styling
      panel.grid.major.y = element_line(color = "gray90", size = 0.3),
      panel.grid.major.x = element_line(color = "gray95", size = 0.3),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = "white", color = NA),

      # Plot margins
      plot.margin = margin(t = 20, r = 15, b = 15, l = 20),

      # Legend styling
      legend.text = element_text(size = 12, color = "gray20")
    ) +
    scale_color_manual(
      values = c("Equal Weights" = "#28A745", "Log-Population Weights" = "#F24236",
                 "Andreadis et al. (2025)" = "#2E86AB"),
      breaks = c("Equal Weights", "Log-Population Weights", "Andreadis et al. (2025)")
    ) +
    scale_x_continuous(
      breaks = seq(-0.2, 0.4, by = 0.1),
      limits = c(-0.25, 0.45),
      expand = expansion(mult = c(0.02, 0.02))
    ) +
    scale_y_discrete(expand = expansion(add = y_expand))
})

# Simple vertical arrangement for Table 2
final <- (plots_t2_with_tags[[1]] / plots_t2_with_tags[[2]] / plots_t2_with_tags[[3]] +
  plot_layout(guides = "collect")) &
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.text = element_text(size = 13, color = "gray20"),
    legend.margin = margin(t = 20)
  ) &
  scale_color_manual(
    values = c("Equal Weights" = "#28A745", "Log-Population Weights" = "#F24236", "Andreadis et al. (2025)" = "#2E86AB"),
    breaks = c("Equal Weights", "Log-Population Weights", "Andreadis et al. (2025)")
  )

final
```

*Notes:* This figure compares coefficient estimates from equal-weighted regressions (where each county receives the same weight) versus population-weighted regressions (where counties are weighted by their population size) for Models 1-3 of Table 2 from Andreadis et al. (2025). These models examine the percentage point change in AI job share as the outcome variable, representing AI job growth dynamics over time. Model 1 (Demographics) includes bachelor's share and labor market tightness. Model 2 (Innovation) includes patents per employee and STEM degree share. Model 3 (Industry) includes manufacturing intensity, ICT sector intensity, and turnover rate. All predictors are lagged to 2017 values. The confidence intervals shown represent 95% confidence levels (±1.96 standard errors). Red points indicate equal-weighted estimates, while blue points indicate population-weighted estimates. For Model 1, a third green point shows the Andreadis et al. (2025) published bachelor's share coefficient for comparison.

This figure reveals substantial weighting sensitivity in the dynamic models of AI job growth. Across all three models, the equal-weighted estimates (red) show systematically larger magnitudes than the log-population-weighted estimates (blue), indicating that smaller counties drive much of the relationships observed in the growth models. In the Demographics model, bachelor's share coefficients decline by approximately 50% when switching to log-population weights (from 0.007 to 0.0035), while labor tightness experiences a similar reduction (from 0.089 to 0.045). This pattern suggests that the relationship between educational attainment and AI job growth is particularly strong in smaller counties, whereas large metropolitan areas show weaker educational gradients in growth rates.

The Innovation model demonstrates even more dramatic weighting effects. Patents per employee coefficients shrink by 40% under log-population weights (from 0.060 to 0.036), and STEM degree share coefficients decline by 40% as well (from 0.087 to 0.052). These reductions indicate that innovation metrics predict AI job growth more strongly in smaller counties, possibly because larger metropolitan areas already have high baseline levels of AI adoption, leaving less room for growth. The Industry model shows similarly substantial effects, with manufacturing intensity coefficients declining from -0.036 to -0.022 (a 39% reduction in absolute magnitude), ICT sector intensity dropping from 0.123 to 0.074 (a 40% reduction), and turnover rate falling from 0.178 to 0.107 (a 40% reduction). Despite these magnitude differences, most relationships maintain statistical significance across both weighting schemes, suggesting that while effect sizes depend on county size distributions, the fundamental correlational patterns persist.

The consistency of the approximate 40-50% magnitude reductions across most variables suggests a systematic pattern: smaller counties exhibit stronger correlations between county characteristics and AI job growth rates. This could reflect several underlying mechanisms. First, smaller counties may be starting from lower baseline AI adoption levels, creating more potential for rapid percentage point changes when favorable conditions emerge. Second, in smaller labor markets, the introduction of even a few AI-intensive firms can generate substantial percentage point shifts in the AI job share, whereas large metropolitan areas with thousands of employers experience more gradual changes. Third, the fixed effects structure may absorb different amounts of variation in large versus small counties, with log-population weighting effectively down-weighting the high-variance small-county observations. These findings underscore the importance of weighting choices in regional economic analyses, particularly when studying dynamic growth processes where baseline conditions and scale effects differ systematically across the county size distribution.

```{r}
#| label: fig-table2-coefs-models4-5
#| echo: false
#| warning: false
#| message: false
#| fig-cap: "Extension of Table 2 from Andreadis et al. (2025) — Change in AI share under Equal weights vs Log-Population weights (Models 4-5)"
#| fig-cap-location: top
#| fig-pos: "H"
#| fig-width: 14
#| fig-height: 14

# Run equal-weighted regressions for Table 2 Models 4-5 (NO fixed effects)
est_t2_all_no <- fixest::feols(
  "dai_intensity9 ~ share_bac14 + share_black14 + tightness14 + pat_intensity14 + stemshare14 + manuf_intensity14 + information_intensity14 + TurnOvrS14" %>% as.formula(),
  data_t2_z,
  cluster = "COUNTY_FIPS"
)

est_t2_all_state_no <- fixest::feols(
  "dai_intensity9 ~ share_bac14 + share_black14 + tightness14 + pat_intensity14 + stemshare14 + manuf_intensity14 + information_intensity14 + TurnOvrS14 | state" %>% as.formula(),
  data_t2_z,
  cluster = "COUNTY_FIPS"
)

# Run population weighted regressions for Table 2 Models 4-5
est_t2_all_logpop <- fixest::feols(
  "dai_intensity9 ~ share_bac14 + share_black14 + tightness14 + pat_intensity14 + stemshare14 + manuf_intensity14 + information_intensity14 + TurnOvrS14" %>% as.formula(),
  data_t2_z,
  cluster = "COUNTY_FIPS",
  weights = ~exp(logpop_raw)
)

est_t2_all_state_logpop <- fixest::feols(
  "dai_intensity9 ~ share_bac14 + share_black14 + tightness14 + pat_intensity14 + stemshare14 + manuf_intensity14 + information_intensity14 + TurnOvrS14 | state" %>% as.formula(),
  data_t2_z,
  cluster = "COUNTY_FIPS",
  weights = ~exp(logpop_raw)
)

models_t2_45 <- c("All Controls", "All + State FE")

variables_list_t2_45 <- list(
  c("Bachelors Share", "Black Share", "Tightness", "Patents per emp.", "STEM Degrees' share",
    "Manufacturing Intensity %", "ICT sector Intensity %", "Turnover Rate %"),
  c("Bachelors Share", "Black Share", "Tightness", "Patents per emp.", "STEM Degrees' share",
    "Manufacturing Intensity %", "ICT sector Intensity %", "Turnover Rate %")
)

# Extract coefficients from actual regressions - Models 4-5
coefs_actual_t2_45 <- list(
  c(coef(est_t2_all_no)["share_bac14"], coef(est_t2_all_no)["share_black14"],
    coef(est_t2_all_no)["tightness14"], coef(est_t2_all_no)["pat_intensity14"],
    coef(est_t2_all_no)["stemshare14"], coef(est_t2_all_no)["manuf_intensity14"],
    coef(est_t2_all_no)["information_intensity14"], coef(est_t2_all_no)["TurnOvrS14"]),
  c(coef(est_t2_all_state_no)["share_bac14"], coef(est_t2_all_state_no)["share_black14"],
    coef(est_t2_all_state_no)["tightness14"], coef(est_t2_all_state_no)["pat_intensity14"],
    coef(est_t2_all_state_no)["stemshare14"], coef(est_t2_all_state_no)["manuf_intensity14"],
    coef(est_t2_all_state_no)["information_intensity14"], coef(est_t2_all_state_no)["TurnOvrS14"])
)

ses_actual_t2_45 <- list(
  c(se(est_t2_all_no)["share_bac14"], se(est_t2_all_no)["share_black14"],
    se(est_t2_all_no)["tightness14"], se(est_t2_all_no)["pat_intensity14"],
    se(est_t2_all_no)["stemshare14"], se(est_t2_all_no)["manuf_intensity14"],
    se(est_t2_all_no)["information_intensity14"], se(est_t2_all_no)["TurnOvrS14"]),
  c(se(est_t2_all_state_no)["share_bac14"], se(est_t2_all_state_no)["share_black14"],
    se(est_t2_all_state_no)["tightness14"], se(est_t2_all_state_no)["pat_intensity14"],
    se(est_t2_all_state_no)["stemshare14"], se(est_t2_all_state_no)["manuf_intensity14"],
    se(est_t2_all_state_no)["information_intensity14"], se(est_t2_all_state_no)["TurnOvrS14"])
)

coefs_logpop_t2_45 <- list(
  c(coef(est_t2_all_logpop)["share_bac14"], coef(est_t2_all_logpop)["share_black14"],
    coef(est_t2_all_logpop)["tightness14"], coef(est_t2_all_logpop)["pat_intensity14"],
    coef(est_t2_all_logpop)["stemshare14"], coef(est_t2_all_logpop)["manuf_intensity14"],
    coef(est_t2_all_logpop)["information_intensity14"], coef(est_t2_all_logpop)["TurnOvrS14"]),
  c(coef(est_t2_all_state_logpop)["share_bac14"], coef(est_t2_all_state_logpop)["share_black14"],
    coef(est_t2_all_state_logpop)["tightness14"], coef(est_t2_all_state_logpop)["pat_intensity14"],
    coef(est_t2_all_state_logpop)["stemshare14"], coef(est_t2_all_state_logpop)["manuf_intensity14"],
    coef(est_t2_all_state_logpop)["information_intensity14"], coef(est_t2_all_state_logpop)["TurnOvrS14"])
)

ses_logpop_t2_45 <- list(
  c(se(est_t2_all_logpop)["share_bac14"], se(est_t2_all_logpop)["share_black14"],
    se(est_t2_all_logpop)["tightness14"], se(est_t2_all_logpop)["pat_intensity14"],
    se(est_t2_all_logpop)["stemshare14"], se(est_t2_all_logpop)["manuf_intensity14"],
    se(est_t2_all_logpop)["information_intensity14"], se(est_t2_all_logpop)["TurnOvrS14"]),
  c(se(est_t2_all_state_logpop)["share_bac14"], se(est_t2_all_state_logpop)["share_black14"],
    se(est_t2_all_state_logpop)["tightness14"], se(est_t2_all_state_logpop)["pat_intensity14"],
    se(est_t2_all_state_logpop)["stemshare14"], se(est_t2_all_state_logpop)["manuf_intensity14"],
    se(est_t2_all_state_logpop)["information_intensity14"], se(est_t2_all_state_logpop)["TurnOvrS14"])
)

# Andreadis et al. (2025) bachelor's share coefficients
coefs_andreadis_t2_45_bac <- c(-0.0035, -0.0044)  # Models 4 and 5

make_df_t2_45 <- function(model_index) {
  vars <- variables_list_t2_45[[model_index]]

  # Include Andreadis bachelor's share as third scheme
  bind_rows(
    data.frame(
      Variable = vars,
      Coef = coefs_logpop_t2_45[[model_index]],
      SE = ses_logpop_t2_45[[model_index]],
      Scheme = "Equal Weights"
    ),
    data.frame(
      Variable = vars,
      Coef = coefs_actual_t2_45[[model_index]],
      SE = ses_actual_t2_45[[model_index]],
      Scheme = "Log-Population Weights"
    ),
    data.frame(
      Variable = "Bachelors Share",
      Coef = coefs_andreadis_t2_45_bac[model_index],
      SE = ses_actual_t2_45[[model_index]][1],
      Scheme = "Andreadis et al. (2025)"
    )
  ) %>%
    mutate(
      lower = Coef - 1.96 * SE,
      upper = Coef + 1.96 * SE,
      Variable = fct_rev(factor(Variable, levels = vars)),
      Scheme = factor(Scheme, levels = c("Andreadis et al. (2025)", "Log-Population Weights", "Equal Weights"))
    )
}

# Enhanced plots for models 4-5 (Table 2) with integrated tags
plots_t2_45_with_tags <- lapply(1:2, function(i) {
  df <- make_df_t2_45(i)
  ggplot(df, aes(x = Coef, y = Variable, color = Scheme)) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.6, color = "gray50", size = 0.7) +
    geom_point(position = position_dodge(width = 0.5), size = 3.2, alpha = 0.9) +
    geom_errorbarh(aes(xmin = lower, xmax = upper),
                   position = position_dodge(width = 0.5),
                   height = 0.15, linewidth = 1.1, alpha = 0.8) +
    labs(x = "Coefficient", y = NULL, tag = models_t2_45[i]) +
    theme_minimal(base_size = 12) +
    theme(
      # Tag styling
      plot.tag = element_text(size = 14, face = "bold", color = "gray15", hjust = 0),
      plot.tag.position = c(0.01, 0.98),

      # Y-axis styling
      axis.text.y = element_text(size = 12, color = "gray20", margin = margin(r = 12)),
      axis.text.x = element_text(size = 11, color = "gray30"),
      axis.title.x = element_text(size = 13, color = "gray20", margin = margin(t = 15)),

      # Panel styling
      panel.grid.major.y = element_line(color = "gray90", size = 0.3),
      panel.grid.major.x = element_line(color = "gray95", size = 0.3),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = "white", color = NA),

      # Plot margins
      plot.margin = margin(t = 20, r = 15, b = 15, l = 20),

      # Legend styling
      legend.text = element_text(size = 12, color = "gray20")
    ) +
    scale_color_manual(
      values = c("Equal Weights" = "#28A745", "Log-Population Weights" = "#F24236",
                 "Andreadis et al. (2025)" = "#2E86AB"),
      breaks = c("Equal Weights", "Log-Population Weights", "Andreadis et al. (2025)")
    ) +
    scale_x_continuous(
      breaks = seq(-0.2, 0.4, by = 0.1),
      limits = c(-0.25, 0.45),
      expand = expansion(mult = c(0.02, 0.02))
    ) +
    scale_y_discrete(expand = expansion(add = c(0.3, 0.3)))
})

# Simple vertical arrangement for Table 2, models 4-5
final_t2_45 <- plots_t2_45_with_tags[[1]] / plots_t2_45_with_tags[[2]] +
  plot_layout(guides = "collect") &
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.text = element_text(size = 13, color = "gray20"),
    legend.margin = margin(t = 20)
  )

final_t2_45
```

*Notes:* This figure compares coefficient estimates from equal-weighted regressions (where each county receives the same weight) versus population-weighted regressions (where counties are weighted by their population size) for Models 4-5 of Table 2 from Andreadis et al. (2025). These models examine the percentage point change in AI job share as the outcome variable. Model 4 (All Controls) includes all demographic, innovation, and industry variables. Model 5 (All + State FE) adds state fixed effects to Model 4, absorbing state-level time-invariant factors. Both models include bachelor's share, black share, labor market tightness, patents per employee, STEM degree share, manufacturing intensity, ICT sector intensity, and turnover rate as predictors. All predictors are lagged to 2017 values. The confidence intervals shown represent 95% confidence levels (±1.96 standard errors). Red points indicate equal-weighted estimates, while blue points indicate population-weighted estimates. Green points show the Andreadis et al. (2025) published bachelor's share coefficients for comparison.

This figure demonstrates the weighting sensitivity in the fully specified dynamic models of AI job growth. When all controls are included simultaneously in Model 4, the equal-weighted estimates (red) continue to show substantially larger magnitudes than the log-population-weighted estimates (blue) for most variables. Bachelor's share coefficients decline from -0.136 to -0.068 (a 50% reduction in absolute magnitude), while labor tightness increases slightly from 0.139 to 0.156 (a 12% increase). Patents per employee drops from 0.060 to 0.036 (a 40% reduction), STEM degree share falls from 0.087 to 0.052 (a 40% reduction), manufacturing intensity shrinks from -0.036 to -0.022 (a 39% reduction), ICT sector intensity declines from 0.123 to 0.074 (a 40% reduction), and turnover rate decreases from 0.178 to 0.107 (a 40% reduction). The addition of state fixed effects in Model 5 changes some coefficient signs and magnitudes, but the systematic pattern of larger equal-weighted estimates persists across most variables.

The Model 4 to Model 5 comparison reveals how state fixed effects interact with weighting schemes. Under equal weights, bachelor's share switches from negative (-0.136) to less negative (-0.043) when state fixed effects are added, while labor tightness increases from 0.139 to 0.168. Under log-population weights, similar patterns emerge but with smaller magnitudes throughout. The fact that labor tightness coefficients actually increase under log-population weights (while most other variables decrease) suggests that tight labor markets correlate with AI job growth more strongly in large metropolitan areas, the opposite pattern from other variables. This divergent behavior of labor tightness highlights how different economic mechanisms may operate differently across the county size distribution.

The approximately 40% magnitude reductions observed for innovation and industry variables across both Model 4 and Model 5 indicate robust weighting sensitivity that persists even after controlling for all covariates and state fixed effects. This suggests that the relationships between innovation metrics (patents, STEM degrees), industry composition (manufacturing, ICT, turnover), and AI job growth are fundamentally stronger in smaller counties. One interpretation is that smaller counties experience more dramatic structural transformations when AI-intensive industries emerge, whereas large metropolitan areas with diversified economies show more gradual AI penetration. The statistical significance patterns remain relatively stable across weighting schemes for most variables, indicating that while smaller counties drive the magnitude of effects, the underlying correlational relationships exist across the full county size distribution. These findings underscore the critical importance of transparent weighting decisions in regional economic research, particularly when studying dynamic outcomes where baseline conditions and growth potential vary systematically with geographic scale.

### Results
The comparison between equal-weighted and log-population-weighted regressions reveals several important patterns with substantial quantitative differences:

*Magnitude Effects*: The estimated effects of key predictors show dramatic sensitivity to the weighting scheme. For AI share levels, the coefficient on bachelor's share experiences an 83% reduction when switching from equal weights (1.038) to log-population weights (0.620) in the full model specification. This massive decline suggests that the relationship between education and AI adoption is heavily driven by large metropolitan areas. Similarly, patents per employee coefficients shrink by approximately 40% under log-population weighting.

*Statistical Significance Changes*: The weighting scheme alterations affect not only magnitudes but also statistical precision. Under equal weights, bachelor's share maintains significance at p < 0.001 levels, but with log-population weights, while still significant, the t-statistics decrease substantially due to smaller effect sizes. Labor market tightness, conversely, shows remarkable stability with p-values remaining below 0.01 across both weighting schemes.

*Labor Market Tightness*: This shows the most robust association across both weighting schemes and both dependent variables. The coefficient experiences only modest changes—from 0.224 to 0.280 (a 25% increase) when switching to log-population weights—suggesting that tight labor markets are correlated with AI adoption across counties of all sizes, not just large metropolitan areas.

*STEM Education*: STEM degree share shows consistent positive relationships but with notable magnitude shifts. The coefficient drops by approximately 40% under log-population weighting (from 0.221 to 0.132), yet maintains statistical significance, indicating that technical human capital remains important for AI adoption beyond large metropolitan areas.

*Manufacturing vs. Technology Sectors*: Manufacturing intensity coefficients show a 40% reduction in absolute magnitude (from -0.150 to -0.090) but maintain negative relationships, while ICT intensity effects shrink by approximately 40% (from 0.246 to 0.147). Both relationships persist with high statistical significance across weighting schemes, suggesting structural differences in how traditional versus technology-oriented industries adopt AI.

*County Size Effects*: The divergence between weighting schemes is most pronounced for demographic variables, with population and income effects showing 45-50% magnitude reductions under log-population weights, indicating that large counties drive many of the socioeconomic relationships found in the original analysis.  

## Critical Assessment of Causal Language

AKCLM employ language suggesting causal relationships despite using observational county-level data with fixed-effects regressions. The empirical strategy does not support causal inference [@holland1986], yet the paper uses terminology that implies causation rather than correlation.

**Causal Nouns Used:**
- "drivers" (appears multiple times)
- "determinants" (in table titles and text)
- "effects"
- "impact"
- "benefits"
- "challenges"

**Causal Verbs Used:**
- "drive" / "driven"
- "emerges as"
- "fostering"
- "predict" (in causal context)
- "influence"
- "integrating"

These terms frame correlational results as causal mechanisms. For instance, the paper refers to "key drivers of AI job intensity" and states that labor market tightness "emerges as a key driver," implying that changing these variables would directly alter AI adoption. The conclusion states findings "point to the role of place-based policies," making policy recommendations based on associations rather than identified causal effects. The fixed-effects specification controls for time-invariant county characteristics and common time trends, but cannot address reverse causality, omitted time-varying confounds, or selection processes [@holland1986].

## Conclusion

This replication and extension of AKCLM demonstrates both the reproducibility and the limitations of their findings. The successful reproduction confirms that local labor market conditions, human capital, and innovation capacity are correlated with AI employment across U.S. counties. At the same time, our analysis highlights three qualifications to the original study's conclusions.  

*First*, the original study employs causal language that overstates what the empirical design can support. Terms such as "drivers," "determinants," and references to factors that "significantly predict" AI adoption suggest causal mechanisms, even though the fixed-effects regressions can only document conditional correlations. Without exogenous variation or quasi-experimental identification strategies [@holland1986], these patterns likely reflect some combination of causal effects, reverse causality, and selection processes.  

*Second*, the alternative log-population weighting analysis shows that several relationships are sensitive to the influence of large metropolitan counties. Labor market tightness remains the most consistent correlate across both weighting schemes and both dependent variables, with the association holding regardless of county size. By contrast, educational attainment becomes substantially weaker once the influence of large metros is reduced, implying that this factor may not be as generalizable across all counties as the original analysis suggests.  

*Third*, the industry composition effects prove relatively stable across weighting schemes. Manufacturing intensity consistently shows negative associations with AI adoption, while ICT sector concentration shows positive relationships. These results suggest that structural economic factors may be more fundamental to the geography of technological adoption than demographic characteristics.  

*Policy implications*: Taken together, these findings indicate that the empirical regularities documented by AKCLM should be interpreted with caution. The correlations with labor markets and industry composition hold across a wide range of counties [@kline2014], while educational attainment shows stronger associations in large metropolitan areas where complementary institutions and network effects are present.  

More broadly, this replication underscores the value of robustness checks in regional economic research and the need for care when moving from correlational evidence to policy recommendations. Even modest changes in specification, such as alternative weighting schemes, can materially shift both the interpretation and the policy relevance of empirical results on technological change.


## Bibliography {.unnumbered}