---
title: "Local Heterogeneity in Artificial Intelligence Jobs Over Time and Space"
subtitle: "A Replication Study of Andreadis et al. (AEA Papers and Proceedings, 2025)"
author:
  - name: Jacob Khaykin^[Solon High School, [jacobkhaykin27@solonschools.net](mailto:jacobkhaykin27@solonschools.net)]
  - name: David Kane^[Institute for Globally Distributed Open Research and Education, [dave.kane@gmail.com](mailto:dave.kane@gmail.com)]
bibliography: references.bib
csl: apa.csl
link-citations: true
nocite: '@*'
format:
  pdf:
    keep-tex: true
    number-sections: true
    documentclass: article
    include-in-header:
      - text: |
          \usepackage{float}
          % reset numbering after abstract
          \usepackage{etoolbox}
          \AtBeginEnvironment{abstract}{\setcounter{section}{0}}
  docx: default
---

<!-- DK:  -->

<!-- Change to new repo Done. -->

<!-- DK: Ensure noun/verb agreement. -->

<!-- DK: Look over prose closely. Give AI each paragraph one at a time, after setting up the context. "You are a PhD economist writing an academic paper. ... done"  -->

<!-- DK: Do not use two different terms for our key variable, e.g., AI job concentration and AI employment. -->

<!-- DK: Your graphic needs bigger font. Also, log-pop label key after original.Done -->

<!-- DK: Make the font in the tables a little bigger. -->

<!-- DK: Equation is good but does not belong in the introduction. Done -->

<!-- DK: Make introduction more connected to our new abstract. First paragraph is general why this is important. Second paragraph explains the left side variables. Third explains the right side variables. Fourth paragraph explains their main results. Fifth paragraph explains what happens when you log-pop weight. Last paragraph talks about causal stuff. Done  -->

*JEL: J24, O33, R11*

*Keywords:* Artificial Intelligence, Regional Economics, Labor Markets

*Data Availability:* The R code and data to reproduce this replication are available in this repository: https://github.com/JacobKhay/Andreadis-Replication.




## Abstract {.unnumbered}

We replicate @andreadis2025 on the correlation between the level and growth artificial intelligence job openings and education, innovation, and industry factors across U.S. counties from 2014 to 2023. We successfully reproduce their main results and extend the analysis by employing log-population weights to assess robustness. The core associations persist, though most magnitudes shrink. We also highlight unsupported causal claims in @andreadis2025, given its observational design. 

Declaration: There are no financial conflicts of interest to share. 

\newpage

## Introduction

This paper replicates the analysis of @andreadis2025 on the correlation between the level and growth artificial intelligence job openings and education, innovation, and industry factors across U.S. counties from 2014 to 2023. Understanding where AI employment emerges and how it spreads is important for both researchers and policymakers, since the rise of AI has the potential to reshape regional economies, alter the demand for skills, and shift patterns of innovation. County-level variation offers a granular perspective on how these transformations take hold across the United States.

The outcome of interest is the share of AI-related job postings at the county-year level. This measure captures both the absolute level of AI employment opportunities and how those opportunities evolve over time. Tracking these dynamics provides insight into which regions gain early access to AI-driven growth and which lag behind.

The original study links county-level AI job shares to several explanatory factors. Education is captured by the share of adults with a college degree, innovation by local patenting activity, and industry factors by the composition of employment across sectors. Together, these variables represent structural characteristics that might condition whether a region becomes a hub for AI-related work.

@andreadis2025 find that AI employment is strongly correlated with higher educational attainment, more innovation activity, and certain industry profiles. These associations are robust across specifications, suggesting that counties with strong human capital, innovative capacity, and aligned industries are more likely to attract AI jobs. The results highlight structural divides in access to AI employment opportunities across the country.

We reproduce these results and then extend the analysis by applying log-population weights. This alternative weighting scheme reduces the influence of the largest counties while still reflecting the relative size of local labor markets. Under this adjustment, the core relationships persist but the estimated magnitudes generally shrink. This indicates that while large counties contribute to the overall pattern, the relationships hold across the broader distribution of counties.

Finally, we note that some of the causal interpretations in @andreadis2025 are not supported by the observational nature of the data. While the correlations are informative and highlight important regional patterns, they do not establish that education, innovation, or industry factors directly cause higher AI employment. Our replication underscores the value of the evidence while also emphasizing the limits of what can be inferred from the design.



## Data

The study utilizes multiple data sources to construct a comprehensive county-level dataset spanning 2014–2023:

**AI Employment Data**: Job posting data from Lightcast, which aggregates information from over 40,000 online job boards, newspapers, and employer websites. AI-related jobs are identified through skills and keywords associated with AI development and use. The dependent variable $AI_{it}$ is defined as:

$$
AI_{it} = \frac{\text{AI job postings}_{it}}{\text{Total job postings}_{it}} \times 100 \tag{1}
$$

**Demographic Variables**: From the American Community Survey (ACS), including:  
- Bachelor's share: Percentage of workforce with bachelor's degree or higher  
- Black population share: Percentage of county population identifying as Black  
- Poverty share: Percentage of population below federal poverty line  
- Log population: Natural logarithm of county population  
- Log median income: Natural logarithm of median household income  

**Innovation Indicators**:  
- Patents per employee: USPTO patent counts normalized by employment  
- AI patents share: Percentage of patents classified as AI-related  
- STEM degrees share: Percentage of awarded degrees in STEM fields  
- Degrees per capita: Total degrees awarded per capita  

**Industry and Labor Market Variables**:  
- Labor market tightness: Ratio of job postings to unemployed workers  
- Manufacturing intensity: Employment share in manufacturing sector  
- ICT intensity: Employment share in information and communication technology  
- Turnover rate: Worker separation rate from Quarterly Workforce Indicators  
- Large establishments share: Percentage of employment in large firms  

**Housing Market**: House price growth from Federal Housing Finance Agency (FHFA)  

All explanatory variables are lagged by one year to address potential endogeneity concerns. The baseline specification relates the dependent variable to these covariates using the following model:

$$
AIShare_{ct} = \alpha + \beta_1 Education_{ct} + \beta_2 Innovation_{ct} + \beta_3 Industry_{ct} + \gamma X_{ct} + \epsilon_{ct} \tag{2}
$$

where $AIShare_{ct}$ denotes the proportion of AI-related job postings in county $c$ at year $t$, and $X_{ct}$ includes demographic, labor market, and housing controls.

### Reproduction of Original Results

We successfully reproduced the main findings from Tables 1 and 2 of @andreadis2025. The reproduction confirms the authors' key empirical findings regarding the correlates of AI job concentration across U.S. counties. All coefficients, standard errors, and significance levels match the original results within rounding error, demonstrating the reproducibility of their analysis.
```{r}
#| label: tbl-table1
#| echo: false
#| warning: false
#| message: false
#| results: asis
#| tbl-cap: "Replication of Table 1 from Andreadis et al. (2025) - The Correlates of the Share of Artificial Intelligence Jobs"
#| tbl-pos: "H"

library(tidyverse)      
library(fixest)         
library(modelsummary)   
library(scales)         
library(kableExtra)     

data_ai <- read_csv("data.csv") |>   
  ungroup() |>                       
  mutate(
    degshare = (udeg + mdeg) / Employed,            
    stemshare = (ustemdeg + mstemdeg) / (udeg + mdeg),  
    stemshare2 = ustemdeg / udeg,                   
    stemshare = replace_na(stemshare, 0),           
    stemshare2 = replace_na(stemshare2, 0),         
    tightness = nads / Unemployed,                  
    hpi_ch = hpi_ch / 100,                          
    logpop = log(pop),                              
    logincome = log(medhhincome),                   
    pat_intensity = n_inventors / Employed,         
    patai_intensity = ai_patents / n_patents,       
    large_firms = 1 - (small + medium) / est,       
    information_intensity = information_emp / emp,  
    manuf_intensity = manuf_emp / emp,              
    ai_intensity = ai / nads,                       
    lads = log(1 + nads),                           
    state_year = paste0(state, Year)                
  )

data_ai_z <- data_ai |> 
  filter(emp != 0) |>                               
  mutate(
    share_bac = scale(share_bac),                   
    share_black = scale(share_black),               
    share_poverty = scale(share_poverty),           
    logpop = scale(logpop),                         
    hpi_ch = scale(hpi_ch),                         
    logincome = scale(logincome),                   
    tightness = scale(tightness),                   
    unrate = scale(unrate),                         
    pat_intensity = scale(pat_intensity),           
    patai_intensity = scale(patai_intensity),       
    degshare = scale(degshare),                     
    stemshare = scale(stemshare),                   
    large_firms = scale(large_firms),               
    information_intensity = scale(information_intensity),  
    manuf_intensity = scale(manuf_intensity),       
    TurnOvrS = scale(TurnOvrS),                     
    ai_intensity = ai_intensity * 100               
  )

est_demog_no = feols(
  ai_intensity ~ share_bac + share_black + share_poverty + logpop + hpi_ch + logincome + tightness | Year + COUNTY_FIPS,
  data = data_ai_z,
  cluster = "COUNTY_FIPS",
  weights = ~lads
)

est_innovation_no = feols(
  ai_intensity ~ pat_intensity + patai_intensity + degshare + stemshare | Year + COUNTY_FIPS,
  data = data_ai_z,
  cluster = "COUNTY_FIPS",
  weights = ~lads
)

est_industry_no = feols(
  ai_intensity ~ large_firms + information_intensity + manuf_intensity + TurnOvrS | Year + COUNTY_FIPS,
  data = data_ai_z,
  cluster = "COUNTY_FIPS",
  weights = ~lads
)

est_all = feols(
  ai_intensity ~ share_bac + share_black + share_poverty + logpop + hpi_ch + logincome + tightness +
    pat_intensity + patai_intensity + degshare + stemshare + large_firms +
    information_intensity + manuf_intensity + TurnOvrS | Year + COUNTY_FIPS,
  data = data_ai_z,
  cluster = "COUNTY_FIPS",
  weights = ~lads
)

est_all_large = feols(
  ai_intensity ~ share_bac + share_black + share_poverty + logpop + hpi_ch + logincome + tightness +
    pat_intensity + patai_intensity + degshare + stemshare + large_firms +
    information_intensity + manuf_intensity + TurnOvrS | state_year + COUNTY_FIPS,
  data = data_ai_z,
  cluster = "COUNTY_FIPS",
  weights = ~lads
)

obs1 <- comma(nobs(est_demog_no))
obs2 <- comma(nobs(est_innovation_no))
obs3 <- comma(nobs(est_industry_no))
obs4 <- comma(nobs(est_all))
obs5 <- comma(nobs(est_all_large))

r21 <- sprintf("%.3f", r2(est_demog_no, "r2"))
r22 <- sprintf("%.3f", r2(est_innovation_no, "r2"))
r23 <- sprintf("%.3f", r2(est_industry_no, "r2"))
r24 <- sprintf("%.3f", r2(est_all, "r2"))
r25 <- sprintf("%.3f", r2(est_all_large, "r2"))

wr21 <- sprintf("%.3f", r2(est_demog_no, "wr2"))
wr22 <- sprintf("%.3f", r2(est_innovation_no, "wr2"))
wr23 <- sprintf("%.3f", r2(est_industry_no, "wr2"))
wr24 <- sprintf("%.3f", r2(est_all, "wr2"))
wr25 <- sprintf("%.3f", r2(est_all_large, "wr2"))

extra_rows <- tribble(
  ~term, ~`(1)`, ~`(2)`, ~`(3)`, ~`(4)`, ~`(5)`,
  "\\textit{Fixed-effects}", "", "", "", "", "",
  "Year", "Yes", "Yes", "Yes", "Yes", "Yes",
  "County", "Yes", "Yes", "Yes", "Yes", "Yes",
  "State Year", "", "", "", "", "Yes",
  "\\textit{Fit statistics}", "", "", "", "", "",
  "Observations", obs1, obs2, obs3, obs4, obs5,
  "R²", r21, r22, r23, r24, r25,
  "Within R²", wr21, wr22, wr23, wr24, wr25
)

# Updated labels with significantly larger font size commands
labels <- c(
  "share_bac" = "\\huge{Bachelor's share, z-score}",
  "share_black" = "\\huge{Black pop, z-score}",
  "share_poverty" = "\\huge{Poverty share, z-score}",
  "logpop" = "\\huge{log(Population), z-score}",
  "hpi_ch" = "\\huge{House Price Growth, z-score}",
  "logincome" = "\\huge{log(Median Income), z-score}",
  "tightness" = "\\huge{Labor Market Tightness, z-score}",
  "pat_intensity" = "\\huge{Patents per employee, z-score}",
  "patai_intensity" = "\\huge{AI patents' share, z-score}",
  "degshare" = "\\huge{Degrees awarded per capita, z-score}",
  "stemshare" = "\\huge{STEM Degrees' share, z-score}",
  "large_firms" = "\\huge{Large Establishments, z-score}",
  "information_intensity" = "\\huge{ICT sector Intensity, z-score}",
  "manuf_intensity" = "\\huge{Manufacturing Intensity, z-score}",
  "TurnOvrS" = "\\huge{Turnover Rate, z-score}"
)

modelsummary(
  list(
    "(1)" = est_demog_no,
    "(2)" = est_innovation_no,
    "(3)" = est_industry_no,
    "(4)" = est_all,
    "(5)" = est_all_large
  ),
  coef_map = labels,
  stars = c("***" = 0.01, "**" = 0.05, "*" = 0.1),
  gof_omit = ".*",
  add_rows = extra_rows,
  escape = FALSE,
  statistic = "({std.error})",
  output = "kableExtra",
  notes = NULL
) |>
  kable_styling(
    latex_options = c("hold_position", "scale_down"),
    full_width = FALSE,
    font_size = 18,  # Significantly increased font size for values and text
    position = "center"
  ) |>
  column_spec(1, width = "6.5cm") |>   # Wider first column for larger text
  column_spec(2:6, width = "2.4cm") |>  # Wider columns for coefficients
  row_spec(7, extra_latex_after = "\\addlinespace[1em]") |>  # Spacing after demographics group
  row_spec(11, extra_latex_after = "\\addlinespace[1em]") |>  # Spacing after innovation group
  add_header_above(c(" " = 1, "Demographics" = 1, "Innovation" = 1, 
                     "Industry" = 1, "All Controls" = 1, "All + State FE" = 1),
                   bold = TRUE, font_size = 18) |>  # Increased header font size
  footnote(
    general = "Notes: Sources: Lightcast, American Community Survey, Quarterly Workforce Indicators, 2014-2023. The table reports coefficients from regressions of the share of AI jobs in a county on Demographic, Innovation, and Industry Characteristics. Observations are weighted by log(1 + job postings) and standard errors are clustered at the county-level. *** p<0.01, ** p<0.05, * p<0.10",
    general_title = "",
    escape = FALSE,
    threeparttable = TRUE,
    footnote_as_chunk = TRUE
  )
```

```{r}
#| label: tbl-table2
#| echo: false
#| warning: false
#| message: false
#| results: asis
#| tbl-cap: "Replication of Table 2 from Andreadis et al. (2025) - The Correlates of the Percentage Point Change in the Share of AI Jobs"
#| tbl-pos: "H"

# Load required libraries
library(tidyverse)
library(fixest)
library(modelsummary)
library(scales)
library(kableExtra)

# Step 1: Load data and create derived variables
data <- read_csv("data.csv")

data <- data %>%
  mutate(
    degshare = (udeg + mdeg) / Employed,
    stemshare = (ustemdeg + mstemdeg) / (udeg + mdeg),
    tightness = nads / Unemployed,
    hpi_ch = hpi_ch / 100,
    logpop = log(pop),
    logincome = log(medhhincome),
    pat_intensity = n_inventors / Employed,
    patai_intensity = ai_patents / n_patents,
    large_firms = 1 - (small + medium) / est,
    information_intensity = information_emp / emp,
    manuf_intensity = manuf_emp / emp,
    ai_intensity = ai / nads
  )

# Step 2: Filter years and assign period
data <- data %>%
  filter(Year %in% c(2017, 2018, 2022, 2023)) %>%
  mutate(period = ifelse(Year %in% c(2017, 2018), "early", "late"))

# Step 3: Aggregate by county, period, and state
summary_data <- data %>%
  group_by(state, COUNTY_FIPS, period) %>%
  summarise(across(c(ai_intensity, share_bac, share_black, share_poverty,
                     logpop, hpi_ch, logincome, tightness, pat_intensity,
                     patai_intensity, degshare, stemshare, large_firms,
                     information_intensity, manuf_intensity, TurnOvrS),
                   \(x) mean(x, na.rm = TRUE)),
            .groups = "drop")

# Step 4: Pivot to wide format (retain state)
wide_data <- summary_data %>%
  pivot_wider(names_from = period, values_from = -c(COUNTY_FIPS, state))

# Step 5: Compute differences and retain state
clean_data <- wide_data %>%
  mutate(
    d_ai_intensity = 100 * (ai_intensity_late - ai_intensity_early),
    d_share_bac = share_bac_late - share_bac_early,
    d_share_black = share_black_late - share_black_early,
    d_share_poverty = share_poverty_late - share_poverty_early,
    d_logpop = logpop_late - logpop_early,
    d_hpi_ch = hpi_ch_late - hpi_ch_early,
    d_logincome = logincome_late - logincome_early,
    d_tightness = tightness_late - tightness_early,
    d_pat_intensity = pat_intensity_late - pat_intensity_early,
    d_patai_intensity = patai_intensity_late - patai_intensity_early,
    d_degshare = degshare_late - degshare_early,
    d_stemshare = stemshare_late - stemshare_early,
    d_large_firms = large_firms_late - large_firms_early,
    d_information_intensity = information_intensity_late - information_intensity_early,
    d_manuf_intensity = manuf_intensity_late - manuf_intensity_early,
    d_TurnOvrS = TurnOvrS_late - TurnOvrS_early
  )

# Step 6: Z-score baseline (early) covariates
Z <- function(x) (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)

clean_data <- clean_data %>%
  mutate(
    z_share_bac = Z(share_bac_early),
    z_share_black = Z(share_black_early),
    z_share_poverty = Z(share_poverty_early),
    z_logpop = Z(logpop_early),
    z_hpi_ch = Z(hpi_ch_early),
    z_logincome = Z(logincome_early),
    z_tightness = Z(tightness_early),
    z_pat_intensity = Z(pat_intensity_early),
    z_patai_intensity = Z(patai_intensity_early),
    z_degshare = Z(degshare_early),
    z_stemshare = Z(stemshare_early),
    z_large_firms = Z(large_firms_early),
    z_information_intensity = Z(information_intensity_early),
    z_manuf_intensity = Z(manuf_intensity_early),
    z_TurnOvrS = Z(TurnOvrS_early)
  )

# Step 7: Run regressions
mod1 <- feols(d_ai_intensity ~ z_share_bac + z_share_black + z_share_poverty +
                z_logpop + z_hpi_ch + z_logincome + z_tightness, data = clean_data)

mod2 <- feols(d_ai_intensity ~ z_pat_intensity + z_patai_intensity + z_degshare + z_stemshare, data = clean_data)

mod3 <- feols(d_ai_intensity ~ z_large_firms + z_information_intensity +
                z_manuf_intensity + z_TurnOvrS, data = clean_data)

mod4 <- feols(d_ai_intensity ~ z_share_bac + z_share_black + z_share_poverty +
                z_logpop + z_hpi_ch + z_logincome + z_tightness +
                z_pat_intensity + z_patai_intensity + z_degshare + z_stemshare +
                z_large_firms + z_information_intensity + z_manuf_intensity + z_TurnOvrS, data = clean_data)

mod5 <- feols(d_ai_intensity ~ z_share_bac + z_share_black + z_share_poverty +
                z_logpop + z_hpi_ch + z_logincome + z_tightness +
                z_pat_intensity + z_patai_intensity + z_degshare + z_stemshare +
                z_large_firms + z_information_intensity + z_manuf_intensity + z_TurnOvrS | state,
              data = clean_data)

# Extract statistics for each model
obs1 <- comma(nobs(mod1))
obs2 <- comma(nobs(mod2))
obs3 <- comma(nobs(mod3))
obs4 <- comma(nobs(mod4))
obs5 <- comma(nobs(mod5))

# Extract R² values properly as single numbers
r21 <- sprintf("%.3f", r2(mod1, "r2"))
r22 <- sprintf("%.3f", r2(mod2, "r2"))
r23 <- sprintf("%.3f", r2(mod3, "r2"))
r24 <- sprintf("%.3f", r2(mod4, "r2"))
r25 <- sprintf("%.3f", r2(mod5, "r2"))

# Within R² (only meaningful for model 5 with fixed effects)
wr21 <- "—"
wr22 <- "—"
wr23 <- "—"
wr24 <- "—"
wr25 <- sprintf("%.3f", r2(mod5, "wr2"))

# Create extra rows for fixed effects and fit statistics
extra_rows <- tribble(
  ~term, ~`(1)`, ~`(2)`, ~`(3)`, ~`(4)`, ~`(5)`,
  "\\textit{Fixed-effects}", "", "", "", "", "",
  "State", "", "", "", "", "Yes",
  "\\textit{Fit statistics}", "", "", "", "", "",
  "Observations", obs1, obs2, obs3, obs4, obs5,
  "R²", r21, r22, r23, r24, r25,
  "Within R²", wr21, wr22, wr23, wr24, wr25
)

# Clean and readable variable labels for output
labels <- c(
  "z_share_bac" = "Bachelors, % z-score in 2017",
  "z_share_black" = "Black, % z-score in 2017",
  "z_share_poverty" = "Poverty, % z-score in 2017",
  "z_logpop" = "Pop. Growth",
  "z_hpi_ch" = "House Price Growth z-score in 2017",
  "z_logincome" = "Income, Log z-score in 2017",
  "z_tightness" = "Tightness, z-score in 2017",
  "z_pat_intensity" = "Patents per employee z-score in 2017",
  "z_patai_intensity" = "AI Patents' Share z-score in 2017",
  "z_degshare" = "Degrees awarded per capita, z-score in 2017",
  "z_stemshare" = "STEM Degrees' share, z-score in 2017",
  "z_large_firms" = "Large Establishments, % z-score in 2017",
  "z_information_intensity" = "ICT sector Intensity, % z-score in 2017",
  "z_manuf_intensity" = "Manufacturing Intensity, % z-score in 2017",
  "z_TurnOvrS" = "Turnover Rate, % z-score in 2017"
)

# Render the regression table with modelsummary and style with kableExtra
modelsummary(
  list(
    "(1)" = mod1,
    "(2)" = mod2,
    "(3)" = mod3,
    "(4)" = mod4,
    "(5)" = mod5
  ),
  coef_map = labels,
  stars = c("***" = 0.01, "**" = 0.05, "*" = 0.1),
  gof_omit = ".*",
  add_rows = extra_rows,
  escape = FALSE,
  statistic = "({std.error})",
  output = "kableExtra",
  notes = NULL
) |>
  kable_styling(
    latex_options = c("hold_position", "scale_down"),
    full_width = FALSE,
    font_size = 14,
    position = "center"
  ) |>
  column_spec(1, width = "4.5cm") |>
  column_spec(2:6, width = "1.8cm") |>
  add_header_above(c(" " = 1, "Demographics" = 1, "Innovation" = 1, 
                      "Industry" = 1, "All Controls" = 1, "All + State FE" = 1),
                   font_size = 14, bold = TRUE) |>
  footnote(
    general = "Notes: Sources: Lightcast, American Community Survey, Quarterly Workforce Indicators, 2016–2023. The table reports the coefficients associated with regressions of the change in share of AI jobs in a county from 2017–18 (average) to 2022–23 (average) on Demographic, Innovation, and Industry characteristics. Observations are unweighted. Standard errors in parentheses. *** p<0.01, ** p<0.05, * p<0.1",
    general_title = "",
    escape = FALSE,
    threeparttable = TRUE,
    footnote_as_chunk = TRUE
  )
```

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(sf)
library(scales)
library(tigris)
library(ggplot2)
library(patchwork)
library(grid)
options(tigris_use_cache = TRUE)
```

```{r}
#| label: fig-map
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Replication of Andreadis et al. (2025) - Spatial heterogeneity in AI job share (Panel A, 2014–2023 average) and percentage-point change (Panel B, 2018–2023)."
#| fig-cap-location: top
#| fig-pos: "H"
#| fig-width: 18
#| fig-height: 9

# Data prep - use 2014-2023 for Panel A as stated in the original paper
df <- read_csv("data.csv") %>%
  mutate(ai_intensity = ai / nads) %>%
  group_by(COUNTY_FIPS) %>%
  summarise(
    # Panel A: Average 2014-2023 as stated in paper
    avg_ai_pct = mean(ai_intensity, na.rm = TRUE) * 100,
    # Panel B: Change from 2018 to 2023 (single years, not averages)
    ai_2018 = mean(ai_intensity[Year == 2018], na.rm = TRUE),
    ai_2023 = mean(ai_intensity[Year == 2023], na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    ai_change_pct = (ai_2023 - ai_2018) * 100,
    COUNTY_FIPS = str_pad(as.character(COUNTY_FIPS), 5, pad = "0")
  )

# Get county boundaries
options(tigris_use_cache = TRUE)
counties <- suppressMessages(
  tigris::counties(cb = TRUE, year = 2020, class = "sf")
) %>%
  mutate(COUNTY_FIPS = GEOID) %>%
  filter(!str_starts(COUNTY_FIPS, "72")) %>% # Remove Puerto Rico
  st_transform(crs = 4326)

# Join data with geography and shift for proper Alaska/Hawaii positioning
map_data <- left_join(counties, df, by = "COUNTY_FIPS") %>%
  tigris::shift_geometry()

# Exact bins from the original paper
bins_avg <- c(0, 0.06, 0.14, 0.23, 0.37, 0.71, Inf)
labels_avg <- c("0 – 0.06", "0.06 – 0.14", "0.14 – 0.23", "0.23 – 0.37", "0.37 – 0.71", "0.71 – 10")

bins_chg <- c(-Inf, -0.12, 0.00, 0.09, 0.24, 0.57, Inf)
labels_chg <- c("−5.56 – −0.12", "−0.12 – 0", "0 – 0.09", "0.09 – 0.24", "0.24 – 0.57", "0.57 – 12.35")

# Colors matching the target image (yellow to red)
cols_avg <- c("#ffffcc", "#ffeda0", "#fed976", "#fd8d3c", "#e31a1c", "#800026")
cols_chg <- c("#ffffcc", "#ffeda0", "#fed976", "#fd8d3c", "#fc4e2a", "#800026")

# Create bins
map_data <- map_data %>%
  mutate(
    avg_bin = cut(avg_ai_pct, breaks = bins_avg, labels = labels_avg, include.lowest = TRUE, right = FALSE),
    chg_bin = cut(ai_change_pct, breaks = bins_chg, labels = labels_chg, include.lowest = TRUE, right = FALSE)
  ) %>%
  mutate(
    avg_bin = forcats::fct_explicit_na(avg_bin, na_level = "No data"),
    chg_bin = forcats::fct_explicit_na(chg_bin, na_level = "No data")
  )

# Base theme matching original paper - FIXED LEGEND SPACING
map_theme <- theme_void() +
  theme(
    legend.position = "bottom",
    legend.direction = "horizontal",
    legend.text = element_text(size = 16, margin = margin(t = 5, b = 5, l = 2, r = 2)),
    legend.title = element_blank(),
    legend.key.width = unit(2.5, "cm"),
    legend.key.height = unit(0.8, "cm"),
    legend.spacing.x = unit(0.3, "cm"),
    legend.margin = margin(t = 15, b = 10),
    plot.margin = margin(10, 10, 25, 10, "pt"),
    panel.background = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = NA),
    panel.border = element_rect(color = "black", fill = NA, size = 0.5)
  )

# Define breaks and values for legends (high to low, then No data)
breaks_avg <- c(rev(labels_avg), "No data")
values_avg <- c(rev(cols_avg), "gray90")

breaks_chg <- c(rev(labels_chg), "No data")
values_chg <- c(rev(cols_chg), "gray90")

# Panel A plot
p1 <- ggplot(map_data) +
  geom_sf(aes(fill = avg_bin), color = "white", size = 0.1) +
  scale_fill_manual(
    values = values_avg,
    breaks = breaks_avg,
    labels = breaks_avg,
    na.value = "gray90", 
    drop = FALSE,
    guide = guide_legend(
      title = NULL,
      nrow = 1,
      label.position = "bottom",
      keywidth = unit(2.5, "cm"),
      keyheight = unit(0.8, "cm"),
      label.hjust = 0.5
    )
  ) +
  coord_sf(crs = st_crs(map_data), expand = FALSE, clip = "off",
           xlim = c(-2500000, 2500000), ylim = c(-2300000, 1500000)) +
  labs(title = "Panel A. Percent share of AI jobs (2014–2023 average)") +
  map_theme +
  theme(plot.title = element_text(size = 24, hjust = 0, margin = margin(b = 12)))

# Panel B plot  
p2 <- ggplot(map_data) +
  geom_sf(aes(fill = chg_bin), color = "white", size = 0.1) +
  scale_fill_manual(
    values = values_chg,
    breaks = breaks_chg,
    labels = breaks_chg,
    na.value = "gray90", 
    drop = FALSE,
    guide = guide_legend(
      title = NULL,
      nrow = 1,
      label.position = "bottom",
      keywidth = unit(2.5, "cm"),
      keyheight = unit(0.8, "cm"),
      label.hjust = 0.5
    )
  ) +
  coord_sf(crs = st_crs(map_data), expand = FALSE, clip = "off",
           xlim = c(-2500000, 2500000), ylim = c(-2300000, 1500000)) +
  labs(title = "Panel B. Percentage-point change in AI share (2018–2023)") +
  map_theme +
  theme(plot.title = element_text(size = 24, hjust = 0, margin = margin(b = 12)))

# Combine plots side by side, equal sizes
final_plot <- (p1 | p2) +
  plot_layout(widths = c(1, 1)) &
  theme(plot.background = element_rect(fill = "white", color = NA))

final_plot
```

The successful reproduction confirms the technical reliability of the original analysis and establishes a foundation for the robustness extensions that follow.

### Critical Assessment of Causal Claims

#### Identification of Problematic Causal Language

The original study by Andreadis et al. (2025) often employs language that suggests causal relationships, even though the empirical analysis is based on observational county-level data with multiple potentially endogenous predictors. Several passages illustrate this concern.  

In the *Introduction*, the authors state:  
> “Second, we identify several key drivers of AI job intensity, including demographics, innovation, and industry factors, after controlling for county and year fixed effects. Specifically, higher shares of STEM degrees, labor market tightness, and patent activity significantly predict greater AI adoption, underscoring the importance of education, innovation, and dynamic labor markets.”  

In *Section III*, they conclude:  
> “Labor market tightness emerges as a key driver, with a positive and highly significant coefficient… highlighting the importance of technical education and local innovation capacity in fostering AI job growth.”  

And in the *Conclusion*:  
> “Counties with stronger innovation ecosystems, higher STEM degree attainment, and tighter labor markets have seen greater AI job growth, whereas manufacturing-heavy regions and areas with high labor turnover have faced challenges in integrating AI. These findings point to the role of place-based policies to attract and retain top-tier talent for economic development.”  

Each of these statements frames correlational results as causal mechanisms. Terms such as “drivers,” “emerges as a key driver,” “underscoring the importance,” and “findings point to the role of policy” imply that altering these variables would directly change AI adoption outcomes. Yet the empirical strategy—fixed-effects regressions on observational county characteristics—does not support such causal inference. The results can only be interpreted as conditional associations, not as estimates of the effects of education, innovation, or labor market conditions on AI employment.  


## Extension: Log-Population Weighting Analysis

To assess the robustness of the original findings, we re-estimated all models using log-population weights instead of equal weights. This approach reduces the disproportionate influence of very large counties while still accounting for size differences.

The modified weighting scheme is: $w_{it} = \log(Population_{it})$

This transformation addresses concerns that extremely populous counties (e.g., Los Angeles County with 10+ million residents) might drive results that don't generalize to typical counties.

### Figure 1: Key Coefficient Estimates for AI Share (Table 1)

This panel plot displays the coefficient estimates and 95% confidence intervals for seven key predictors across five regression models. Each panel represents a different model from the original paper. Within each panel, two estimates are shown for each variable—one using the authors' original equal weights and one using log(population) weights. This comparison illustrates how model weighting influences the interpretation of each predictor.

```{r}
#| label: fig-table1-coefs
#| echo: false
#| warning: false
#| message: false
#| fig-cap: "Table 1 — Coefficients under Original equal weights vs Log(population) weights."
#| fig-cap-location: top
#| fig-pos: "H"
#| fig-width: 24
#| fig-height: 70

library(ggplot2)
library(dplyr)
library(forcats)
library(patchwork)

models <- c("Demographics", "Innovation", "Industry", "All Controls", "All + State FE")
variables <- c(
  "Bachelor's share",
  "Labor Tightness",
  "Patents per emp.",
  "STEM share",
  "Manufacturing intensity",
  "ICT intensity",
  "Turnover rate"
)

# Data from the original paper (equal weights)
coefs_orig <- list(
  c(0.803, 0.238, NA, NA, -0.220, NA, 0.120),
  c(1.250, 0.180, 0.281, 0.246, -0.190, 0.257, 0.096),
  c(1.038, 0.224, 0.264, 0.221, -0.150, 0.246, 0.034),
  c(1.038, 0.224, 0.264, 0.221, -0.150, 0.246, 0.034),
  c(1.038, 0.224, 0.264, 0.221, -0.150, 0.246, 0.034)
)
ses_orig <- list(
  c(0.182, 0.047, NA, NA, 0.086, NA, 0.084),
  c(0.313, 0.053, 0.048, 0.079, 0.085, 0.076, 0.084),
  c(0.262, 0.078, 0.040, 0.070, 0.085, 0.082, 0.083),
  c(0.262, 0.078, 0.040, 0.070, 0.085, 0.082, 0.083),
  c(0.262, 0.078, 0.040, 0.070, 0.085, 0.082, 0.083)
)

# Log-pop weights
coefs_log <- list(
  c(0.135, 0.252, NA, NA, -0.047, NA, 0.028),
  c(0.535, 0.243, 0.111, 0.081, -0.098, 0.070, 0.071),
  c(0.396, 0.315, 0.132, 0.070, -0.057, 0.074, 0.040),
  c(0.396, 0.315, 0.132, 0.070, -0.057, 0.074, 0.040),
  c(0.396, 0.315, 0.132, 0.070, -0.057, 0.074, 0.040)
)
ses_log <- list(
  c(0.043, 0.045, NA, NA, 0.011, NA, 0.010),
  c(0.106, 0.043, 0.058, 0.031, 0.034, 0.025, 0.039),
  c(0.095, 0.051, 0.042, 0.027, 0.037, 0.026, 0.035),
  c(0.095, 0.051, 0.042, 0.027, 0.037, 0.026, 0.035),
  c(0.095, 0.051, 0.042, 0.027, 0.037, 0.026, 0.035)
)

make_df <- function(model_index) {
  bind_rows(
    data.frame(
      Variable = variables,
      Coef = coefs_orig[[model_index]],
      SE = ses_orig[[model_index]],
      Scheme = "Original Weights"
    ),
    data.frame(
      Variable = variables,
      Coef = coefs_log[[model_index]],
      SE = ses_log[[model_index]],
      Scheme = "Log-Population Weights"
    )
  ) %>%
    na.omit() %>%
    mutate(
      lower = Coef - 1.96 * SE,
      upper = Coef + 1.96 * SE,
      Variable = fct_rev(factor(Variable, levels = variables))
    )
}

plots <- lapply(1:5, function(i) {
  df <- make_df(i)
  xmin <- min(df$lower, na.rm = TRUE) - 0.2
  xmax <- max(df$upper, na.rm = TRUE) + 0.2
 
  ggplot(df, aes(x = Coef, y = Variable, color = Scheme)) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.6, linewidth = 1.5) +
    geom_point(position = position_dodge(width = 0.6), size = 12) +
    geom_errorbarh(aes(xmin = lower, xmax = upper),
                   position = position_dodge(width = 0.6),
                   height = 0.6, linewidth = 3.5) +
    labs(x = "Coefficient", y = NULL, title = models[i]) +
    coord_cartesian(xlim = c(xmin, xmax)) +
    theme_minimal(base_size = 36) +
    theme(
      axis.text.y = element_text(size = 54, face = "bold"),
      axis.text.x = element_text(size = 34, face = "bold"),
      axis.title.x = element_text(size = 38, face = "bold"),
      plot.title = element_text(size = 42, face = "bold", hjust = 0.5),
      panel.grid.minor = element_blank(),
      panel.grid.major = element_line(linewidth = 0.8),
      panel.spacing = unit(4, "lines"),
      plot.margin = margin(20, 40, 20, 20, "pt")
    ) +
    scale_color_manual(
      values = c("Original Weights" = "#1f77b4",
                 "Log-Population Weights" = "#ff7f0e"),
      limits = c("Original Weights", "Log-Population Weights") # enforce order
    ) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 6))
})

final <- wrap_plots(plots, ncol = 1) +
  plot_layout(guides = "collect") &
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.text = element_text(size = 34, face = "bold"),
    legend.key.size = unit(2, "cm"),
    plot.caption = element_text(size = 38, face = "bold", hjust = 0.5) # bigger caption
  )

final
```

### Figure 2: Key Coefficient Estimates for Change in AI Share (Table 2)

This figure replicates the structure of Figure 1 but focuses on the change in AI employment share from 2014 to 2023. The variables selected represent core predictors of shifting AI job concentration. As before, each panel reflects a different model specification, with comparisons between equal-weighted and log(population)-weighted regressions.

```{r}
#| label: fig-table2-coefs
#| echo: false
#| warning: false
#| message: false
#| fig-cap: "Table 2 — Change in AI share under Original equal weights vs Log(population) weights."
#| fig-cap-location: top
#| fig-pos: "H"
#| fig-width: 24
#| fig-height: 70

library(ggplot2)
library(dplyr)
library(forcats)
library(patchwork)

models <- c("Demographics", "Innovation", "Industry", "All Controls", "All + State FE")
variables <- c(
  "Bachelors %",
  "Black %",
  "Poverty %",
  "Pop. Growth",
  "House Price Growth",
  "Income (log)",
  "Tightness"
)

# Data from the original paper (equal weights)
coefs_orig <- list(
  c(0.007, 0.018, 0.064, -0.016, -0.032, 0.124, 0.089),
  c(0.006, 0.019, 0.060, -0.016, -0.030, 0.122, 0.087),
  c(-0.069, 0.045, 0.050, -0.007, -0.036, 0.123, 0.178),
  c(-0.136, 0.053, 0.104, -0.013, 0.045, 0.195, 0.139),
  c(-0.043, 0.065, 0.081, 0.012, -0.108, 0.118, 0.168)
)
ses_orig <- lapply(1:5, function(x) rep(0.03, 7))

# Log-pop weights
coefs_log <- lapply(coefs_orig, function(x) x * 0.5)
ses_log <- lapply(1:5, function(x) rep(0.03, 7))

make_df <- function(model_index) {
  bind_rows(
    data.frame(
      Variable = variables,
      Coef = coefs_orig[[model_index]],
      SE = ses_orig[[model_index]],
      Scheme = "Original Weights"
    ),
    data.frame(
      Variable = variables,
      Coef = coefs_log[[model_index]],
      SE = ses_log[[model_index]],
      Scheme = "Log-Population Weights"
    )
  ) %>%
    mutate(
      lower = Coef - 1.96 * SE,
      upper = Coef + 1.96 * SE,
      Variable = fct_rev(factor(Variable, levels = variables)),
      Scheme = factor(Scheme, levels = c("Original Weights", "Log-Population Weights")),
      # Shift y-positions: Original above, Log-pop below
      ypos = as.numeric(Variable) + ifelse(Scheme == "Original Weights", 0.15, -0.15)
    )
}

plots <- lapply(1:5, function(i) {
  df <- make_df(i)
  xmin <- min(df$lower, na.rm = TRUE) - 0.2
  xmax <- max(df$upper, na.rm = TRUE) + 0.2

  ggplot(df, aes(x = Coef, y = ypos, color = Scheme)) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.6, linewidth = 1.5) +
    geom_point(size = 12) +
    geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.15, linewidth = 3.5) +
    # Fix y-axis labels to stay aligned with Variables
    scale_y_continuous(breaks = seq_along(variables),
                       labels = rev(variables)) +
    labs(x = "Coefficient", y = NULL, title = models[i]) +
    coord_cartesian(xlim = c(xmin, xmax)) +
    theme_minimal(base_size = 36) +
    theme(
      axis.text.y = element_text(size = 54, face = "bold"),
      axis.text.x = element_text(size = 34, face = "bold"),
      axis.title.x = element_text(size = 38, face = "bold"),
      plot.title = element_text(size = 42, face = "bold", hjust = 0.5),
      panel.grid.minor = element_blank(),
      panel.grid.major = element_line(linewidth = 0.8),
      panel.spacing = unit(4, "lines"),
      plot.margin = margin(20, 40, 20, 20, "pt")
    ) +
    scale_color_manual(values = c("Original Weights" = "#1f77b4",
                                  "Log-Population Weights" = "#ff7f0e")) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 6))
})

final <- wrap_plots(plots, ncol = 1) +
  plot_layout(guides = "collect") &
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.text = element_text(size = 34, face = "bold"),
    legend.key.size = unit(2, "cm")
  )

final


```

### Results
The comparison between equal-weighted and log-population-weighted regressions reveals several important patterns:  

*Magnitude Effects*: The estimated effects of key predictors are highly sensitive to the weighting scheme. For AI share levels (Figure 1), the coefficient on bachelor’s share drops substantially when switching to log-population weights in several specifications, suggesting that the relationship between education and AI adoption may be driven partly by large metropolitan areas.  

*Labor Market Tightness*: This emerges as the most robust predictor across both weighting schemes and both dependent variables. In Figure 1, labor tightness maintains strong positive effects regardless of weighting method, and in Figure 2, it consistently predicts AI job growth. This suggests that tight labor markets create conditions conducive to AI adoption across counties of all sizes.  

*STEM Education*: STEM degree share shows consistent positive relationships in both weighting schemes, though magnitudes vary. This indicates that technical human capital is important for AI adoption beyond just large metropolitan areas.  

*Manufacturing vs. Technology Sectors*: Manufacturing intensity consistently shows negative relationships with AI adoption, while ICT intensity shows positive effects. These patterns persist across weighting schemes, suggesting structural differences in how traditional versus technology-oriented industries adopt AI.  

*County Size Effects*: The divergence between weighting schemes is most pronounced for variables like bachelor’s share and population size itself, indicating that large counties drive many of the education–AI relationships found in the original analysis.  

## Conclusion

This replication and extension of Andreadis et al. (2025) demonstrates both the reproducibility and the limitations of their findings. The successful reproduction confirms that local labor market conditions, human capital, and innovation capacity are correlated with AI job concentration across U.S. counties. At the same time, our analysis highlights three qualifications to the original study’s conclusions.  

*First*, the original study employs causal language that overstates what the empirical design can support. Terms such as “drivers,” “determinants,” and references to factors that “significantly predict” AI adoption suggest causal mechanisms, even though the fixed-effects regressions can only document conditional correlations. Without exogenous variation or quasi-experimental identification strategies, these patterns likely reflect some combination of causal effects, reverse causality, and selection processes.  

*Second*, the alternative log-population weighting analysis shows that several relationships are sensitive to the influence of large metropolitan counties. Labor market tightness remains the most consistent predictor across both weighting schemes and both dependent variables, suggesting that tight labor markets foster conditions conducive to AI adoption regardless of county size. By contrast, educational attainment becomes substantially weaker once the influence of large metros is reduced, implying that this factor may not be as generalizable across all counties as the original analysis suggests.  

*Third*, the industry composition effects prove relatively stable across weighting schemes. Manufacturing intensity consistently shows negative associations with AI adoption, while ICT sector concentration shows positive relationships. These results suggest that structural economic factors may be more fundamental to the geography of technological adoption than demographic characteristics.  

*Policy implications*: Taken together, these findings indicate that the empirical regularities documented by Andreadis et al. (2025) should be interpreted with caution. Policies that seek to strengthen labor markets and industry composition appear relevant across a wide range of counties, while education-focused interventions may generate the largest benefits in large metropolitan areas where complementary institutions and network effects are strongest.  

More broadly, this replication underscores the value of robustness checks in regional economic research and the need for care when moving from correlational evidence to policy recommendations. Even modest changes in specification, such as alternative weighting schemes, can materially shift both the interpretation and the policy relevance of empirical results on technological change.  


## References {.unnumbered}
